{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "573d9ef5-d92c-4099-8cd3-806ca9db29d9",
   "metadata": {},
   "source": [
    "# Classification accuracy assessment\n",
    "\n",
    "Rainey Aberle\n",
    "\n",
    "2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dbfb0e-ce5e-4fb9-b745-40c8cb4a2130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Import packages\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import rasterio as rio\n",
    "from shapely.geometry import Polygon\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce56d706-e737-4d8f-b8f1-a868cb822bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Define paths in directory\n",
    "site_name = 'LemonCreek'\n",
    "# base directory (path to snow-cover-mapping/)\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/'\n",
    "# path to classified points used to train and test classifiers\n",
    "data_pts_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/snow_cover_mapping/classified-points/LemonCreek/'\n",
    "# full path and file name to AOI shapefile\n",
    "AOI_fn = data_pts_path + '../../study-sites/LemonCreek/AOIs/LemonCreek_RGI_outline.shp' \n",
    "# full path and file name to DEM\n",
    "DEM_fn = data_pts_path + '../../study-sites/LemonCreek/DEMs/LemonCreek_ArcticDEM_clip.tif' \n",
    "\n",
    "# -----Determine settings\n",
    "terrain_parameters = False # whether to use terrain parameters (elevation, slope, aspect) in classification\n",
    "save_figures = True # whether to save output figures\n",
    "\n",
    "# -----Add path to functions\n",
    "sys.path.insert(1, base_path + 'functions/')\n",
    "import pipeline_utils as f\n",
    "\n",
    "# -----Load dataset characteristics dictionary\n",
    "dataset_dict = json.load(open(base_path + 'inputs-outputs/datasets_characteristics.json'))\n",
    "\n",
    "# -----Load classified points\n",
    "os.chdir(data_pts_path)\n",
    "data_pts_fns = glob.glob('LemonCreek*.shp')\n",
    "data_pts_fns = sorted(data_pts_fns)\n",
    "data_pts_fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80865d3c-9df8-466f-b147-f94a0bcc45b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Load AOI as gpd.GeoDataFrame\n",
    "AOI = gpd.read_file(AOI_fn)\n",
    "# reproject the AOI to WGS to solve for the optimal UTM zone\n",
    "AOI_WGS = AOI.to_crs('EPSG:4326')\n",
    "AOI_WGS_centroid = [AOI_WGS.geometry[0].centroid.xy[0][0],\n",
    "                    AOI_WGS.geometry[0].centroid.xy[1][0]]\n",
    "# grab the optimal UTM zone EPSG code\n",
    "epsg_UTM = f.convert_wgs_to_utm(AOI_WGS_centroid[0], AOI_WGS_centroid[1])\n",
    "print('Optimal UTM CRS = EPSG:' + str(epsg_UTM))\n",
    "\n",
    "# -----Load DEM as Xarray DataSet\n",
    "# reproject AOI to UTM\n",
    "AOI_UTM = AOI.to_crs('EPSG:'+str(epsg_UTM))\n",
    "# load DEM as xarray DataSet\n",
    "DEM = xr.open_dataset(DEM_fn)\n",
    "DEM = DEM.rename({'band_data': 'elevation'})\n",
    "# reproject the DEM to the optimal UTM zone\n",
    "DEM = DEM.rio.reproject('EPSG:'+str(epsg_UTM))\n",
    "DEM = DEM.rio.write_crs('EPSG:'+str(epsg_UTM))\n",
    "# remove unnecessary data (possible extra bands from ArcticDEM or other DEM)\n",
    "if len(np.shape(DEM.elevation.data))>2:\n",
    "    DEM['elevation'] = DEM.elevation[0]\n",
    "    \n",
    "\n",
    "# -----Plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,6))\n",
    "dem_im = ax.imshow(DEM.elevation.data, cmap='terrain', \n",
    "          extent=(np.min(DEM.x.data)/1e3, np.max(DEM.x.data)/1e3, np.min(DEM.y.data)/1e3, np.max(DEM.y.data)/1e3))\n",
    "if type(AOI_UTM.geometry[0])==Polygon:\n",
    "    ax.plot([x/1e3 for x in AOI_UTM.geometry[0].exterior.coords.xy[0]],\n",
    "            [y/1e3 for y in AOI_UTM.geometry[0].exterior.coords.xy[1]], '-k')\n",
    "elif type(AOI_UTM.geometry[0])==MultiPolygon:\n",
    "    [ax.plot([x/1e3 for x in geom.exterior.coords.xy[0]],\n",
    "            [y/1e3 for y in geom.exterior.coords.xy[1]], '-k') for geom in AOI_UTM.geometry[0].geoms]\n",
    "ax.grid()\n",
    "ax.set_xlabel('Easting [km]')\n",
    "ax.set_ylabel('Northing [km]')\n",
    "fig.colorbar(dem_im, ax=ax, shrink=0.5, label='Elevation [m]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1943b536-8487-4684-b38c-8fe2add83163",
   "metadata": {},
   "source": [
    "## Loop through datasets, load images and classified points, calculate performance metrics, save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad70d141-d89c-465e-a506-c526b2f0ccad",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Define datasets\n",
    "datasets = ['Landsat', 'PlanetScope', 'Sentinel-2_SR', 'Sentinel-2_TOA']\n",
    "\n",
    "# -----Loop through datasets\n",
    "results_df_full = pd.DataFrame()\n",
    "for dataset in datasets:\n",
    "    \n",
    "    print(dataset)\n",
    "    print('----------')\n",
    "\n",
    "    # -----Load trained classifier and feature columns\n",
    "    clf_fn = base_path+'inputs-outputs/'+dataset+'_classifier_all_sites.joblib'\n",
    "    clf = load(clf_fn)\n",
    "    feature_cols_fn = base_path+'inputs-outputs/'+dataset+'_feature_columns.json'\n",
    "    feature_cols = json.load(open(feature_cols_fn))\n",
    "\n",
    "    # -----Subset dataset dictionary\n",
    "    ds_dict = dataset_dict[dataset]\n",
    "\n",
    "    # -----Set up testing data\n",
    "    # load image file names\n",
    "    im_fns = sorted(glob.glob(data_pts_path + 'LemonCreek_'+dataset+'*.tif'))\n",
    "    # load data point file names\n",
    "    data_pts_snow_fns = sorted(glob.glob(data_pts_path+'*_snow.shp'))\n",
    "    data_pts_no_snow_fns = sorted(glob.glob(data_pts_path+'*_no-snow.shp'))\n",
    "    # initialize full data frame\n",
    "    data_pts_full = gpd.GeoDataFrame()\n",
    "    # loop through Landsat images\n",
    "    for i, im_fn in enumerate(im_fns):\n",
    "\n",
    "        if dataset=='PlanetScope':\n",
    "            # Load image\n",
    "            im_fn = im_fns[i] # image file name\n",
    "            im_date = data_pts_snow_fns[i].split('LemonCreek_')[1][0:8]\n",
    "            im_dt = np.datetime64(im_date)\n",
    "            im_xr = xr.open_dataset(im_fn)\n",
    "            # Adjust image radiometry\n",
    "            polygons_top, polygons_bottom = f.create_aoi_elev_polys(AOI_UTM, DEM)\n",
    "            im_ds, im_adj_method = f.planetscope_adjust_image_radiometry(im_xr, im_dt, polygons_top, polygons_bottom, dataset_dict, skip_clipped=False) \n",
    "        else:\n",
    "            # load image as xarray.DataArray\n",
    "            im_da = rxr.open_rasterio(im_fn)\n",
    "            # reproject to optimal UTM zone (if necessary)\n",
    "            im_da = im_da.rio.reproject('EPSG:' + str(epsg_UTM))\n",
    "            # convert to xarray.DataSet\n",
    "            im_ds = im_da.to_dataset('band')\n",
    "            band_names = list(ds_dict['refl_bands'].keys())\n",
    "            im_ds = im_ds.rename({i + 1: name for i, name in enumerate(band_names)})\n",
    "            # account for image scalar and no data values\n",
    "            im_ds = xr.where(im_ds != ds_dict['no_data_value'],\n",
    "                             im_ds / ds_dict['image_scalar'], np.nan)\n",
    "            # expand dimensions to include time\n",
    "            im_dt = np.datetime64(datetime.datetime.fromtimestamp(im_da.attrs['system-time_start'] / 1000))\n",
    "            im_ds = im_ds.expand_dims({'time': [im_dt]})\n",
    "            # set CRS\n",
    "            im_ds.rio.write_crs('EPSG:' + str(im_da.rio.crs.to_epsg()), inplace=True)\n",
    "\n",
    "        # load classified points\n",
    "        # no-snow\n",
    "        data_pts_no_snow_fn = data_pts_no_snow_fns[i]\n",
    "        data_pts_no_snow = gpd.read_file(data_pts_no_snow_fn) # read file\n",
    "        data_pts_no_snow['class'] = 0 # determine class ID\n",
    "        # snow\n",
    "        data_pts_snow_fn = data_pts_snow_fns[i]\n",
    "        data_pts_snow = gpd.read_file(data_pts_snow_fn) # read file\n",
    "        data_pts_snow['class'] = 1 # determine class ID\n",
    "        # combine data pts, reproject to image CRS\n",
    "        data_pts = pd.concat([data_pts_snow, data_pts_no_snow])\n",
    "        data_pts = data_pts.to_crs(im_xr.rio.crs)\n",
    "        # drop any NaN points\n",
    "        data_pts = data_pts.drop('id', axis=1)\n",
    "        data_pts = data_pts.dropna()\n",
    "\n",
    "        # grab x and y sample points\n",
    "        data_pts_x = [x.geoms[0].coords.xy[0][0] for x in data_pts.geometry]\n",
    "        data_pts_y = [x.geoms[0].coords.xy[1][0] for x in data_pts.geometry]\n",
    "        # sample image values at data points\n",
    "        for band in feature_cols[0:-1]:\n",
    "            data_pts[band] = [im_ds[band].sel(x=xx, y=yy, method='nearest').data[0] for xx, yy in list(zip(data_pts_x, data_pts_y))]\n",
    "        # concatenate to data_pts_full\n",
    "        data_pts_full = pd.concat([data_pts_full, data_pts])\n",
    "\n",
    "    # Add NDSI column\n",
    "    data_pts_full['NDSI'] = ((data_pts_full[ds_dict['NDSI_bands'][0]] - data_pts_full[ds_dict['NDSI_bands'][1]]) \n",
    "                              /(data_pts_full[ds_dict['NDSI_bands'][0]] + data_pts_full[ds_dict['NDSI_bands'][1]]) )\n",
    "\n",
    "    # Reduce memory usage in data pts\n",
    "    data_pts_full = data_pts_full.dropna().reset_index(drop=True)\n",
    "    data_pts_full = f.reduce_memory_usage(data_pts_full, verbose=False)\n",
    "\n",
    "    # -----Test the trained classifier\n",
    "    # features\n",
    "    X = data_pts_full[feature_cols] \n",
    "    # target variable\n",
    "    y = data_pts_full['class'] \n",
    "    # Predict class values using trained classifier\n",
    "    y_pred = clf.predict(X)\n",
    "    # Adjust outputs to only test snow and no-snow\n",
    "    y_pred[y_pred <= 2] = 1 # snow = 1, 2\n",
    "    y_pred[y_pred > 2] = 0 # no-snow = 3, 4, 5\n",
    "    # Calculate overall accuracy\n",
    "    accuracy = metrics.accuracy_score(y, y_pred)\n",
    "    # Calculate Kappa score\n",
    "    K = metrics.cohen_kappa_score(y, y_pred)\n",
    "    # Calculate recall\n",
    "    R = metrics.recall_score(y, y_pred)\n",
    "    # Calculate precision\n",
    "    P = metrics.precision_score(y, y_pred)\n",
    "    # Calculate F1 score\n",
    "    F1 = metrics.f1_score(y, y_pred)\n",
    "    # Calculate confusion matrix\n",
    "    CM = metrics.confusion_matrix(y, y_pred)\n",
    "    # print results\n",
    "    print('n = '+str(len(y_pred)))\n",
    "    print('Overall accuracy = '+str(accuracy))\n",
    "    print('Kappa score = '+str(K))\n",
    "    print('Recall = '+str(R))\n",
    "    print('Precision = '+str(P))\n",
    "    print('F1 score = '+str(F1))\n",
    "    print('Confusion matrix: ')\n",
    "    print(CM)\n",
    "    print(' ')\n",
    "    \n",
    "    # -----Add results to dataframe\n",
    "    results_df = pd.DataFrame({'Dataset': dataset,\n",
    "                               'Overall accuracy': accuracy,\n",
    "                               'Kappa score': K,\n",
    "                               'Recall': R,\n",
    "                               'Precision': P,\n",
    "                               'F1 score': F1,\n",
    "                               'Confusion matrix': [CM],\n",
    "                               'N': len(y_pred)\n",
    "                              })\n",
    "    results_df_full = pd.concat([results_df_full, results_df])\n",
    "\n",
    "# -----Save results to file\n",
    "results_fn = base_path + 'inputs-outputs/classification_performace_metrics.csv'\n",
    "results_df_full.to_csv(results_fn, index=False)\n",
    "print('Performance metrics saved to file: '+results_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
