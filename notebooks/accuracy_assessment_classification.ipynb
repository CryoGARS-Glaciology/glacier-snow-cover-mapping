{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "573d9ef5-d92c-4099-8cd3-806ca9db29d9",
   "metadata": {},
   "source": [
    "# Classification performance assessment\n",
    "\n",
    "Rainey Aberle\n",
    "\n",
    "2022/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dbfb0e-ce5e-4fb9-b745-40c8cb4a2130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Import packages\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import rasterio as rio\n",
    "from shapely.geometry import Polygon\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce56d706-e737-4d8f-b8f1-a868cb822bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Define paths in directory\n",
    "# site_names = ['Emmons', 'LemonCreek']\n",
    "# base directory (path to snow-cover-mapping/)\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/'\n",
    "# path to classified points used to train and test classifiers\n",
    "data_pts_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/snow_cover_mapping/classified-points/assessment/'\n",
    "# full path and file name to AOI shapefile\n",
    "AOI_fns = [data_pts_path + '../../study-sites/LemonCreek/AOIs/LemonCreek_RGI_outline.shp',\n",
    "           data_pts_path + '../../study-sites/Emmons/AOIs/Emmons_RGI_outline.shp']\n",
    "# full path and file name to DEM\n",
    "DEM_fns = [data_pts_path + '../../study-sites/LemonCreek/DEMs/LemonCreek_ArcticDEM_clip.tif',\n",
    "           data_pts_path + '../../study-sites/Emmons/DEMs/Emmons_NASADEM_clip.tif']\n",
    "\n",
    "# -----Determine settings\n",
    "terrain_parameters = False # whether to use terrain parameters (elevation, slope, aspect) in classification\n",
    "save_figures = True # whether to save output figures\n",
    "\n",
    "# -----Add path to functions\n",
    "sys.path.insert(1, base_path + 'functions/')\n",
    "import pipeline_utils as f\n",
    "\n",
    "# -----Load dataset characteristics dictionary\n",
    "dataset_dict = json.load(open(base_path + 'inputs-outputs/datasets_characteristics.json'))\n",
    "\n",
    "# -----Load classified points\n",
    "os.chdir(data_pts_path)\n",
    "data_pts_fns = sorted(glob.glob('LemonCreek*.shp') + glob.glob('Emmons*.shp'))\n",
    "data_pts_fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80865d3c-9df8-466f-b147-f94a0bcc45b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_AOI_DEM(AOI_fn, DEM_fn):\n",
    "    # -----Load AOI as gpd.GeoDataFrame\n",
    "    AOI = gpd.read_file(AOI_fn)\n",
    "    # reproject the AOI to WGS to solve for the optimal UTM zone\n",
    "    AOI_WGS = AOI.to_crs('EPSG:4326')\n",
    "    AOI_WGS_centroid = [AOI_WGS.geometry[0].centroid.xy[0][0],\n",
    "                        AOI_WGS.geometry[0].centroid.xy[1][0]]\n",
    "    # grab the optimal UTM zone EPSG code\n",
    "    epsg_UTM = f.convert_wgs_to_utm(AOI_WGS_centroid[0], AOI_WGS_centroid[1])\n",
    "    print('Optimal UTM CRS = EPSG:' + str(epsg_UTM))\n",
    "    # -----Load DEM as Xarray DataSet\n",
    "    # reproject AOI to UTM\n",
    "    AOI_UTM = AOI.to_crs('EPSG:'+str(epsg_UTM))\n",
    "    # load DEM as xarray DataSet\n",
    "    DEM = xr.open_dataset(DEM_fn)\n",
    "    DEM = DEM.rename({'band_data': 'elevation'})\n",
    "    # reproject the DEM to the optimal UTM zone\n",
    "    DEM = DEM.rio.reproject('EPSG:'+str(epsg_UTM))\n",
    "    DEM = DEM.rio.write_crs('EPSG:'+str(epsg_UTM))\n",
    "    # remove unnecessary data (possible extra bands from ArcticDEM or other DEM)\n",
    "    if len(np.shape(DEM.elevation.data))>2:\n",
    "        DEM['elevation'] = DEM.elevation[0]\n",
    "    # -----Plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6,6))\n",
    "    dem_im = ax.imshow(DEM.elevation.data, cmap='terrain', \n",
    "              extent=(np.min(DEM.x.data)/1e3, np.max(DEM.x.data)/1e3, np.min(DEM.y.data)/1e3, np.max(DEM.y.data)/1e3))\n",
    "    if type(AOI_UTM.geometry[0])==Polygon:\n",
    "        ax.plot([x/1e3 for x in AOI_UTM.geometry[0].exterior.coords.xy[0]],\n",
    "                [y/1e3 for y in AOI_UTM.geometry[0].exterior.coords.xy[1]], '-k')\n",
    "    elif type(AOI_UTM.geometry[0])==MultiPolygon:\n",
    "        [ax.plot([x/1e3 for x in geom.exterior.coords.xy[0]],\n",
    "                [y/1e3 for y in geom.exterior.coords.xy[1]], '-k') for geom in AOI_UTM.geometry[0].geoms]\n",
    "    ax.grid()\n",
    "    ax.set_xlabel('Easting [km]')\n",
    "    ax.set_ylabel('Northing [km]')\n",
    "    fig.colorbar(dem_im, ax=ax, shrink=0.5, label='Elevation [m]')\n",
    "    plt.show()\n",
    "    return AOI_UTM, DEM, epsg_UTM\n",
    "\n",
    "AOIs, DEMs, epsg_UTMs = [], [], []\n",
    "for AOI_fn, DEM_fn in list(zip(AOI_fns, DEM_fns)):\n",
    "    AOI, DEM, epsg_UTM = load_AOI_DEM(AOI_fn, DEM_fn)\n",
    "    AOIs.append(AOI)\n",
    "    DEMs.append(DEM)\n",
    "    epsg_UTMs.append(epsg_UTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1943b536-8487-4684-b38c-8fe2add83163",
   "metadata": {},
   "source": [
    "## Loop through datasets, load images and classified points, calculate performance metrics, save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad70d141-d89c-465e-a506-c526b2f0ccad",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Define datasets\n",
    "datasets = ['Landsat', 'PlanetScope', 'Sentinel-2_SR', 'Sentinel-2_TOA']\n",
    "# load data point file names\n",
    "data_pts_snow_fns = sorted(glob.glob(data_pts_path+'*_snow.shp'))\n",
    "data_pts_no_snow_fns = sorted(glob.glob(data_pts_path+'*_no-snow.shp'))\n",
    "    \n",
    "# -----Loop through datasets\n",
    "results_df_full = pd.DataFrame()\n",
    "for dataset in datasets:\n",
    "    \n",
    "    print(dataset)\n",
    "    print('----------')\n",
    "\n",
    "    # -----Load trained classifier and feature columns\n",
    "    clf_fn = base_path+'inputs-outputs/'+dataset+'_classifier_all_sites.joblib'\n",
    "    clf = load(clf_fn)\n",
    "    feature_cols_fn = base_path+'inputs-outputs/'+dataset+'_feature_columns.json'\n",
    "    feature_cols = json.load(open(feature_cols_fn))\n",
    "\n",
    "    # -----Subset dataset dictionary\n",
    "    ds_dict = dataset_dict[dataset]\n",
    "\n",
    "    # -----Set up testing data\n",
    "    # load image file names\n",
    "    im_fns = sorted(glob.glob(data_pts_path + 'LemonCreek_'+dataset+'*.tif') \n",
    "                    + glob.glob(data_pts_path + 'Emmons_'+dataset+'*.tif'))\n",
    "    # grab dates from image file names\n",
    "    im_dates = [im_fn[-12:-4] for im_fn in im_fns]\n",
    "    # initialize full data frame\n",
    "    data_pts_full = gpd.GeoDataFrame()\n",
    "    \n",
    "    # loop through images\n",
    "    for i, im_fn in enumerate(im_fns):\n",
    "        \n",
    "        im_date = im_dates[i]\n",
    "        \n",
    "        if 'LemonCreek' in im_fn:\n",
    "            site_name = 'LemonCreek'\n",
    "        elif 'Emmons' in im_fn:\n",
    "            site_name = 'Emmons'\n",
    "\n",
    "        if site_name=='LemonCreek':\n",
    "            AOI_UTM = AOIs[0]\n",
    "            DEM = DEMs[0]\n",
    "            epsg_UTM = epsg_UTMs[0]\n",
    "        elif site_name=='Emmons':\n",
    "            AOI_UTM = AOIs[1]\n",
    "            DEM = DEMs[1]\n",
    "            epsg_UTM = epsg_UTMs[1]\n",
    "            \n",
    "        print(site_name, im_date)\n",
    "\n",
    "        # Load and pre-process image\n",
    "        if dataset=='PlanetScope':\n",
    "            im_xr = xr.open_dataset(im_fn)\n",
    "            # Adjust image radiometry\n",
    "            polygons_top, polygons_bottom = f.create_aoi_elev_polys(AOI_UTM, DEM)\n",
    "            im_ds, im_adj_method = f.planetscope_adjust_image_radiometry(im_xr, im_dt, polygons_top, polygons_bottom, dataset_dict, skip_clipped=False) \n",
    "        else:\n",
    "            # load image as xarray.DataArray\n",
    "            im_da = rxr.open_rasterio(im_fn)\n",
    "            # reproject to optimal UTM zone (if necessary)\n",
    "            im_da = im_da.rio.reproject('EPSG:' + str(epsg_UTM))\n",
    "            # convert to xarray.DataSet\n",
    "            im_ds = im_da.to_dataset('band')\n",
    "            band_names = list(ds_dict['refl_bands'].keys())\n",
    "            im_ds = im_ds.rename({i + 1: name for i, name in enumerate(band_names)})\n",
    "            # account for image scalar and no data values\n",
    "            im_ds = xr.where(im_ds != ds_dict['no_data_value'],\n",
    "                             im_ds / ds_dict['image_scalar'], np.nan)\n",
    "            # expand dimensions to include time\n",
    "            im_dt = np.datetime64(datetime.datetime.fromtimestamp(im_da.attrs['system-time_start'] / 1000))\n",
    "            im_ds = im_ds.expand_dims({'time': [im_dt]})\n",
    "            # set CRS\n",
    "            im_ds.rio.write_crs('EPSG:' + str(im_da.rio.crs.to_epsg()), inplace=True)\n",
    "            \n",
    "        # add NDSI band to image\n",
    "        im_ds['NDSI'] = ((im_ds[ds_dict['NDSI_bands'][0]] - im_ds[ds_dict['NDSI_bands'][1]])\n",
    "                         / (im_ds[ds_dict['NDSI_bands'][0]] + im_ds[ds_dict['NDSI_bands'][1]]))\n",
    "        # classify image \n",
    "        crop_to_aoi = False\n",
    "        im_classified_fn = site_name + '_' + dataset + '_' + im_date + '_classified.nc'\n",
    "        out_path = data_pts_path\n",
    "        if os.path.exists(out_path + im_classified_fn):\n",
    "            im_classified_xr = xr.open_dataset(out_path + im_classified_fn)\n",
    "            # remove no data values\n",
    "            im_classified_xr = xr.where(im_classified_xr==-9999, np.nan, im_classified_xr) \n",
    "            # set CRS\n",
    "            im_classified_xr.rio.write_crs('EPSG:' + str(im_da.rio.crs.to_epsg()), inplace=True)\n",
    "        else:\n",
    "            im_classified_xr = f.classify_image(im_ds, clf, feature_cols, crop_to_aoi, AOI_UTM, DEM, \n",
    "                                                dataset_dict, dataset, im_classified_fn, out_path)\n",
    "\n",
    "        print(im_classified_fn)\n",
    "        \n",
    "        # load classified points\n",
    "        # snow\n",
    "        data_pts_snow_fn = [x for x in data_pts_snow_fns if (im_date[0:6] in x) and (site_name in x)][0]\n",
    "        print(data_pts_snow_fn)\n",
    "        data_pts_snow = gpd.read_file(data_pts_snow_fn) # read file\n",
    "        data_pts_snow['class'] = 1 # determine class ID\n",
    "        # no-snow\n",
    "        if len([x for x in data_pts_no_snow_fns if im_date[0:6] in x])>0:\n",
    "            data_pts_no_snow_fn =[x for x in data_pts_no_snow_fns if (im_date[0:6] in x) and (site_name in x)][0]\n",
    "            print(data_pts_no_snow_fn)\n",
    "            data_pts_no_snow = gpd.read_file(data_pts_no_snow_fn) # read file\n",
    "            data_pts_no_snow['class'] = 0 # determine class ID\n",
    "            # combine data pts, reproject to image CRS\n",
    "            data_pts = pd.concat([data_pts_snow, data_pts_no_snow])\n",
    "        else: \n",
    "            data_pts = data_pts_snow\n",
    "        data_pts = data_pts.to_crs(im_ds.rio.crs)\n",
    "        # drop any NaN points\n",
    "        data_pts = data_pts.drop('id', axis=1)\n",
    "        data_pts = data_pts.dropna()\n",
    "        # grab x and y sample points\n",
    "        data_pts_x = [x.geoms[0].coords.xy[0][0] for x in data_pts.geometry]\n",
    "        data_pts_y = [x.geoms[0].coords.xy[1][0] for x in data_pts.geometry]\n",
    "        # sample image values at data points\n",
    "        for band in feature_cols[0:-1]:\n",
    "            data_pts[band] = [im_ds[band].sel(x=xx, y=yy, method='nearest').data[0] for xx, yy in list(zip(data_pts_x, data_pts_y))]\n",
    "        \n",
    "        # plot RGB and classified image with data points overlain\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(10,6))\n",
    "        ax[0].imshow(np.dstack([im_ds[ds_dict['RGB_bands'][0]].data[0], \n",
    "                              im_ds[ds_dict['RGB_bands'][1]].data[0], \n",
    "                              im_ds[ds_dict['RGB_bands'][2]].data[0]]),\n",
    "                     extent=(np.min(im_ds.x.data), np.max(im_ds.x.data),\n",
    "                             np.min(im_ds.y.data), np.max(im_ds.y.data)))\n",
    "        ax[0].plot([x.geoms[0].coords.xy[0] for x in data_pts.loc[data_pts['class'] == 1]['geometry']],\n",
    "                   [x.geoms[0].coords.xy[1] for x in data_pts.loc[data_pts['class'] == 1]['geometry']], '.c', markersize=2)\n",
    "        if len([x for x in data_pts_no_snow_fns if im_date[0:6] in x])>0:\n",
    "            ax[0].plot([x.geoms[0].coords.xy[0] for x in data_pts.loc[data_pts['class'] == 0]['geometry']],\n",
    "                       [x.geoms[0].coords.xy[1] for x in data_pts.loc[data_pts['class'] == 0]['geometry']], '.m', markersize=2)\n",
    "        ax[1].imshow(im_classified_xr.classified.data[0], clim=(2,3), cmap='cool',\n",
    "                     extent=(np.min(im_classified_xr.x.data), np.max(im_classified_xr.x.data),\n",
    "                             np.min(im_classified_xr.y.data), np.max(im_classified_xr.y.data)))\n",
    "        ax[1].plot([x.geoms[0].coords.xy[0] for x in data_pts.loc[data_pts['class'] == 1]['geometry']],\n",
    "                   [x.geoms[0].coords.xy[1] for x in data_pts.loc[data_pts['class'] == 1]['geometry']], '.c', markersize=2)\n",
    "        if len([x for x in data_pts_no_snow_fns if im_date[0:6] in x])>0:\n",
    "            ax[1].plot([x.geoms[0].coords.xy[0] for x in data_pts.loc[data_pts['class'] == 0]['geometry']],\n",
    "                       [x.geoms[0].coords.xy[1] for x in data_pts.loc[data_pts['class'] == 0]['geometry']], '.m', markersize=2)\n",
    "        plt.show()\n",
    "        \n",
    "        # reproject to WGS84 for compatibility\n",
    "        data_pts = data_pts.to_crs('EPSG:4326')\n",
    "        # concatenate to data_pts_full\n",
    "        data_pts_full = pd.concat([data_pts_full, data_pts])\n",
    "\n",
    "    # Add NDSI column\n",
    "    data_pts_full['NDSI'] = ((data_pts_full[ds_dict['NDSI_bands'][0]] - data_pts_full[ds_dict['NDSI_bands'][1]]) \n",
    "                              /(data_pts_full[ds_dict['NDSI_bands'][0]] + data_pts_full[ds_dict['NDSI_bands'][1]]) )\n",
    "\n",
    "    # Reduce memory usage in data pts\n",
    "    data_pts_full = data_pts_full.dropna().reset_index(drop=True)\n",
    "    data_pts_full = f.reduce_memory_usage(data_pts_full, verbose=False)\n",
    "\n",
    "    # -----Test the trained classifier\n",
    "    # features\n",
    "    X = data_pts_full[feature_cols] \n",
    "    # target variable\n",
    "    y = data_pts_full['class'] \n",
    "    # Predict class values using trained classifier\n",
    "    y_pred = clf.predict(X)\n",
    "    # Adjust outputs to only test snow and no-snow\n",
    "    y_pred[y_pred <= 2] = 1 # snow = 1, 2\n",
    "    y_pred[y_pred > 2] = 0 # no-snow = 3, 4, 5\n",
    "    # Calculate overall accuracy\n",
    "    accuracy = metrics.accuracy_score(y, y_pred)\n",
    "    # Calculate Kappa score\n",
    "    K = metrics.cohen_kappa_score(y, y_pred)\n",
    "    # Calculate recall\n",
    "    R = metrics.recall_score(y, y_pred)\n",
    "    # Calculate precision\n",
    "    P = metrics.precision_score(y, y_pred)\n",
    "    # Calculate F1 score\n",
    "    F1 = metrics.f1_score(y, y_pred)\n",
    "    # Calculate confusion matrix\n",
    "    CM = metrics.confusion_matrix(y, y_pred)\n",
    "    # print results\n",
    "    print('n = '+str(len(y_pred)))\n",
    "    print('Overall accuracy = '+str(accuracy))\n",
    "    print('Kappa score = '+str(K))\n",
    "    print('Recall = '+str(R))\n",
    "    print('Precision = '+str(P))\n",
    "    print('F1 score = '+str(F1))\n",
    "    print('Confusion matrix: ')\n",
    "    print(CM)\n",
    "    print(' ')\n",
    "    \n",
    "    # -----Add results to dataframe\n",
    "    results_df = pd.DataFrame({'Dataset': dataset,\n",
    "                               'Overall accuracy': accuracy,\n",
    "                               'Kappa score': K,\n",
    "                               'Recall': R,\n",
    "                               'Precision': P,\n",
    "                               'F1 score': F1,\n",
    "                               'Confusion matrix': [CM],\n",
    "                               'N': len(y_pred)\n",
    "                              })\n",
    "    results_df_full = pd.concat([results_df_full, results_df])\n",
    "\n",
    "    print(' ')\n",
    "\n",
    "# -----Save results to file\n",
    "results_fn = base_path + 'inputs-outputs/classification_performace_metrics.csv'\n",
    "results_df_full.to_csv(results_fn, index=False)\n",
    "print('Performance metrics saved to file: '+results_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfed4a34-8290-4506-a21b-f2f5a5c8572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geedim as gd\n",
    "# gd.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29da76ca-b800-473d-a54a-7fa70347fdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# site_name = 'LemonCreek'\n",
    "# i=0\n",
    "# AOI=AOIs[i]\n",
    "# AOI = AOI.to_crs('EPSG:'+epsg_UTMs[i])\n",
    "# AOI_buffer = AOI.buffer(2000)\n",
    "# AOI_WGS = AOI_buffer.to_crs('EPSG:4326')\n",
    "\n",
    "# region = {'type': 'Polygon',\n",
    "#           'coordinates': [[[AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.miny[0]],\n",
    "#                            [AOI_WGS.geometry.bounds.maxx[0], AOI_WGS.geometry.bounds.miny[0]],\n",
    "#                            [AOI_WGS.geometry.bounds.maxx[0], AOI_WGS.geometry.bounds.maxy[0]],\n",
    "#                            [AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.maxy[0]],\n",
    "#                            [AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.miny[0]]\n",
    "#                            ]]}\n",
    "# im_col = gd.MaskedCollection.from_name('LANDSAT/LC08/C02/T1_L2').search(start_date='2021-07-25',\n",
    "#                                                                         end_date='2021-07-31',\n",
    "#                                                                         region=region,\n",
    "#                                                                         fill_portion=70)\n",
    "# print(im_col.properties_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b957a8e9-ff31-48ae-981d-1b7fc10191a0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# im = gd.MaskedImage.from_id('LANDSAT/LC08/C02/T1_L2/LC08_058019_20210729', mask=False)\n",
    "# im.download(site_name+'_Landsat_20210729.tif', scale=30, region=region, bands=im.refl_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984a55d8-7107-4de4-bcc5-cc6f3ad0bc53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
