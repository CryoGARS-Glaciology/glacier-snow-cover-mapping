{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "573d9ef5-d92c-4099-8cd3-806ca9db29d9",
   "metadata": {},
   "source": [
    "# Classification accuracy assessment at Lemon Creek Glacier\n",
    "\n",
    "Rainey Aberle\n",
    "\n",
    "2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1dbfb0e-ce5e-4fb9-b745-40c8cb4a2130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Import packages\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import rasterio as rio\n",
    "from shapely.geometry import Polygon\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce56d706-e737-4d8f-b8f1-a868cb822bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LemonCreek_20210729_20_no-snow.shp',\n",
       " 'LemonCreek_20210729_20_snow.shp',\n",
       " 'LemonCreek_20210822_19_no-snow.shp',\n",
       " 'LemonCreek_20210822_19_snow.shp']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----Define paths in directory\n",
    "site_name = 'LemonCreek'\n",
    "# base directory (path to snow-cover-mapping/)\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/'\n",
    "# output folder for best classifier\n",
    "out_path = base_path + 'inputs-outputs/'\n",
    "# path to classified points used to train and test classifiers\n",
    "data_pts_path = base_path + '../classified-points/'\n",
    "# path to AOI shapefiles\n",
    "AOI_path = base_path + '../study-sites/' \n",
    "\n",
    "# -----Determine settings\n",
    "terrain_parameters = False # whether to use terrain parameters (elevation, slope, aspect) in classification\n",
    "save_figures = True # whether to save output figures\n",
    "\n",
    "# -----Add path to functions\n",
    "sys.path.insert(1, base_path + 'functions/')\n",
    "import pipeline_utils as f\n",
    "\n",
    "# -----Load dataset characteristics dictionary\n",
    "with open(base_path + 'inputs-outputs/datasets_characteristics.pkl', 'rb') as fn:\n",
    "    dataset_dict = pickle.load(fn)\n",
    "\n",
    "# -----Load classified points\n",
    "os.chdir(data_pts_path)\n",
    "data_pts_fns = glob.glob('*LemonCreek*.shp')\n",
    "data_pts_fns = sorted(data_pts_fns)\n",
    "data_pts_fns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1943b536-8487-4684-b38c-8fe2add83163",
   "metadata": {},
   "source": [
    "## PlanetScope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad70d141-d89c-465e-a506-c526b2f0ccad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 0.09 Mb (12.5% reduction)\n",
      "PlanetScope\n",
      "----------\n",
      "Overall accuracy: 0.9104104104104104\n",
      "Kappa score: 0.8210019709492689\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[864,  34],\n",
       "       [145, 955]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----Load trained classifier and feature columns\n",
    "clf_fn = base_path+'inputs-outputs/PS_classifier_all_sites.sav'\n",
    "clf = pickle.load(open(clf_fn, 'rb'))\n",
    "feature_cols_fn = base_path+'inputs-outputs/PS_feature_cols.pkl'\n",
    "feature_cols = pickle.load(open(feature_cols_fn,'rb'))\n",
    "\n",
    "# -----Subset dataset dictionary\n",
    "dataset = 'PlanetScope'\n",
    "ds_dict = dataset_dict[dataset]\n",
    "\n",
    "# -----Set up testing data\n",
    "# path to images\n",
    "im_path = base_path + '../snowline-package/' + site_name + '/images/'\n",
    "# determine number of images used for classified points\n",
    "num_images = len([s for s in data_pts_fns if (site_name in s) and ('_snow.shp' in s)])\n",
    "im_dates = [s[len(site_name)+1:len(site_name)+9] for s in data_pts_fns if (site_name in s) and ('_snow.shp' in s)]\n",
    "# loop through each image\n",
    "for j in range(0, num_images):\n",
    "    # determine image date\n",
    "    im_date = im_dates[j]\n",
    "    # load classified points\n",
    "    data_pts = pd.DataFrame() # dataframe to hold applicable data classes\n",
    "    # no-snow\n",
    "    if len([s for s in data_pts_fns if (site_name in s) and ('_no-snow.shp' in s) and (im_date in s)])>0: # check if class exists for site and date\n",
    "        data_pts_snow_fn = [s for s in data_pts_fns if (site_name in s) and ('_no-snow.shp' in s) and (im_date in s)][0]\n",
    "        data_pts_snow = gpd.read_file(data_pts_path + data_pts_snow_fn) # read file\n",
    "        data_pts_snow['class'] = 0 # determine class ID\n",
    "        data_pts = pd.concat([data_pts, data_pts_snow], ignore_index=True) # concatenate to full data points df\n",
    "        # print(data_pts_snow_fn)\n",
    "    # snow\n",
    "    if len([s for s in data_pts_fns if (site_name in s) and ('_snow.shp' in s) and (im_date in s)])>0: # check if class exists for site and date\n",
    "        data_pts_snow_fn = [s for s in data_pts_fns if (site_name in s) and ('_snow.shp' in s) and (im_date in s)][0]\n",
    "        data_pts_snow = gpd.read_file(data_pts_path + data_pts_snow_fn) # read file\n",
    "        data_pts_snow['class'] = 1 # determine class ID\n",
    "        data_pts = pd.concat([data_pts, data_pts_snow], ignore_index=True) # concatenate to full data points df\n",
    "        # print(data_pts_snow_fn)\n",
    "\n",
    "    # Load image\n",
    "    Idate = data_pts_snow_fn.index('_')+1\n",
    "    im_fn = data_pts_snow_fn[Idate:Idate+11]+'_adj.tif' # image file name\n",
    "    im_date = im_fn[0:4]+'-'+im_fn[4:6]+'-'+im_fn[6:8] # image capture date\n",
    "    im = xr.open_dataset(im_path + im_fn)\n",
    "    # remove no data values and account for image scalar\n",
    "    im = im.where(im!=-9999)\n",
    "    im = im / 1e4\n",
    "    # define bands\n",
    "    im['blue'] = im['band_data'][0]\n",
    "    im['green'] = im['band_data'][1]\n",
    "    im['red'] = im['band_data'][2]\n",
    "    im['NIR'] = im['band_data'][3]\n",
    "    im['NDSI'] = (im['green'] - im['NIR']) / (im['green'] + im['NIR'])    \n",
    "    \n",
    "    # reproject data points to image CRS\n",
    "    data_pts = data_pts.to_crs(im.rio.crs)\n",
    "    data_pts = data_pts.drop(columns=['id'])\n",
    "    data_pts = data_pts.dropna()\n",
    "    # grab x and y sample points\n",
    "    x = [x.geoms[0].coords.xy[0][0] for x in data_pts.geometry]\n",
    "    y = [x.geoms[0].coords.xy[1][0] for x in data_pts.geometry]\n",
    "    # sample image values at data points\n",
    "    for band in feature_cols:\n",
    "        data_pts[band] = [im[band].sel(x=x, y=y, method='nearest').data for x, y in list(zip(x, y))]\n",
    "    if j==0:\n",
    "        data_pts_full = data_pts\n",
    "    else:\n",
    "        data_pts_full = pd.concat([data_pts_full, data_pts]) \n",
    "# Reduce memory usage in data pts\n",
    "data_pts_full = data_pts_full.reset_index(drop=True)\n",
    "data_pts_full = f.reduce_memory_usage(data_pts_full)\n",
    "\n",
    "# -----Test the trained classifier\n",
    "# features\n",
    "X = data_pts_full[feature_cols] \n",
    "# target variable\n",
    "y = data_pts_full['class'] \n",
    "# Predict class values using trained classifier\n",
    "y_pred = clf.predict(X)\n",
    "# Adjust outputs to only test snow and no-snow\n",
    "y_pred[y_pred <= 2] = 1 # snow = 1, 2\n",
    "y_pred[y_pred > 2] = 0 # no-snow = 3, 4, 5\n",
    "# Calculate overall accuracy\n",
    "accuracy = metrics.accuracy_score(y, y_pred)\n",
    "# Calculate Kappa score\n",
    "K = metrics.cohen_kappa_score(y, y_pred)\n",
    "# Calculate confusion matrix\n",
    "CM = metrics.confusion_matrix(y, y_pred)\n",
    "# Print results\n",
    "print('PlanetScope')\n",
    "print('----------')\n",
    "print('n='+str(len(y_pred)))\n",
    "print('Overall accuracy: ' + str(accuracy))\n",
    "print('Kappa score: ' + str(K))\n",
    "print('Confusion matrix:')\n",
    "CM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73de939f-065e-4c8c-855e-13a33c31a10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20210729', '20210822']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62467144-2540-4b9e-807f-f244000d5bf6",
   "metadata": {},
   "source": [
    "## Landsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa0e0f1e-3388-45b0-97ac-009b3cc41135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2021-08-23T20:00:38.000000000']\n",
      "Mem. usage decreased to 0.05 Mb (9.7% reduction)\n",
      "Landsat\n",
      "----------\n",
      "Overall accuracy: 0.9038929440389294\n",
      "Kappa score: 0.8074507641125086\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[399,  18],\n",
       "       [ 61, 344]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----Load trained classifier and feature columns\n",
    "clf_fn = base_path+'inputs-outputs/L_classifier_all_sites.sav'\n",
    "clf = pickle.load(open(clf_fn, 'rb'))\n",
    "feature_cols_fn = base_path+'inputs-outputs/L_feature_cols.pkl'\n",
    "feature_cols = pickle.load(open(feature_cols_fn,'rb'))\n",
    "\n",
    "# -----Subset dataset dictionary\n",
    "dataset = 'Landsat'\n",
    "ds_dict = dataset_dict[dataset]\n",
    "\n",
    "# -----Set up testing data\n",
    "# load image\n",
    "im_path = base_path + '../study-sites/' + site_name + '/imagery/Landsat/masked/'\n",
    "ims_fn = glob.glob(im_path + '*masked.nc')\n",
    "im_fn = [im_fn for im_fn in ims_fn if '20210823' in im_fn][0]\n",
    "im = xr.open_dataset(im_fn)\n",
    "# load classified points\n",
    "PS_im_date = '20210822'\n",
    "data_pts = pd.DataFrame() # dataframe to hold applicable data classes\n",
    "# no-snow\n",
    "data_pts_snow_fn = [s for s in data_pts_fns if (site_name in s) and ('_no-snow.shp' in s) and (PS_im_date in s)][0]\n",
    "data_pts_snow = gpd.read_file(data_pts_path + data_pts_snow_fn) # read file\n",
    "data_pts_snow['class'] = 0 # determine class ID\n",
    "data_pts = pd.concat([data_pts, data_pts_snow], ignore_index=True) # concatenate to full data points df\n",
    "# print(data_pts_snow_fn)\n",
    "# snow\n",
    "data_pts_snow_fn = [s for s in data_pts_fns if (site_name in s) and ('_snow.shp' in s) and (PS_im_date in s)][0]\n",
    "data_pts_snow = gpd.read_file(data_pts_path + data_pts_snow_fn) # read file\n",
    "data_pts_snow['class'] = 1 # determine class ID\n",
    "data_pts = pd.concat([data_pts, data_pts_snow], ignore_index=True) # concatenate to full data points df\n",
    "# print(data_pts_snow_fn)\n",
    "# reproject data points to image CRS\n",
    "data_pts = data_pts.to_crs('EPSG:32608')\n",
    "data_pts = data_pts.drop(columns=['id'])\n",
    "data_pts = data_pts.dropna()\n",
    "# grab x and y sample points\n",
    "x = [x.geoms[0].coords.xy[0][0] for x in data_pts.geometry]\n",
    "y = [x.geoms[0].coords.xy[1][0] for x in data_pts.geometry]\n",
    "# sample image values at data points\n",
    "for band in feature_cols:\n",
    "    data_pts[band] = [im[band].sel(x=x, y=y, method='nearest').data for x, y in list(zip(x, y))]\n",
    "# remove rows with no data\n",
    "idrop = [~np.isnan(x) for x in data_pts['SR_B2']]\n",
    "data_pts = data_pts.iloc[idrop].reset_index(drop=True)\n",
    "    \n",
    "# reduce memory usage in data pts\n",
    "data_pts = f.reduce_memory_usage(data_pts)\n",
    "\n",
    "# -----Test the trained classifier\n",
    "# features\n",
    "X = data_pts[feature_cols] \n",
    "# target variable\n",
    "y = data_pts['class'] \n",
    "# Predict class values using trained classifier\n",
    "y_pred = clf.predict(X)\n",
    "# Adjust outputs to only test snow and no-snow\n",
    "y_pred[y_pred <= 2] = 1 # snow = 1, 2\n",
    "y_pred[y_pred > 2] = 0 # no-snow = 3, 4, 5\n",
    "# Calculate overall accuracy\n",
    "accuracy = metrics.accuracy_score(y, y_pred)\n",
    "# Calculate Kappa score\n",
    "K = metrics.cohen_kappa_score(y, y_pred)\n",
    "# Calculate confusion matrix\n",
    "CM = metrics.confusion_matrix(y, y_pred)\n",
    "# Print results\n",
    "print('Landsat')\n",
    "print('----------')\n",
    "print('Overall accuracy: ' + str(accuracy))\n",
    "print('Kappa score: ' + str(K))\n",
    "print('Confusion matrix:')\n",
    "CM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d0b27-a51c-4916-b2ad-cf044bfa53a1",
   "metadata": {},
   "source": [
    "## Sentinel-2 SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f113ef74-0bdb-441a-96a5-98368a995981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 0.15 Mb (7.3% reduction)\n",
      "Sentinel-2\n",
      "----------\n",
      "n=1824\n",
      "Overall accuracy: 0.9764254385964912\n",
      "Kappa score: 0.9516203052598854\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 744,    4],\n",
       "       [  39, 1037]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----Load images\n",
    "im_path = base_path + '../study-sites/' + site_name + '/imagery/Sentinel-2/masked/'\n",
    "im1_fn = im_path + 'LemonCreek_Sentinel2_20210730T202846_masked.nc'\n",
    "im1 = xr.open_dataset(im1_fn)\n",
    "im2_fn = im_path + 'LemonCreek_Sentinel2_20210831T201849_masked.nc'\n",
    "im2 = xr.open_dataset(im2_fn)\n",
    "im_list = [im1, im2]\n",
    "\n",
    "# -----Load trained classifier and feature columns\n",
    "clf_fn = base_path+'inputs-outputs/S2_classifier_all_sites.sav'\n",
    "clf = pickle.load(open(clf_fn, 'rb'))\n",
    "feature_cols_fn = base_path+'inputs-outputs/S2_feature_cols.pkl'\n",
    "feature_cols = pickle.load(open(feature_cols_fn,'rb'))\n",
    "\n",
    "# -----Subset dataset dictionary\n",
    "dataset = 'Sentinel2'\n",
    "ds_dict = dataset_dict[dataset]\n",
    "\n",
    "# -----Set up testing data\n",
    "for i, im in enumerate(im_list):\n",
    "    data_pts = pd.DataFrame() # dataframe to hold applicable data classes\n",
    "    # no-snow\n",
    "    data_pts_snow_fn = [s for s in data_pts_fns if (site_name in s) and ('_no-snow.shp' in s) and (im_dates[i] in s)][0]\n",
    "    data_pts_snow = gpd.read_file(data_pts_path + data_pts_snow_fn) # read file\n",
    "    data_pts_snow['class'] = 0 # determine class ID\n",
    "    data_pts = pd.concat([data_pts, data_pts_snow], ignore_index=True) # concatenate to full data points df\n",
    "    # print(data_pts_snow_fn)\n",
    "    # snow\n",
    "    data_pts_snow_fn = [s for s in data_pts_fns if (site_name in s) and ('_snow.shp' in s) and (im_dates[i] in s)][0]\n",
    "    data_pts_snow = gpd.read_file(data_pts_path + data_pts_snow_fn) # read file\n",
    "    data_pts_snow['class'] = 1 # determine class ID\n",
    "    data_pts = pd.concat([data_pts, data_pts_snow], ignore_index=True) # concatenate to full data points df\n",
    "    # print(data_pts_snow_fn)\n",
    "    # reproject data points to image CRS\n",
    "    data_pts = data_pts.to_crs('EPSG:32608')\n",
    "    data_pts = data_pts.drop(columns=['id'])\n",
    "    data_pts = data_pts.dropna()\n",
    "    # grab x and y sample points\n",
    "    x = [x.geoms[0].coords.xy[0][0] for x in data_pts.geometry]\n",
    "    y = [x.geoms[0].coords.xy[1][0] for x in data_pts.geometry]\n",
    "    # sample image values at data points\n",
    "    for band in feature_cols:\n",
    "        data_pts[band] = [im[band].sel(x=x, y=y, method='nearest').data for x, y in list(zip(x, y))]\n",
    "    # remove rows with no data\n",
    "    idrop = [~np.isnan(x) for x in data_pts['B2']]\n",
    "    data_pts = data_pts.iloc[idrop].reset_index(drop=True)\n",
    "    # concatenate to full data points\n",
    "    if i==0:\n",
    "        data_pts_full = data_pts\n",
    "    else:\n",
    "        data_pts_full = pd.concat([data_pts_full, data_pts])\n",
    "# reduce memory usage in data pts\n",
    "data_pts_full = f.reduce_memory_usage(data_pts_full)\n",
    "data_pts_full\n",
    "    \n",
    "# -----Test the trained classifier\n",
    "# features\n",
    "X = data_pts_full[feature_cols] \n",
    "# target variable\n",
    "y = data_pts_full['class'] \n",
    "# Predict class values using trained classifier\n",
    "y_pred = clf.predict(X)\n",
    "# Adjust outputs to only test snow and no-snow\n",
    "y_pred[y_pred <= 2] = 1 # snow = 1, 2\n",
    "y_pred[y_pred > 2] = 0 # no-snow = 3, 4, 5\n",
    "# Calculate overall accuracy\n",
    "accuracy = metrics.accuracy_score(y, y_pred)\n",
    "# Calculate Kappa score\n",
    "K = metrics.cohen_kappa_score(y, y_pred)\n",
    "# Calculate confusion matrix\n",
    "CM = metrics.confusion_matrix(y, y_pred)\n",
    "# Print results\n",
    "print('Sentinel-2')\n",
    "print('----------')\n",
    "print('n='+str(len(y_pred)))\n",
    "print('Overall accuracy: ' + str(accuracy))\n",
    "print('Kappa score: ' + str(K))\n",
    "print('Confusion matrix:')\n",
    "CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083e22e3-44e7-4e50-9275-0c72876e04db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planet-snow",
   "language": "python",
   "name": "planet-snow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
