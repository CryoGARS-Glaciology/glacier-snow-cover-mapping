{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "573d9ef5-d92c-4099-8cd3-806ca9db29d9",
   "metadata": {},
   "source": [
    "# Classification accuracy assessment\n",
    "\n",
    "Rainey Aberle\n",
    "\n",
    "2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dbfb0e-ce5e-4fb9-b745-40c8cb4a2130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Import packages\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import rasterio as rio\n",
    "from shapely.geometry import Polygon\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce56d706-e737-4d8f-b8f1-a868cb822bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Define paths in directory\n",
    "site_name = 'LemonCreek'\n",
    "# base directory (path to snow-cover-mapping/)\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/'\n",
    "# path to classified points used to train and test classifiers\n",
    "data_pts_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/snow_cover_mapping/classified-points/LemonCreek/'\n",
    "# full path and file name to AOI shapefile\n",
    "AOI_fn = data_pts_path + '../../study-sites/LemonCreek/AOIs/LemonCreek_RGI_outline.shp' \n",
    "# full path and file name to DEM\n",
    "DEM_fn = data_pts_path + '../../study-sites/LemonCreek/DEMs/LemonCreek_ArcticDEM_clip.tif' \n",
    "\n",
    "# -----Determine settings\n",
    "terrain_parameters = False # whether to use terrain parameters (elevation, slope, aspect) in classification\n",
    "save_figures = True # whether to save output figures\n",
    "\n",
    "# -----Add path to functions\n",
    "sys.path.insert(1, base_path + 'functions/')\n",
    "import pipeline_utils as f\n",
    "\n",
    "# -----Load dataset characteristics dictionary\n",
    "dataset_dict = json.load(open(base_path + 'inputs-outputs/datasets_characteristics.json'))\n",
    "\n",
    "# -----Load classified points\n",
    "os.chdir(data_pts_path)\n",
    "data_pts_fns = glob.glob('LemonCreek*.shp')\n",
    "data_pts_fns = sorted(data_pts_fns)\n",
    "data_pts_fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80865d3c-9df8-466f-b147-f94a0bcc45b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Load AOI as gpd.GeoDataFrame\n",
    "AOI = gpd.read_file(AOI_fn)\n",
    "# reproject the AOI to WGS to solve for the optimal UTM zone\n",
    "AOI_WGS = AOI.to_crs('EPSG:4326')\n",
    "AOI_WGS_centroid = [AOI_WGS.geometry[0].centroid.xy[0][0],\n",
    "                    AOI_WGS.geometry[0].centroid.xy[1][0]]\n",
    "# grab the optimal UTM zone EPSG code\n",
    "epsg_UTM = f.convert_wgs_to_utm(AOI_WGS_centroid[0], AOI_WGS_centroid[1])\n",
    "print('Optimal UTM CRS = EPSG:' + str(epsg_UTM))\n",
    "\n",
    "# -----Load DEM as Xarray DataSet\n",
    "# reproject AOI to UTM\n",
    "AOI_UTM = AOI.to_crs('EPSG:'+str(epsg_UTM))\n",
    "# load DEM as xarray DataSet\n",
    "DEM = xr.open_dataset(DEM_fn)\n",
    "DEM = DEM.rename({'band_data': 'elevation'})\n",
    "# reproject the DEM to the optimal UTM zone\n",
    "DEM = DEM.rio.reproject('EPSG:'+str(epsg_UTM))\n",
    "DEM = DEM.rio.write_crs('EPSG:'+str(epsg_UTM))\n",
    "# remove unnecessary data (possible extra bands from ArcticDEM or other DEM)\n",
    "if len(np.shape(DEM.elevation.data))>2:\n",
    "    DEM['elevation'] = DEM.elevation[0]\n",
    "    \n",
    "\n",
    "# -----Plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,6))\n",
    "dem_im = ax.imshow(DEM.elevation.data, cmap='terrain', \n",
    "          extent=(np.min(DEM.x.data)/1e3, np.max(DEM.x.data)/1e3, np.min(DEM.y.data)/1e3, np.max(DEM.y.data)/1e3))\n",
    "if type(AOI_UTM.geometry[0])==Polygon:\n",
    "    ax.plot([x/1e3 for x in AOI_UTM.geometry[0].exterior.coords.xy[0]],\n",
    "            [y/1e3 for y in AOI_UTM.geometry[0].exterior.coords.xy[1]], '-k')\n",
    "elif type(AOI_UTM.geometry[0])==MultiPolygon:\n",
    "    [ax.plot([x/1e3 for x in geom.exterior.coords.xy[0]],\n",
    "            [y/1e3 for y in geom.exterior.coords.xy[1]], '-k') for geom in AOI_UTM.geometry[0].geoms]\n",
    "ax.grid()\n",
    "ax.set_xlabel('Easting [km]')\n",
    "ax.set_ylabel('Northing [km]')\n",
    "fig.colorbar(dem_im, ax=ax, shrink=0.5, label='Elevation [m]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1943b536-8487-4684-b38c-8fe2add83163",
   "metadata": {},
   "source": [
    "## PlanetScope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad70d141-d89c-465e-a506-c526b2f0ccad",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Load trained classifier and feature columns\n",
    "clf_fn = base_path+'inputs-outputs/PlanetScope_classifier_all_sites.joblib'\n",
    "clf = load(clf_fn)\n",
    "feature_cols_fn = base_path+'inputs-outputs/PlanetScope_feature_columns.json'\n",
    "feature_cols = json.load(open(feature_cols_fn))\n",
    "\n",
    "# -----Subset dataset dictionary\n",
    "dataset = 'PlanetScope'\n",
    "ds_dict = dataset_dict[dataset]\n",
    "\n",
    "# -----Set up testing data\n",
    "# load image file names\n",
    "im_fns = sorted(glob.glob(data_pts_path + 'LemonCreek_'+dataset+'*.tif'))\n",
    "# load data point file names\n",
    "data_pts_snow_fns = sorted(glob.glob(data_pts_path+'*_snow.shp'))\n",
    "data_pts_no_snow_fns = sorted(glob.glob(data_pts_path+'*_no-snow.shp'))\n",
    "# initialize full data frame\n",
    "data_pts_full = gpd.GeoDataFrame()\n",
    "# loop through Landsat images\n",
    "for i, im_fn in enumerate(im_fns):\n",
    "\n",
    "    # Load image\n",
    "    im_fn = im_fns[i] # image file name\n",
    "    im_date = data_pts_snow_fns[i].split('LemonCreek_')[1][0:8]\n",
    "    im_dt = np.datetime64(im_date)\n",
    "    im_xr = xr.open_dataset(im_fn)\n",
    "    \n",
    "    # Adjust image radiometry\n",
    "    polygons_top, polygons_bottom = f.create_aoi_elev_polys(AOI_UTM, DEM)\n",
    "    im_adj, im_adj_method = f.planetscope_adjust_image_radiometry(im_xr, im_dt, polygons_top, polygons_bottom, dataset_dict, skip_clipped=False)  \n",
    "    \n",
    "    # load classified points\n",
    "    # no-snow\n",
    "    data_pts_no_snow_fn = data_pts_no_snow_fns[i]\n",
    "    data_pts_no_snow = gpd.read_file(data_pts_no_snow_fn) # read file\n",
    "    data_pts_no_snow['class'] = 0 # determine class ID\n",
    "    # snow\n",
    "    data_pts_snow_fn = data_pts_snow_fns[i]\n",
    "    data_pts_snow = gpd.read_file(data_pts_snow_fn) # read file\n",
    "    data_pts_snow['class'] = 1 # determine class ID\n",
    "    # combine data pts, reproject to image CRS\n",
    "    data_pts = pd.concat([data_pts_snow, data_pts_no_snow])\n",
    "    data_pts = data_pts.to_crs(im_xr.rio.crs)\n",
    "    # drop any NaN points\n",
    "    data_pts = data_pts.drop('id', axis=1)\n",
    "    data_pts = data_pts.dropna()\n",
    "\n",
    "    # grab x and y sample points\n",
    "    x = [x.geoms[0].coords.xy[0][0] for x in data_pts.geometry]\n",
    "    y = [x.geoms[0].coords.xy[1][0] for x in data_pts.geometry]\n",
    "    # sample image values at data points\n",
    "    for band in feature_cols[0:-1]:\n",
    "        data_pts[band] = [im_adj[band].sel(x=x, y=y, method='nearest').data[0] for x, y in list(zip(x, y))]\n",
    "    \n",
    "    # concatenate to full dataframe\n",
    "    data_pts_full = pd.concat([data_pts_full, data_pts])\n",
    "\n",
    "# Add NDSI column\n",
    "data_pts_full['NDSI'] = ((data_pts_full[ds_dict['NDSI_bands'][0]] - data_pts_full[ds_dict['NDSI_bands'][1]]) \n",
    "                          /(data_pts_full[ds_dict['NDSI_bands'][0]] + data_pts_full[ds_dict['NDSI_bands'][1]]) )\n",
    "\n",
    "# Reduce memory usage in data pts\n",
    "data_pts_full = data_pts_full.dropna().reset_index(drop=True)\n",
    "data_pts_full = f.reduce_memory_usage(data_pts_full, verbose=False)\n",
    "\n",
    "# -----Test the trained classifier\n",
    "# features\n",
    "X = data_pts_full[feature_cols] \n",
    "# target variable\n",
    "y = data_pts_full['class'] \n",
    "# Predict class values using trained classifier\n",
    "y_pred = clf.predict(X)\n",
    "# Adjust outputs to only test snow and no-snow\n",
    "y_pred[y_pred <= 2] = 1 # snow = 1, 2\n",
    "y_pred[y_pred > 2] = 0 # no-snow = 3, 4, 5\n",
    "# Calculate overall accuracy\n",
    "accuracy = metrics.accuracy_score(y, y_pred)\n",
    "# Calculate Kappa score\n",
    "K = metrics.cohen_kappa_score(y, y_pred)\n",
    "# Calculate confusion matrix\n",
    "CM = metrics.confusion_matrix(y, y_pred)\n",
    "# print results\n",
    "print('n = '+str(len(y_pred)))\n",
    "print('Overall accuracy = '+str(accuracy))\n",
    "print('Kappa score = '+str(K))\n",
    "print('Confusion matrix: ')\n",
    "print(CM)\n",
    "\n",
    "# -----Save results to file\n",
    "results = {'Dataset': dataset,\n",
    "           'Overall accuracy': accuracy,\n",
    "           'Kappa score': K,\n",
    "           'Confusion matrix': {\n",
    "               'TP': str(CM[0,0]),\n",
    "               'TN': str(CM[1,1]),\n",
    "               'FP': str(CM[0,1]),\n",
    "               'FN': str(CM[1,0])\n",
    "           },\n",
    "           'N': len(y_pred)\n",
    "          }\n",
    "results_fn = base_path + 'inputs-outputs/classification_performace_metrics_'+dataset+'.json'\n",
    "json.dump( results, open( results_fn, 'w' ) )\n",
    "print('Performance metrics saved to file: '+results_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62467144-2540-4b9e-807f-f244000d5bf6",
   "metadata": {},
   "source": [
    "## Landsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0e0f1e-3388-45b0-97ac-009b3cc41135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Load trained classifier and feature columns\n",
    "clf_fn = base_path+'inputs-outputs/Landsat_classifier_all_sites.joblib'\n",
    "clf = load(clf_fn)\n",
    "feature_cols_fn = base_path+'inputs-outputs/Landsat_feature_columns.json'\n",
    "feature_cols = json.load(open(feature_cols_fn))\n",
    "\n",
    "# -----Subset dataset dictionary\n",
    "dataset = 'Landsat'\n",
    "ds_dict = dataset_dict[dataset]\n",
    "\n",
    "# -----Set up testing data\n",
    "# load image file names\n",
    "im_fns = sorted(glob.glob(data_pts_path + 'LemonCreek_'+dataset+'*.tif'))\n",
    "# load data point file names\n",
    "data_pts_snow_fns = sorted(glob.glob(data_pts_path+'*_snow.shp'))\n",
    "data_pts_no_snow_fns = sorted(glob.glob(data_pts_path+'*_no-snow.shp'))\n",
    "# initialize full data frame\n",
    "data_pts_full = gpd.GeoDataFrame()\n",
    "# loop through Landsat images\n",
    "for i, im_fn in enumerate(im_fns):\n",
    "    \n",
    "    # load image as xarray.DataArray\n",
    "    im_da = rxr.open_rasterio(im_fn)\n",
    "    # reproject to optimal UTM zone (if necessary)\n",
    "    im_da = im_da.rio.reproject('EPSG:' + str(epsg_UTM))\n",
    "    # convert to xarray.DataSet\n",
    "    im_ds = im_da.to_dataset('band')\n",
    "    band_names = list(ds_dict['refl_bands'].keys())\n",
    "    im_ds = im_ds.rename({i + 1: name for i, name in enumerate(band_names)})\n",
    "    # account for image scalar and no data values\n",
    "    im_ds = xr.where(im_ds != ds_dict['no_data_value'],\n",
    "                     im_ds / ds_dict['image_scalar'], np.nan)\n",
    "    # expand dimensions to include time\n",
    "    im_dt = np.datetime64(datetime.datetime.fromtimestamp(im_da.attrs['system-time_start'] / 1000))\n",
    "    im_ds = im_ds.expand_dims({'time': [im_dt]})\n",
    "    # set CRS\n",
    "    im_ds.rio.write_crs('EPSG:' + str(im_da.rio.crs.to_epsg()), inplace=True)\n",
    "    \n",
    "    # load classified points\n",
    "    # no-snow\n",
    "    data_pts_no_snow_fn = data_pts_no_snow_fns[i]\n",
    "    data_pts_no_snow = gpd.read_file(data_pts_no_snow_fn) # read file\n",
    "    data_pts_no_snow['class'] = 0 # determine class ID\n",
    "    # snow\n",
    "    data_pts_snow_fn = data_pts_snow_fns[i]\n",
    "    data_pts_snow = gpd.read_file(data_pts_snow_fn) # read file\n",
    "    data_pts_snow['class'] = 1 # determine class ID\n",
    "    # combine data pts, reproject to image CRS\n",
    "    data_pts = pd.concat([data_pts_snow, data_pts_no_snow])\n",
    "    data_pts = data_pts.to_crs('EPSG:'+str(im_da.rio.crs.to_epsg()))\n",
    "    # drop any NaN points\n",
    "    data_pts = data_pts.drop('id', axis=1)\n",
    "    data_pts = data_pts.dropna()\n",
    "    # grab x and y sample points\n",
    "    x = [x.geoms[0].coords.xy[0][0] for x in data_pts.geometry]\n",
    "    y = [x.geoms[0].coords.xy[1][0] for x in data_pts.geometry]\n",
    "    # sample image values at data points\n",
    "    for band in feature_cols[0:-1]:\n",
    "        data_pts[band] = [im_ds[band].sel(x=x, y=y, method='nearest').data[0] for x, y in list(zip(x, y))]\n",
    "    # concatenate to data_pts_full\n",
    "    data_pts_full = pd.concat([data_pts_full, data_pts])\n",
    "\n",
    "\n",
    "# add NDSI column\n",
    "data_pts_full['NDSI'] = ((data_pts_full[ds_dict['NDSI_bands'][0]] - data_pts_full[ds_dict['NDSI_bands'][1]]) \n",
    "                            /(data_pts_full[ds_dict['NDSI_bands'][0]] + data_pts_full[ds_dict['NDSI_bands'][1]]) )\n",
    "# remove rows with no data\n",
    "data_pts_full = data_pts_full.dropna().reset_index(drop=True)\n",
    "# reduce memory usage in data pts\n",
    "data_pts_full = f.reduce_memory_usage(data_pts_full)\n",
    "\n",
    "# -----Test the trained classifier\n",
    "# features\n",
    "X = data_pts_full[feature_cols] \n",
    "# target variable\n",
    "y = data_pts_full['class'].values\n",
    "# Predict class values using trained classifier\n",
    "y_pred = clf.predict(X)\n",
    "# Adjust outputs to only test snow and no-snow\n",
    "y_pred[y_pred <= 2] = 1 # snow = 1, 2\n",
    "y_pred[y_pred > 2] = 0 # no-snow = 3, 4, 5\n",
    "# Calculate overall accuracy\n",
    "accuracy = metrics.accuracy_score(y, y_pred)\n",
    "# Calculate Kappa score\n",
    "K = metrics.cohen_kappa_score(y, y_pred)\n",
    "# Calculate confusion matrix\n",
    "CM = metrics.confusion_matrix(y, y_pred)\n",
    "# print results\n",
    "print('n = '+str(len(y_pred)))\n",
    "print('Overall accuracy = '+str(accuracy))\n",
    "print('Kappa score = '+str(K))\n",
    "print('Confusion matrix: ')\n",
    "print(CM)\n",
    "\n",
    "# -----Save results to file\n",
    "results = {'Dataset': dataset,\n",
    "           'Overall accuracy': accuracy,\n",
    "           'Kappa score': K,\n",
    "           'Confusion matrix': {\n",
    "               'TP': str(CM[0,0]),\n",
    "               'TN': str(CM[1,1]),\n",
    "               'FP': str(CM[0,1]),\n",
    "               'FN': str(CM[1,0])\n",
    "           },\n",
    "           'N': len(y_pred)\n",
    "          }\n",
    "results_fn = base_path + 'inputs-outputs/classification_performace_metrics_Landsat.json'\n",
    "json.dump( results, open( results_fn, 'w' ) )\n",
    "print('Performance metrics saved to file: '+results_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7cc126-38cd-4a58-a481-91ee009de6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = f.plot_xr_rgb_image(im_ds, ds_dict['RGB_bands'])\n",
    "ax.plot([x.geoms[0].coords.xy[0][0]/1e3 for x in data_pts.loc[data_pts['class']==1, 'geometry']],\n",
    "         [x.geoms[0].coords.xy[1][0]/1e3 for x in data_pts.loc[data_pts['class']==1, 'geometry']], '.c')\n",
    "ax.plot([x.geoms[0].coords.xy[0][0]/1e3 for x in data_pts.loc[data_pts['class']==0, 'geometry']],\n",
    "         [x.geoms[0].coords.xy[1][0]/1e3 for x in data_pts.loc[data_pts['class']==0, 'geometry']], \n",
    "         'o', markerfacecolor='none', markeredgecolor='m')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d0b27-a51c-4916-b2ad-cf044bfa53a1",
   "metadata": {},
   "source": [
    "## Sentinel-2 SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f113ef74-0bdb-441a-96a5-98368a995981",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Load trained classifier and feature columns\n",
    "clf_fn = base_path+'inputs-outputs/Sentinel-2_SR_classifier_all_sites.joblib'\n",
    "clf = load(clf_fn)\n",
    "feature_cols_fn = base_path+'inputs-outputs/Sentinel-2_SR_feature_columns.json'\n",
    "feature_cols = json.load(open(feature_cols_fn))\n",
    "\n",
    "# -----Subset dataset dictionary\n",
    "dataset = 'Sentinel-2_SR'\n",
    "ds_dict = dataset_dict[dataset]\n",
    "\n",
    "# -----Set up testing data\n",
    "# load image file names\n",
    "im_fns = sorted(glob.glob(data_pts_path + 'LemonCreek_'+dataset+'*.tif'))\n",
    "# load data point file names\n",
    "data_pts_snow_fns = sorted(glob.glob(data_pts_path+'*_snow.shp'))\n",
    "data_pts_no_snow_fns = sorted(glob.glob(data_pts_path+'*_no-snow.shp'))\n",
    "# initialize full data frame\n",
    "data_pts_full = gpd.GeoDataFrame()\n",
    "# loop through Landsat images\n",
    "for i, im_fn in enumerate(im_fns):\n",
    "    \n",
    "    # load image as xarray.DataArray\n",
    "    im_da = rxr.open_rasterio(im_fn)\n",
    "    # reproject to optimal UTM zone (if necessary)\n",
    "    im_da = im_da.rio.reproject('EPSG:' + str(epsg_UTM))\n",
    "    # convert to xarray.DataSet\n",
    "    im_ds = im_da.to_dataset('band')\n",
    "    band_names = list(ds_dict['refl_bands'].keys())\n",
    "    im_ds = im_ds.rename({i + 1: name for i, name in enumerate(band_names)})\n",
    "    # account for image scalar and no data values\n",
    "    im_ds = xr.where(im_ds != ds_dict['no_data_value'],\n",
    "                     im_ds / ds_dict['image_scalar'], np.nan)\n",
    "    # expand dimensions to include time\n",
    "    im_dt = np.datetime64(datetime.datetime.fromtimestamp(im_da.attrs['system-time_start'] / 1000))\n",
    "    im_ds = im_ds.expand_dims({'time': [im_dt]})\n",
    "    # set CRS\n",
    "    im_ds.rio.write_crs('EPSG:' + str(im_da.rio.crs.to_epsg()), inplace=True)\n",
    "    \n",
    "    # load classified points\n",
    "    # no-snow\n",
    "    data_pts_no_snow_fn = data_pts_no_snow_fns[i]\n",
    "    data_pts_no_snow = gpd.read_file(data_pts_no_snow_fn) # read file\n",
    "    data_pts_no_snow['class'] = 0 # determine class ID\n",
    "    # snow\n",
    "    data_pts_snow_fn = data_pts_snow_fns[i]\n",
    "    data_pts_snow = gpd.read_file(data_pts_snow_fn) # read file\n",
    "    data_pts_snow['class'] = 1 # determine class ID\n",
    "    # combine data pts, reproject to image CRS\n",
    "    data_pts = pd.concat([data_pts_snow, data_pts_no_snow])\n",
    "    data_pts = data_pts.to_crs('EPSG:'+str(im_da.rio.crs.to_epsg()))\n",
    "    # drop any NaN points\n",
    "    data_pts = data_pts.drop('id', axis=1)\n",
    "    data_pts = data_pts.dropna()\n",
    "    # grab x and y sample points\n",
    "    x = [x.geoms[0].coords.xy[0][0] for x in data_pts.geometry]\n",
    "    y = [x.geoms[0].coords.xy[1][0] for x in data_pts.geometry]\n",
    "    # sample image values at data points\n",
    "    for band in feature_cols[0:-1]:\n",
    "        data_pts[band] = [im_ds[band].sel(x=x, y=y, method='nearest').data[0] for x, y in list(zip(x, y))]\n",
    "    # concatenate to data_pts_full\n",
    "    data_pts_full = pd.concat([data_pts_full, data_pts])\n",
    "\n",
    "# add NDSI column\n",
    "data_pts_full['NDSI'] = ((data_pts_full[ds_dict['NDSI_bands'][0]] - data_pts_full[ds_dict['NDSI_bands'][1]]) \n",
    "                            /(data_pts_full[ds_dict['NDSI_bands'][0]] + data_pts_full[ds_dict['NDSI_bands'][1]]) )\n",
    "# remove rows with no data\n",
    "data_pts_full = data_pts_full.dropna().reset_index(drop=True)\n",
    "# reduce memory usage in data pts\n",
    "data_pts_full = f.reduce_memory_usage(data_pts_full, verbose=False)\n",
    "\n",
    "# -----Test the trained classifier\n",
    "# features\n",
    "X = data_pts_full[feature_cols] \n",
    "# target variable\n",
    "y = data_pts_full['class'].values\n",
    "# Predict class values using trained classifier\n",
    "y_pred = clf.predict(X)\n",
    "# Adjust outputs to only test snow and no-snow\n",
    "y_pred[y_pred <= 2] = 1 # snow = 1, 2\n",
    "y_pred[y_pred > 2] = 0 # no-snow = 3, 4, 5\n",
    "# Calculate overall accuracy\n",
    "accuracy = metrics.accuracy_score(y, y_pred)\n",
    "# Calculate Kappa score\n",
    "K = metrics.cohen_kappa_score(y, y_pred)\n",
    "# Calculate confusion matrix\n",
    "CM = metrics.confusion_matrix(y, y_pred)\n",
    "# print results\n",
    "print('n = '+str(len(y_pred)))\n",
    "print('Overall accuracy = '+str(accuracy))\n",
    "print('Kappa score = '+str(K))\n",
    "print('Confusion matrix: ')\n",
    "print(CM)\n",
    "\n",
    "# -----Save results to file\n",
    "results = {'Dataset': dataset,\n",
    "           'Overall accuracy': accuracy,\n",
    "           'Kappa score': K,\n",
    "           'Confusion matrix': {\n",
    "               'TP': str(CM[0,0]),\n",
    "               'TN': str(CM[1,1]),\n",
    "               'FP': str(CM[0,1]),\n",
    "               'FN': str(CM[1,0])\n",
    "           },\n",
    "           'N': len(y_pred)\n",
    "          }\n",
    "results_fn = base_path + 'inputs-outputs/classification_performace_metrics_'+dataset+'.json'\n",
    "json.dump( results, open( results_fn, 'w' ) )\n",
    "print('Performance metrics saved to file: '+results_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10db20e-235e-4987-8412-5c75906dfe08",
   "metadata": {},
   "source": [
    "## Sentinel-2 TOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f22714-0eb5-4084-8567-c1265176724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Load trained classifier and feature columns\n",
    "clf_fn = base_path+'inputs-outputs/Sentinel-2_TOA_classifier_all_sites.joblib'\n",
    "clf = load(clf_fn)\n",
    "feature_cols_fn = base_path+'inputs-outputs/Sentinel-2_TOA_feature_columns.json'\n",
    "feature_cols = json.load(open(feature_cols_fn))\n",
    "\n",
    "# -----Subset dataset dictionary\n",
    "dataset = 'Sentinel-2_TOA'\n",
    "ds_dict = dataset_dict[dataset]\n",
    "\n",
    "# -----Set up testing data\n",
    "# load image file names\n",
    "im_fns = sorted(glob.glob(data_pts_path + 'LemonCreek_'+dataset+'*.tif'))\n",
    "# load data point file names\n",
    "data_pts_snow_fns = sorted(glob.glob(data_pts_path+'*_snow.shp'))\n",
    "data_pts_no_snow_fns = sorted(glob.glob(data_pts_path+'*_no-snow.shp'))\n",
    "# initialize full data frame\n",
    "data_pts_full = gpd.GeoDataFrame()\n",
    "# loop through Landsat images\n",
    "for i, im_fn in enumerate(im_fns):\n",
    "    \n",
    "    # load image as xarray.DataArray\n",
    "    im_da = rxr.open_rasterio(im_fn)\n",
    "    # reproject to optimal UTM zone (if necessary)\n",
    "    im_da = im_da.rio.reproject('EPSG:' + str(epsg_UTM))\n",
    "    # convert to xarray.DataSet\n",
    "    im_ds = im_da.to_dataset('band')\n",
    "    band_names = list(ds_dict['refl_bands'].keys())\n",
    "    im_ds = im_ds.rename({i + 1: name for i, name in enumerate(band_names)})\n",
    "    # account for image scalar and no data values\n",
    "    im_ds = xr.where(im_ds != ds_dict['no_data_value'],\n",
    "                     im_ds / ds_dict['image_scalar'], np.nan)\n",
    "    # expand dimensions to include time\n",
    "    im_dt = np.datetime64(datetime.datetime.fromtimestamp(im_da.attrs['system-time_start'] / 1000))\n",
    "    im_ds = im_ds.expand_dims({'time': [im_dt]})\n",
    "    # set CRS\n",
    "    im_ds.rio.write_crs('EPSG:' + str(im_da.rio.crs.to_epsg()), inplace=True)\n",
    "    \n",
    "    # load classified points\n",
    "    # no-snow\n",
    "    data_pts_no_snow_fn = data_pts_no_snow_fns[i]\n",
    "    data_pts_no_snow = gpd.read_file(data_pts_no_snow_fn) # read file\n",
    "    data_pts_no_snow['class'] = 0 # determine class ID\n",
    "    # snow\n",
    "    data_pts_snow_fn = data_pts_snow_fns[i]\n",
    "    data_pts_snow = gpd.read_file(data_pts_snow_fn) # read file\n",
    "    data_pts_snow['class'] = 1 # determine class ID\n",
    "    # combine data pts, reproject to image CRS\n",
    "    data_pts = pd.concat([data_pts_snow, data_pts_no_snow])\n",
    "    data_pts = data_pts.to_crs('EPSG:'+str(im_da.rio.crs.to_epsg()))\n",
    "    # drop any NaN points\n",
    "    data_pts = data_pts.drop('id', axis=1)\n",
    "    data_pts = data_pts.dropna()\n",
    "    # grab x and y sample points\n",
    "    x = [x.geoms[0].coords.xy[0][0] for x in data_pts.geometry]\n",
    "    y = [x.geoms[0].coords.xy[1][0] for x in data_pts.geometry]\n",
    "    # sample image values at data points\n",
    "    for band in feature_cols[0:-1]:\n",
    "        data_pts[band] = [im_ds[band].sel(x=x, y=y, method='nearest').data[0] for x, y in list(zip(x, y))]\n",
    "    # concatenate to data_pts_full\n",
    "    data_pts_full = pd.concat([data_pts_full, data_pts])\n",
    "\n",
    "# add NDSI column\n",
    "data_pts_full['NDSI'] = ((data_pts_full[ds_dict['NDSI_bands'][0]] - data_pts_full[ds_dict['NDSI_bands'][1]]) \n",
    "                            /(data_pts_full[ds_dict['NDSI_bands'][0]] + data_pts_full[ds_dict['NDSI_bands'][1]]) )\n",
    "# remove rows with no data\n",
    "data_pts_full = data_pts_full.dropna().reset_index(drop=True)\n",
    "# reduce memory usage in data pts\n",
    "data_pts_full = f.reduce_memory_usage(data_pts_full, verbose=False)\n",
    "\n",
    "# -----Test the trained classifier\n",
    "# features\n",
    "X = data_pts_full[feature_cols] \n",
    "# target variable\n",
    "y = data_pts_full['class'].values\n",
    "# Predict class values using trained classifier\n",
    "y_pred = clf.predict(X)\n",
    "# Adjust outputs to only test snow and no-snow\n",
    "y_pred[y_pred <= 2] = 1 # snow = 1, 2\n",
    "y_pred[y_pred > 2] = 0 # no-snow = 3, 4, 5\n",
    "# Calculate overall accuracy\n",
    "accuracy = metrics.accuracy_score(y, y_pred)\n",
    "# Calculate Kappa score\n",
    "K = metrics.cohen_kappa_score(y, y_pred)\n",
    "# Calculate confusion matrix\n",
    "CM = metrics.confusion_matrix(y, y_pred)\n",
    "# print results\n",
    "print('n = '+str(len(y_pred)))\n",
    "print('Overall accuracy = '+str(accuracy))\n",
    "print('Kappa score = '+str(K))\n",
    "print('Confusion matrix: ')\n",
    "print(CM)\n",
    "\n",
    "# -----Save results to file\n",
    "results = {'Dataset': dataset,\n",
    "           'Overall accuracy': accuracy,\n",
    "           'Kappa score': K,\n",
    "           'Confusion matrix': {\n",
    "               'TP': str(CM[0,0]),\n",
    "               'TN': str(CM[1,1]),\n",
    "               'FP': str(CM[0,1]),\n",
    "               'FN': str(CM[1,0])\n",
    "           },\n",
    "           'N': len(y_pred)\n",
    "          }\n",
    "results_fn = base_path + 'inputs-outputs/classification_performace_metrics_'+dataset+'.json'\n",
    "json.dump( results, open( results_fn, 'w' ) )\n",
    "print('Performance metrics saved to file: '+results_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115d4b91-400d-4b08-93c5-3655647e3795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
