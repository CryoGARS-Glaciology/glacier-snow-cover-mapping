{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16794d14-1223-48e3-8dd8-a9c749a7b2d8",
   "metadata": {},
   "source": [
    "# Filter snowline time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e7d881-cb28-43f2-bb00-85bc6465a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MODIFY HERE #####\n",
    "# path to snow-cover-mapping\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/'\n",
    "# path where figures will be saved\n",
    "figures_out_path = base_path + 'figures/'\n",
    "#######################\n",
    "\n",
    "# import packages\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import iqr\n",
    "from time import mktime\n",
    "import seaborn as sns\n",
    "from scipy.stats import median_abs_deviation as mad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5f214b-2263-41e3-97f5-27fdb35171fc",
   "metadata": {},
   "source": [
    "### Stack observations by month, filter points using the monthly distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e875b2-744d-4899-a2f9-0892aca09e68",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Identify site names (used in folder names)\n",
    "site_names = ['Wolverine', 'Gulkana', 'LemonCreek', 'SouthCascade', 'Sperry']\n",
    "\n",
    "# -----Loop through sites\n",
    "for i, site_name in enumerate(site_names):\n",
    "    \n",
    "    print(site_name)\n",
    "    \n",
    "    # -----Load snowlines    \n",
    "    \n",
    "    ###########################\n",
    "    ### MODIFY IF NECESSARY ###\n",
    "    # path to snowline pkl files\n",
    "    sl_est_path = base_path + '../study-sites/' + site_name + '/imagery/snowlines/' \n",
    "    # path where filtered snowlines will be saved\n",
    "    out_path = sl_est_path \n",
    "    # path to USGS mass balance data/ELA csvs (if no USGS files, set usgs_path=None)\n",
    "    usgs_path = '/Volumes/GoogleDrive/My Drive/Research/PhD/GIS_data/USGS/benchmarkGlacier_massBalance/'\n",
    "    ###########################\n",
    "    \n",
    "    sl_est_fns = glob.glob(sl_est_path + '*snowline.pkl')\n",
    "    # compile all snowline files into one DataFrame\n",
    "    sl_est_full = pd.DataFrame()\n",
    "    for fn in sl_est_fns:\n",
    "        sl_est = pd.read_pickle(fn) # read file\n",
    "        sl_est_full = pd.concat([sl_est_full, sl_est]) # concatenate to df\n",
    "    sl_est_full = sl_est_full.reset_index(drop=True).sort_values(by=['datetime']) # renumber, sort by date\n",
    "    \n",
    "    # -----Reformat snowlines dataframes\n",
    "    # unify datetime datatypes\n",
    "    sl_est_full['datetime'] = sl_est_full['datetime'].astype(np.datetime64)\n",
    "    # add month column\n",
    "    sl_est_full['month'] = [x.month for x in sl_est_full['datetime']]\n",
    "    # extract all unique months\n",
    "    months = np.unique(sl_est_full['month'])  \n",
    "    # set datetime as index\n",
    "    sl_est_full.index = sl_est_full['datetime']\n",
    "    \n",
    "    # remove PlanetScope\n",
    "    # sl_est_full = sl_est_full.mask(sl_est_full['dataset']=='PlanetScope').dropna()\n",
    " \n",
    "    # ----Set up figure\n",
    "    fig = plt.figure(figsize=(20, 6))\n",
    "    plt.rcParams.update({'font.size':18, 'font.sans-serif':'Arial'})\n",
    "    spec = fig.add_gridspec(ncols=2, nrows=1, width_ratios=[1, 2.5])\n",
    "    ax1 = fig.add_subplot(spec[0, 0])\n",
    "    ax1.set_xlabel('Month')\n",
    "    ax1.set_ylabel('Median snowline elevations [m]')\n",
    "    ax1.grid()\n",
    "    ax2 = fig.add_subplot(spec[0, 1])\n",
    "    ax2.set_xlabel('Date')\n",
    "    ax2.grid()\n",
    "\n",
    "    # -----Filter points using median and IQR trend\n",
    "    med = np.array([np.nanmedian(sl_est_full.loc[sl_est_full['month']==month]['snowlines_elevs_median']) for month in months])\n",
    "    # mean = np.array([np.nanmean(sl_est_full.loc[sl_est_full['month']==month]['snowlines_elevs_median']) for month in months])\n",
    "    # std = np.array([np.nanstd(sl_est_full.loc[sl_est_full['month']==month]['snowlines_elevs_median']) for month in months])\n",
    "    # MAD = np.array([mad(sl_est_full.loc[sl_est_full['month']==month]['snowlines_elevs_median'], nan_policy='omit') for month in months])\n",
    "    IQR_min = np.array([iqr(sl_est_full.loc[sl_est_full['month']==month]['snowlines_elevs_median'], \n",
    "                            rng=(25,50), nan_policy='omit') for month in months])\n",
    "    # if minimum goes below the glacier elevation range, make it the minimum elevation\n",
    "    IQR_min[IQR_min < np.nanmin(sl_est_full['snowlines_elevs_median'])]== np.nanmin(sl_est_full['snowlines_elevs_median'])\n",
    "    IQR_max = np.array([iqr(sl_est_full.loc[sl_est_full['month']==month]['snowlines_elevs_median'], \n",
    "                            rng=(50,75), nan_policy='omit')*3 for month in months])\n",
    "    # if the IQR_max = IQR_min, increase the max value by 10% the elevation range\n",
    "    IQR_max[IQR_max==IQR_min] = (np.nanmax(sl_est_full['snowlines_elevs_median']) - np.nanmin(sl_est_full['snowlines_elevs_median']))*0.1\n",
    "    \n",
    "    sl_est_full_filt = sl_est_full.copy() # filtered dataframe\n",
    "    n_filt = 0 # count number of filtered points\n",
    "    for j, month in enumerate(months):\n",
    "        Ifilt = np.ravel(np.argwhere((sl_est_full.loc[sl_est_full['month']==month]['snowlines_elevs_median'] > med[j]+IQR_max[j]).values |\n",
    "                        ((sl_est_full.loc[sl_est_full['month']==month]['snowlines_elevs_median'] < med[j]-IQR_min[j]).values)))\n",
    "        n_filt = n_filt + len(Ifilt)\n",
    "        if len(Ifilt)>0:\n",
    "            sl_est_full_filt = sl_est_full_filt.mask((sl_est_full_filt['month']==month) & (sl_est_full_filt['snowlines_elevs_median'] > med[j]+IQR_max[j]))\n",
    "            sl_est_full_filt = sl_est_full_filt.mask((sl_est_full_filt['month']==month) & (sl_est_full_filt['snowlines_elevs_median'] < med[j]-IQR_min[j]))                                                     \n",
    "                                                    \n",
    "        # removed points\n",
    "        ax1.plot(sl_est_full.loc[sl_est_full['month']==month].iloc[Ifilt]['month'], \n",
    "                 sl_est_full.loc[sl_est_full['month']==month].iloc[Ifilt]['snowlines_elevs_median'], \n",
    "                 'x', markersize=5, color='#969696', label='_nolegend_') \n",
    "        ax2.plot(sl_est_full.loc[sl_est_full['month']==month].iloc[Ifilt]['datetime'], \n",
    "                 sl_est_full.loc[sl_est_full['month']==month].iloc[Ifilt]['snowlines_elevs_median'], \n",
    "                 'x', markersize=5, color='#969696')\n",
    "   \n",
    "    # -----Determine annual ELAs\n",
    "    # add years column\n",
    "    sl_est_full_filt['year'] = [x.year for x in sl_est_full_filt['datetime']]\n",
    "    # grab all unique years\n",
    "    years = np.unique(sl_est_full_filt['year'].dropna())\n",
    "    # initialize dataframe for ELAs\n",
    "    ELAs_df = pd.DataFrame()\n",
    "    # loop through years, save maximum median snowline elevation and date of observation\n",
    "    for year in years:\n",
    "        sl_est_year = sl_est_full_filt.loc[sl_est_full_filt['year']==year]\n",
    "        ELA, dt = sl_est_year.loc[sl_est_year['snowlines_elevs_median']==np.max(sl_est_year['snowlines_elevs_median'])][['snowlines_elevs_median', 'datetime']].values[0]\n",
    "        df = pd.DataFrame({'ELA': ELA, \n",
    "                           'datetime':dt}, \n",
    "                          index=[0])\n",
    "        ELAs_df = pd.concat([ELAs_df, df])\n",
    "    ELAs_df = ELAs_df.reset_index(drop=True)\n",
    "    \n",
    "    # -----Plot\n",
    "    ax1.set_xlim(np.nanmin(months)-0.5, np.nanmax(months)+0.5)\n",
    "    ax2.set_xlim(np.datetime64('2016-01-01'), np.datetime64('2023-01-01'))\n",
    "    # plot minimum elevation\n",
    "    elev_min = np.nanmin(sl_est_full['snowlines_elevs_median'])\n",
    "    ax1.plot([ax1.get_xlim()[0], ax1.get_xlim()[1]], \n",
    "             [elev_min, elev_min], '-', color='#d95f02', label='_nolegend_')\n",
    "    ax2.plot([ax2.get_xlim()[0], ax2.get_xlim()[1]], \n",
    "             [elev_min, elev_min], '-', color='#d95f02', label='_nolegend_')\n",
    "    # ax1.text(ax1.get_xlim()[1], elev_min, 'Min. glacier elevation', color='#d95f02') # add text label\n",
    "    # range of acceptable values\n",
    "    ax1.fill_between(months, med-IQR_min, med+IQR_max, color='#4eb3d3', label='Acceptable range') \n",
    "    # monthly median\n",
    "    ax1.plot(months, med, '-b', label='Monthly median') \n",
    "    # filtered time series\n",
    "    ax1.plot(sl_est_full_filt['month'], sl_est_full_filt['snowlines_elevs_median'], '.k', markersize=10, label='Filtered time series') \n",
    "    ax2.plot(sl_est_full_filt['datetime'], sl_est_full_filt['snowlines_elevs_median'], '.k', markersize=10)\n",
    "    # ELAs\n",
    "    ax2.plot(ELAs_df['datetime'], ELAs_df['ELA'], 's', color='b', markersize=10)\n",
    "    # dummy points for legend\n",
    "    ax1.plot(0, 0, 'x', markersize=5, color='#969696', label='Removed points')\n",
    "    ax1.plot(0, 0,  's', color='b', markersize=10, label='ELA')\n",
    "    # optional: plot USGS ELA estimates\n",
    "    if usgs_path:\n",
    "        usgs_fn = usgs_path + site_name+'/Output_'+site_name+'_Glacier_Wide_solutions_calibrated.csv'\n",
    "        usgs_file = pd.read_csv(usgs_fn)\n",
    "        ELA = usgs_file['ELA']\n",
    "        ELA_date = usgs_file['Ba_Date'].astype(np.datetime64)\n",
    "        ax1.plot(0,0, 's', markerfacecolor='None', markeredgecolor='orange', \n",
    "                 ms=10, markeredgewidth=2, label='USGS ELA')\n",
    "        ax2.plot(ELA_date, ELA, 's', markerfacecolor='None', markeredgecolor='orange', \n",
    "                 ms=10, markeredgewidth=2, label='_nolegend_')\n",
    "    # set axis limits\n",
    "    ax1.set_xticks(np.linspace(months[0], months[-1], num=months[-1]-months[0]+1))\n",
    "    ax1.set_xlim(np.min(months)-0.5, np.max(months)+0.5)\n",
    "    ax2.set_xlim(np.datetime64('2016-01-01'), np.datetime64('2023-01-01'))\n",
    "    ymin, ymax = (np.nanmin(np.concatenate([sl_est_full['snowlines_elevs_median'].values, np.array(med-IQR_min)]))-25, \n",
    "                  np.nanmax(np.concatenate([sl_est_full['snowlines_elevs_median'].values, np.array(med+IQR_max)]))+25)\n",
    "    ax1.set_ylim(ymin, ymax)\n",
    "    ax2.set_ylim(ymin, ymax)\n",
    "    # add legend to figure\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center', ncol=len(labels))   \n",
    "    \n",
    "    plt.show()\n",
    "    print('Number of removed points = '+str(n_filt))\n",
    "    \n",
    "    # -----Save figure\n",
    "    fig_fn = figures_out_path + 'filtered_snowline_timeseries_' + site_name + '.png'\n",
    "    fig.savefig(fig_fn, dpi=300, facecolor='w')\n",
    "    print('figure saved to file: ' + fig_fn)\n",
    "    \n",
    "    # -----Save filtered snowline time series\n",
    "    sl_est_full_filt = sl_est_full_filt.dropna().drop(['datetime', 'month'], axis=1)\n",
    "    sl_fn = (site_name + '_snowlines_filtered_' + str(np.min(sl_est_full['datetime']))[0:10].replace('-','') \n",
    "             + '_' + str(np.max(sl_est_full['datetime']))[0:10].replace('-','') + '.pkl')\n",
    "    sl_est_full_filt.to_pickle(out_path + sl_fn)\n",
    "    print('filtered snowlines saved to file: ' + out_path + sl_fn)\n",
    "    \n",
    "    # -----Save ELA times series\n",
    "    ELAs_fn = (site_name + '_ELAs_' + str(years[0]) + '_' + str(years[-1]) + '.pkl')\n",
    "    ELAs_df.to_pickle(out_path + ELAs_fn)\n",
    "    print('ELAs saved to file: ' + out_path + ELAs_fn)\n",
    "    print(' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adae50b5-6b9e-4315-934b-3191b056327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack weeks to filter\n",
    "sl_est_full['week'] = [x.week for x in sl_est_full['datetime']]\n",
    "weeks = np.unique(sl_est_full['week'])\n",
    "med_week = [np.nanmedian(sl_est_full.loc[sl_est_full['week']==week]['snowlines_elevs_median']) for week in weeks]\n",
    "IQR_min_week = np.array([iqr(sl_est_full.loc[sl_est_full['week']==week]['snowlines_elevs_median'], \n",
    "                            rng=(25,50), nan_policy='omit') for week in weeks])\n",
    "IQR_min_week[IQR_min_week < np.nanmin(sl_est_full['snowlines_elevs_median'])]== np.nanmin(sl_est_full['snowlines_elevs_median'])\n",
    "IQR_max_week = np.array([iqr(sl_est_full.loc[sl_est_full['week']==week]['snowlines_elevs_median'], \n",
    "                        rng=(50,85), nan_policy='omit') for week in weeks])\n",
    "# if the IQR_max = IQR_min, increase the max value by 10% the elevation range\n",
    "IQR_max_week[IQR_max_week==IQR_min_week] = (np.nanmax(sl_est_full['snowlines_elevs_median']) - np.nanmin(sl_est_full['snowlines_elevs_median']))*0.1\n",
    "\n",
    "# ----Set up figure\n",
    "fig = plt.figure(figsize=(20, 6))\n",
    "plt.rcParams.update({'font.size':18, 'font.sans-serif':'Arial'})\n",
    "spec = fig.add_gridspec(ncols=2, nrows=1, width_ratios=[1, 2.5])\n",
    "ax1 = fig.add_subplot(spec[0, 0])\n",
    "ax1.set_xlabel('Week')\n",
    "ax1.set_ylabel('Median snowline elevations [m]')\n",
    "ax1.grid()\n",
    "ax2 = fig.add_subplot(spec[0, 1])\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.grid()\n",
    "\n",
    "# -----Filter points outside the acceptable range\n",
    "sl_est_full_filt = sl_est_full.copy(deep=True)\n",
    "for j, week in enumerate(weeks):\n",
    "    Ifilt = np.ravel(np.argwhere((sl_est_full.loc[sl_est_full['week']==week]['snowlines_elevs_median'] > med_week[j]+IQR_max_week[j]).values |\n",
    "                    ((sl_est_full.loc[sl_est_full['week']==week]['snowlines_elevs_median'] < med_week[j]-IQR_min_week[j]).values)))\n",
    "    n_filt = n_filt + len(Ifilt)\n",
    "    if len(Ifilt)>0:\n",
    "        sl_est_full_filt = sl_est_full_filt.mask((sl_est_full_filt['week']==week) & (sl_est_full_filt['snowlines_elevs_median'] > med_week[j]+IQR_max_week[j]))\n",
    "        sl_est_full_filt = sl_est_full_filt.mask((sl_est_full_filt['week']==week) & (sl_est_full_filt['snowlines_elevs_median'] < med_week[j]-IQR_min_week[j]))                                                     \n",
    "\n",
    "    # removed points\n",
    "    ax1.plot(sl_est_full.loc[sl_est_full['week']==week].iloc[Ifilt]['week'], \n",
    "             sl_est_full.loc[sl_est_full['week']==week].iloc[Ifilt]['snowlines_elevs_median'], \n",
    "             'x', markersize=5, color='#969696', label='_nolegend_') \n",
    "    ax2.plot(sl_est_full.loc[sl_est_full['week']==week].iloc[Ifilt]['datetime'], \n",
    "             sl_est_full.loc[sl_est_full['week']==week].iloc[Ifilt]['snowlines_elevs_median'], \n",
    "             'x', markersize=5, color='#969696')\n",
    "\n",
    "ax1.fill_between(weeks, med_week-IQR_min_week, med_week+IQR_max_week, color='#4eb3d3', label='Acceptable range') \n",
    "ax1.plot(weeks, med_week, '-b')\n",
    "ax1.plot(sl_est_full_filt['week'], sl_est_full_filt['snowlines_elevs_median'], '.k')\n",
    "ax2.plot(sl_est_full_filt['datetime'], sl_est_full_filt['snowlines_elevs_median'], '.k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae821718-bbf0-4110-b6b4-9df97da758c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_filt = sl_est_full.copy()\n",
    "for i in range(1,len(sl_est_full)):\n",
    "    gradient = sl_est_full.iloc[i]['snowlines_elevs_median'] - sl_est_full.iloc[i-1]['snowlines_elevs_median']\n",
    "    plt.plot(sl_est_full.iloc[i]['datetime'], gradient, '.b')\n",
    "    if gradient > 100:\n",
    "        sl_filt.iloc[i] = np.nan\n",
    "        i+=1\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(sl_est_full['datetime'], sl_est_full['snowlines_elevs_median'], '.b')\n",
    "plt.plot(sl_filt['datetime'], sl_filt['snowlines_elevs_median'], '.m')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22502019-abaf-4275-bf7d-3e8943b7aa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24 hours per site\n",
    "# 5 sites/day\n",
    "# 300 sites * 1 day / 5 sites = 60 days\n",
    "# 60 days * 1 week / 5 days = 12 weeks\n",
    "# 12 weeks * 1 month / 4 weeks = 3 months"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planet-snow",
   "language": "python",
   "name": "planet-snow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
