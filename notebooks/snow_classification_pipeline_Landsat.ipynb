{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bab1b699",
   "metadata": {},
   "source": [
    "# Classify snow-covered area (SCA) in Landsat surface reflectance imagery: full pipeline\n",
    "\n",
    "Rainey Aberle\n",
    "\n",
    "Department of Geosciences, Boise State University\n",
    "\n",
    "2022\n",
    "\n",
    "### Requirements:\n",
    "- Area of Interest (AOI) shapefile: where snow will be classified in all available images. \n",
    "- Google Earth Engine (GEE) account: used to pull DEM over the AOI. Sign up for a free account [here](https://earthengine.google.com/new_signup/). \n",
    "\n",
    "### Outline:\n",
    "__0. Setup__ paths in directory, file locations, authenticate GEE - _modify this section!_\n",
    "\n",
    "__1. Load images__ over the AOI \n",
    "\n",
    "__2. Classify SCA__ and use the snow elevations distribution to estimate the seasonal snowline\n",
    "\n",
    "__3. Delineate snowlines__ using classified images. \n",
    "\n",
    "-------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e4e890",
   "metadata": {},
   "source": [
    "### 0. Setup\n",
    "\n",
    "#### Define paths in directory and desired settings. \n",
    "Modify lines located within the following:\n",
    "\n",
    "`#### MODIFY HERE ####`  \n",
    "\n",
    "`#####################`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337a86c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MODIFY HERE #####\n",
    "\n",
    "# -----Paths in directory\n",
    "site_name = 'SouthCascade'\n",
    "# path to snow-cover-mapping/\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/'\n",
    "# path to AOI including the name of the shapefile\n",
    "AOI_fn = base_path + '../study-sites/' + site_name + '/glacier_outlines/' + site_name + '_USGS_*.shp'\n",
    "# path to DEM including the name of the tif file\n",
    "# Note: set DEM_fn=None if you want to use the ASTER GDEM on Google Earth Engine\n",
    "DEM_fn = base_path + '../study-sites/' + site_name + '/DEMs/' + site_name + '*_DEM*.tif'\n",
    "# path for output images\n",
    "out_path = base_path + '../study-sites/' + site_name + '/imagery/Landsat/'\n",
    "# path for output figures\n",
    "figures_out_path = base_path + '../study-sites/' + site_name + '/figures/'\n",
    "\n",
    "# -----Define image search filters\n",
    "date_start = '2013-01-01'\n",
    "date_end = '2016-12-01'\n",
    "month_start = 5\n",
    "month_end = 10\n",
    "cloud_cover_max = 100\n",
    "\n",
    "# -----Determine settings\n",
    "plot_results = True # = True to plot figures of results for each image where applicable\n",
    "skip_clipped = False # = True to skip images where bands appear \"clipped\", i.e. max blue SR < 0.8\n",
    "crop_to_AOI = True # = True to crop images to AOI before calculating SCA\n",
    "save_outputs = True # = True to save SCA images to file\n",
    "save_figures = True # = True to save SCA output figures to file\n",
    "\n",
    "#######################\n",
    "\n",
    "# -----Import packages\n",
    "import xarray as xr\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import rasterio as rio\n",
    "import geopandas as gpd\n",
    "import sys\n",
    "import ee\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# -----Set paths for output files\n",
    "im_masked_path = out_path + 'masked/'\n",
    "im_classified_path = out_path + 'classified/'\n",
    "snowlines_path = out_path + 'snowlines/'\n",
    "\n",
    "# -----Add path to functions\n",
    "sys.path.insert(1, base_path+'functions/')\n",
    "import pipeline_utils as f\n",
    "\n",
    "# -----Load dataset dictionary\n",
    "with open(base_path + 'inputs-outputs/datasets_characteristics.pkl', 'rb') as fn:\n",
    "    dataset_dict = pickle.load(fn)\n",
    "dataset = 'Landsat'\n",
    "ds_dict = dataset_dict[dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7e78f7",
   "metadata": {},
   "source": [
    "#### Authenticate and initialize Google Earth Engine (GEE). \n",
    "\n",
    "__Note:__ The first time you run the following cell, you will be asked to authenticate your GEE account for use in this notebook. This will send you to an external web page, where you will walk through the GEE authentication workflow and copy an authentication code back in this notebook when prompted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d23d4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ee.Initialize()\n",
    "except: \n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ca0891",
   "metadata": {},
   "source": [
    "#### Load AOI and DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4807ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Load AOI as gpd.GeoDataFrame\n",
    "AOI_fn = glob.glob(AOI_fn)[0]\n",
    "AOI = gpd.read_file(AOI_fn)\n",
    "# reproject the AOI to WGS to solve for the optimal UTM zone\n",
    "AOI_WGS = AOI.to_crs(4326)\n",
    "AOI_WGS_centroid = [AOI_WGS.geometry[0].centroid.xy[0][0],\n",
    "                    AOI_WGS.geometry[0].centroid.xy[1][0]]\n",
    "epsg_UTM = f.convert_wgs_to_utm(AOI_WGS_centroid[0], AOI_WGS_centroid[1])\n",
    "    \n",
    "# -----Load DEM as Xarray DataSet\n",
    "if DEM_fn==None:\n",
    "    \n",
    "    # query GEE for DEM\n",
    "    DEM, AOI_UTM = f.query_GEE_for_DEM(AOI)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # reproject AOI to UTM\n",
    "    AOI_UTM = AOI.to_crs(str(epsg_UTM))\n",
    "    # load DEM as xarray DataSet\n",
    "    DEM_fn = glob.glob(DEM_fn)[0]\n",
    "    DEM = xr.open_dataset(DEM_fn)\n",
    "    DEM = DEM.rename({'band_data': 'elevation'})\n",
    "    # reproject the DEM to the optimal UTM zone\n",
    "    DEM = DEM.rio.reproject(str('EPSG:'+epsg_UTM))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5b99f6",
   "metadata": {},
   "source": [
    "## 1. Load images over the AOI and mask cloudy pixels using the `QA_PIXEL` band\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7f8995",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(date_start + ' ' + date_end)\n",
    "print('----------')\n",
    "L_ds_fns = f.query_GEE_for_Landsat_SR(AOI, date_start, date_end, month_start, month_end, cloud_cover_max, \n",
    "                                       site_name, dataset, ds_dict, im_masked_path, plot_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8226b3a7",
   "metadata": {},
   "source": [
    "## 2. Classify images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ec9df6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load trained classifier and feature columns\n",
    "clf_fn = base_path+'inputs-outputs/L_classifier_all_sites.sav'\n",
    "clf = pickle.load(open(clf_fn, 'rb'))\n",
    "feature_cols_fn = base_path+'inputs-outputs/L_feature_cols.pkl'\n",
    "feature_cols = pickle.load(open(feature_cols_fn,'rb'))\n",
    "\n",
    "# read masked images\n",
    "im_masked_fns = glob.glob(im_masked_path + '*_masked.nc')\n",
    "im_masked_fns = sorted(im_masked_fns) # sort chronologically\n",
    "\n",
    "# loop through masked image files\n",
    "for im_masked_fn in im_masked_fns:\n",
    "    # load file\n",
    "    im_masked = xr.open_dataset(im_masked_fn)\n",
    "    # classify images\n",
    "    plot_results=False\n",
    "    im_classified = f.classify_image(im_masked, clf, feature_cols, \n",
    "                                     crop_to_AOI, AOI_UTM, ds_dict, dataset, \n",
    "                                     site_name, im_classified_path, plot_results, \n",
    "                                     figures_out_path)\n",
    "    print(' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f6b74a",
   "metadata": {},
   "source": [
    "## 3. Delineate snowlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0acce25",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Read image file names\n",
    "# masked images\n",
    "im_masked_fns = glob.glob(im_masked_path + '*_masked.nc')\n",
    "im_masked_fns = sorted(im_masked_fns) # sort chronologically\n",
    "# classified images\n",
    "im_classified_fns = glob.glob(im_classified_path + '*_classified.nc')\n",
    "im_classified_fns = sorted(im_classified_fns) # sort chronologically\n",
    "\n",
    "# -----Initialize snowlines data frame\n",
    "snowlines_df = pd.DataFrame(columns=['study_site', 'datetime', 'snowlines_coords', 'snowlines_elevs', 'snowlines_elevs_median'])\n",
    "    \n",
    "# -----Loop through classified images\n",
    "for im_classified_fn in im_classified_fns:\n",
    "        \n",
    "    # load classified file\n",
    "    im_classified = xr.open_dataset(im_classified_fn)\n",
    "    im_dt = str(im_classified.time.data[0]).replace('-','').replace(':','')[0:15]\n",
    "    print(im_dt)\n",
    "    \n",
    "    # check if snowline exists in directory already\n",
    "    snowline_fn = site_name + '_' + dataset + '_' + im_dt + '_snowline.pkl'\n",
    "    if os.path.exists(os.path.join(snowlines_path, snowline_fn)):\n",
    "        print('snowline already exist in file, loading...')\n",
    "        snowline_df = pickle.load(open(snowlines_path + snowline_fn,'rb'))\n",
    "        \n",
    "    else:\n",
    "        # load masked image file\n",
    "        masked_fn = [x for x in im_masked_fns if (im_dt in x)][0] \n",
    "        im_masked = xr.open_dataset(masked_fn)\n",
    "        # delineate snowline\n",
    "        plot_results=True\n",
    "        snowline_df = f.delineate_im_snowline(im_masked, im_classified, site_name, AOI_UTM, DEM, ds_dict, \n",
    "                                              dataset, im_dt, snowlines_path, figures_out_path, plot_results)\n",
    "        \n",
    "    # save snowline to file\n",
    "    snowline_df.to_pickle(snowlines_path + snowline_fn)\n",
    "    print('snowline saved to file:' + snowlines_path + snowline_fn)\n",
    "    # concatenate results to snowlines_df\n",
    "    snowlines_df = pd.concat([snowlines_df, snowline_df])\n",
    "    print(' ')\n",
    "    \n",
    "# -----Save snowlines_df to file\n",
    "date_start = im_classified_fns[0].split(dataset+'_')[1][0:8]\n",
    "date_end = im_classified_fns[-1].split(dataset+'_')[1][0:8]\n",
    "snowlines_fn = site_name + '_' + dataset + '_' + date_start + '_' + date_end + '_snowlines.pkl'\n",
    "snowlines_df = snowlines_df.reset_index(drop=True)\n",
    "snowlines_df.to_pickle(snowlines_path + snowlines_fn)\n",
    "print('snowlines saved to file:' + snowlines_path + snowlines_fn)\n",
    "\n",
    "# -----Plot median snow line elevations\n",
    "if plot_results:\n",
    "    fig2, ax2 = plt.subplots(figsize=(10,6))\n",
    "    plt.rcParams.update({'font.size':12, 'font.sans-serif':'Arial'})\n",
    "    # plot snowlines\n",
    "    ax2.plot(snowlines_df['datetime'].astype(np.datetime64),\n",
    "             snowlines_df['snowlines_elevs_median'], '.b', markersize=10)\n",
    "    ax2.set_ylabel('Median snow line elevation [m]')\n",
    "    ax2.grid()\n",
    "    # format x-axis\n",
    "    xmin, xmax = np.datetime64('2016-05-01T00:00:00'), np.datetime64('2022-11-01T00:00:00')\n",
    "    fmt_month = matplotlib.dates.MonthLocator(bymonth=(5, 11)) # minor ticks every month.\n",
    "    fmt_year = matplotlib.dates.YearLocator() # minor ticks every year.\n",
    "    ax2.xaxis.set_minor_formatter(matplotlib.dates.DateFormatter('%b'))\n",
    "    ax2.xaxis.set_major_locator(fmt_month)\n",
    "    ax2.xaxis.set_major_formatter(matplotlib.dates.DateFormatter('%b'))\n",
    "    # create a second x-axis beneath the first x-axis to show the year in YYYY format\n",
    "    sec_xaxis = ax2.secondary_xaxis(-0.1)\n",
    "    sec_xaxis.xaxis.set_major_locator(fmt_year)\n",
    "    sec_xaxis.xaxis.set_major_formatter(matplotlib.dates.DateFormatter('%Y'))\n",
    "    # Hide the second x-axis spines and ticks\n",
    "    sec_xaxis.spines['bottom'].set_visible(False)\n",
    "    sec_xaxis.tick_params(axis='x', length=0, pad=-10)\n",
    "    fig2.suptitle(site_name + ' Glacier median snow line elevations')\n",
    "    fig2.tight_layout()\n",
    "    plt.show()\n",
    "    # save figure\n",
    "    fig2_fn = figures_out_path + site_name + '_' + dataset + '_' + date_start.replace('-','') + '_' + date_end.replace('-','')+ '_snowline_median_elevs.png'\n",
    "    fig2.savefig(fig2_fn, dpi=300, facecolor='white', edgecolor='none')\n",
    "    print('figure saved to file:' + fig2_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce54968-e9c8-4f12-a131-288bce826952",
   "metadata": {},
   "source": [
    "#### Optional: compile figures into .gif, delete individual figures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cfc854-a1f0-4ed2-9510-11798a94ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Modify the strings below according to your file names ###\n",
    "\n",
    "# identify the string that is present in all filenames of the figures that you want to compile\n",
    "fig_fns_str = site_name + '_' + dataset + '_*snowline.png'\n",
    "# define the output .gif filename\n",
    "gif_fn = site_name + '_' + dataset + '_' + date_start.replace('-','') + '_' + date_end.replace('-','') + '_snowlines.gif' \n",
    "\n",
    "# -----Make a .gif of output images\n",
    "from PIL import Image as PIL_Image\n",
    "from IPython.display import Image as IPy_Image\n",
    "os.chdir(figures_out_path)\n",
    "fig_fns = glob.glob(fig_fns_str) # load all output figure file names\n",
    "fig_fns = sorted(fig_fns) # sort chronologically\n",
    "\n",
    "# grab figures date range for .gif file name\n",
    "frames = [PIL_Image.open(im) for im in fig_fns]\n",
    "frame_one = frames[0]\n",
    "frame_one.save(figures_out_path + gif_fn, format=\"GIF\", append_images=frames, save_all=True, duration=2000, loop=0)\n",
    "print('GIF saved to file:' + figures_out_path + gif_fn)\n",
    "\n",
    "\n",
    "# -----Clean up: delete individual figure files\n",
    "for fn in fig_fns:\n",
    "    os.remove(os.path.join(figures_out_path, fn))\n",
    "print('Individual figure files deleted.')\n",
    "\n",
    "# -----Display .gif\n",
    "IPy_Image(filename = figures_out_path + gif_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11d3593-db2a-40c0-9676-02c05aaf3fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planet-snow",
   "language": "python",
   "name": "planet-snow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
