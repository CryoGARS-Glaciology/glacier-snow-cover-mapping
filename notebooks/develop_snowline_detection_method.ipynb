{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc156d32-a952-4bfd-af5c-24a0e077a14c",
   "metadata": {},
   "source": [
    "# Develop snow line detection method\n",
    "\n",
    "Rainey Aberle\n",
    "\n",
    "September 2022\n",
    "\n",
    "### Methods tested:\n",
    "\n",
    "1. Fill no-data pixels using the normalized histogram of snow elevations, vectorize snow-covered area, extract snowline coordinates\n",
    "    \n",
    "2. Solve for the optimal snow elevation percentile \n",
    "    \n",
    "3. Edge detection in classified images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf5936f-0dc8-4fa1-b30f-e3520bee8ee7",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d2c624-4c19-43cb-b520-91e3e5a8475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import rioxarray as rxr\n",
    "import rasterio as rio\n",
    "import xarray as xr\n",
    "import ee\n",
    "import wxee as wx\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "import sys\n",
    "from scipy import stats\n",
    "import skimage.io\n",
    "from skimage import feature\n",
    "from skimage.measure import find_contours\n",
    "from scipy.signal import medfilt\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "from shapely.geometry import Point, LineString, shape, MultiPolygon, Polygon\n",
    "from shapely.ops import split, unary_union, polygonize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef96b0e-aad9-456c-98b8-2145e6a3a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to snow-cover-mapping\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/'\n",
    "# names of study sites\n",
    "site_names = ['Gulkana'] #['Gulkana', 'SCascade', 'Sperry', 'Wolverine']\n",
    "# path for output figures\n",
    "figures_out_path = base_path+'figures/'\n",
    "\n",
    "# add path to functions\n",
    "sys.path.insert(1, base_path+'functions/')\n",
    "import ps_pipeline_utils as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d238caaf-c8f3-4aa8-82f7-2b214566aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate Google Earth Engine (GEE)\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except: \n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74599c5-2656-4b71-81cb-404a142bb5c6",
   "metadata": {},
   "source": [
    "## 1. Fill no-data pixels using the normalized histogram of snow elevations, identify contours in binary snow image, filter contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a653dff-3c31-400a-b97c-8ad17d3177de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### MAKE POLYGONS OF SNOW-COVERED AREAS\n",
    "\n",
    "# ----Initialize observed and estimated snow line median elevation\n",
    "sl_obs_elev_medians = []\n",
    "sl_est_elev_medians = []\n",
    "\n",
    "# -----Loop through sites\n",
    "for site_name in site_names:    \n",
    "    \n",
    "    # define path to classified snow images\n",
    "    im_path = base_path + '../study-sites/' + site_name + '/imagery/PlanetScope/'\n",
    "\n",
    "    # define path to digitized snow lines\n",
    "    sl_obs_path = base_path + '../snowline-package/' + site_name + '/snowlines/'\n",
    "\n",
    "    # load AOI as gpd.GeoDataFrame\n",
    "    AOI_fn = base_path + '../../GIS_data/RGI_outlines/' + site_name + '_RGI.shp'\n",
    "    AOI = gpd.read_file(AOI_fn)\n",
    "\n",
    "    # query GEE for DEM\n",
    "    DEM, AOI_UTM = f.query_GEE_for_DEM(AOI)\n",
    "\n",
    "    # load snow line shapefile names\n",
    "    sl_obs_fns = glob.glob(sl_obs_path + '*.shp')\n",
    "    sl_obs_fns.sort() # sort chronologically\n",
    "\n",
    "    # initialize variables\n",
    "    sl_obs_elevs = [None]*len(sl_obs_fns) # observed snow elevations\n",
    "    datetimes = [None]*len(sl_obs_fns) # image datetimes\n",
    "\n",
    "    # loop through observed snow lines\n",
    "    for sl_obs_fn in sl_obs_fns:\n",
    "\n",
    "        # -----Load datasets\n",
    "        # load snow line\n",
    "        sl_obs = gpd.read_file(sl_obs_fn)\n",
    "        # extract date from filename\n",
    "        date = sl_obs_fn.split('/'+site_name+'_')[1][0:8]\n",
    "        # reproject snow line to UTM\n",
    "        sl_obs_UTM = sl_obs.to_crs(str(AOI_UTM.crs.to_epsg()))\n",
    "        # open adjusted image of the same date\n",
    "        im_adj_fn = glob.glob(im_path + 'adjusted-filtered/*' + date + '*.tif')[0] # define file name\n",
    "        im_adj = rxr.open_rasterio(im_adj_fn) # open image as xarray.DataArray\n",
    "        im_adj = im_adj / 1e4\n",
    "         # open classified image from the same date\n",
    "        im_classified_fn = glob.glob(im_path + 'classified/*' + date + '*.tif')[0] # define file name\n",
    "        im_classified = rxr.open_rasterio(im_classified_fn) # open image as xarray.DataArray\n",
    "        # create no data mask\n",
    "        no_data_mask = xr.where(im_classified==-9999, 1, 0).data[0]\n",
    "        # convert to polygons\n",
    "        no_data_polygons = []\n",
    "        for s, value in rio.features.shapes(no_data_mask.astype(np.int16), \n",
    "                                            mask=(no_data_mask >0), transform=rio.open(im_adj_fn).transform):\n",
    "            no_data_polygons.append(shape(s))\n",
    "        no_data_polygons = MultiPolygon(no_data_polygons)\n",
    "        # mask no data points in classified image\n",
    "        im_classified = im_classified.where(im_classified!=-9999) # now, remove no data values\n",
    "        \n",
    "        # -----Interpolate elevations at observed snow line\n",
    "        sl_obs_elev = np.array([DEM.sel(time=DEM.time.data[0], x=x, y=y, method='nearest').elevation.data \n",
    "                                for x, y in list(zip(sl_obs_UTM.geometry[0].xy[0], \n",
    "                                                     sl_obs_UTM.geometry[0].xy[1]))])\n",
    "        # calculate median snow line elevation\n",
    "        sl_obs_elev_median = np.nanmedian(sl_obs_elev)\n",
    "        sl_obs_elev_medians = sl_obs_elev_medians + [sl_obs_elev_median] # add to list\n",
    "       \n",
    "        # -----Mask the DEM using the AOI\n",
    "        # create AOI mask with DEM cordinates\n",
    "        mask = rio.features.geometry_mask(AOI_UTM.geometry,\n",
    "                                          out_shape=(len(DEM.y), len(DEM.x)),\n",
    "                                          transform=DEM.transform,\n",
    "                                          invert=True)\n",
    "        # convert mask to xarray DataArray\n",
    "        mask = xr.DataArray(mask , dims=(\"y\", \"x\"))\n",
    "        # mask DEM values outside the AOI\n",
    "        DEM_AOI = DEM.where(mask == True)\n",
    "\n",
    "        # -----Interpolate DEM at the image coordinates\n",
    "        # grab image indices\n",
    "        band, x, y = im_classified.indexes.values() \n",
    "        # interpolate DEM at image coordinates\n",
    "        DEM_AOI_interp = DEM_AOI.interp(x=x, y=y, method=\"nearest\") \n",
    "\n",
    "        # -----Determine snow covered elevations\n",
    "        DEM_AOI_interp_snow = DEM_AOI_interp.where(im_classified<=2) # mask pixels not classified as snow\n",
    "        snow_est_elev = DEM_AOI_interp_snow.elevation.data.flatten() # create array of snow-covered pixel elevations\n",
    "\n",
    "        # -----Calculate elevation histograms\n",
    "        # determine histogram bins using DEM elevation range\n",
    "        elev_min = np.fix(np.nanmin(DEM_AOI_interp.elevation.data.flatten())/10)*10\n",
    "        elev_max = np.round(np.nanmax(DEM_AOI_interp.elevation.data.flatten())/10)*10\n",
    "        bin_edges = np.linspace(elev_min, elev_max, num=int((elev_max-elev_min)/10 + 1))\n",
    "        bin_centers = (bin_edges[1:] + bin_edges[0:-1]) / 2\n",
    "        # calculate elevation histograms\n",
    "        H_DEM = np.histogram(DEM_AOI_interp.elevation.data.flatten(), bins=bin_edges)[0]\n",
    "        H_snow_est_elev = np.histogram(snow_est_elev, bins=bin_edges)[0]\n",
    "        H_snow_est_elev_norm = H_snow_est_elev / H_DEM\n",
    "\n",
    "        # -----All pixels above the first elevation band with > 75% snow-coverage = snow\n",
    "        # determine elevation with > 75% snow coverage\n",
    "        elev_75_snow = bin_centers[np.where(H_snow_est_elev_norm > 0.75)[0][0]]\n",
    "        # set all pixels above the elev_75_snow to snow (1)\n",
    "        im_classified_adj = xr.where(DEM_AOI_interp.elevation > elev_75_snow, 1, im_classified) # set all values above elev_75_snow to snow (1)\n",
    "        im_classified_adj = im_classified_adj.squeeze(drop=True) # drop unecessary dimensions\n",
    "        \n",
    "        # -----Generate and filter binary snow matrix\n",
    "        # create binary snow matrix\n",
    "        im_binary = xr.where(im_classified_adj <=2, 1, 0).data\n",
    "        # apply median filter to binary image with kernel_size of 33 pixels (~99 m)\n",
    "        im_binary_filt = medfilt(im_binary, kernel_size=33)\n",
    "        # fill holes in binary image (0s within 1s = 1)\n",
    "        im_binary_filt_no_holes = binary_fill_holes(im_binary_filt)\n",
    "\n",
    "        # -----Find contours between 0 and 1 in binary image\n",
    "        # Find contours at a constant value of 0.5 (between 0 and 1)\n",
    "        contours = find_contours(im_binary_filt_no_holes, 0.5)\n",
    "        # convert contour points to image coordinates\n",
    "        contours_coords = []\n",
    "        for contour in contours: \n",
    "            ix = np.round(contour[:,1]).astype(int)\n",
    "            iy = np.round(contour[:,0]).astype(int)\n",
    "            coords = (im_adj.isel(x=ix, y=iy).x.data, # image x coordinates\n",
    "                      im_adj.isel(x=ix, y=iy).y.data) # image y coordinates\n",
    "            # zip points together\n",
    "            xy = list(zip([x for x in coords[0]], \n",
    "                          [y for y in coords[1]]))\n",
    "            contours_coords = contours_coords + [xy]\n",
    "\n",
    "       # -----Create snow polygons\n",
    "        c_polys = []\n",
    "        for c in contours_coords:\n",
    "            c_points = [Point(x,y) for x,y in c]\n",
    "            c_poly = Polygon([[p.x, p.y] for p in c_points])\n",
    "            c_polys = c_polys + [c_poly]\n",
    "        # only save the largest 2 polygons\n",
    "        if len(c_polys) > 2:\n",
    "            # calculate polygon areas\n",
    "            areas = np.array([poly.area for poly in c_polys])\n",
    "            # grab top 3 areas with their polygon indices\n",
    "            areas_max = sorted(zip(areas, np.arange(0,len(c_polys))), reverse=True)[:2]\n",
    "            # grab indices\n",
    "            ic_polys_filt = [x[1] for x in areas_max]\n",
    "            # grab polygons at indices\n",
    "            c_polys_filt = [c_polys[i] for i in ic_polys_filt]\n",
    "        # extract coordinates in polygon\n",
    "        polys_coords = [list(zip(c.exterior.coords.xy[0], c.exterior.coords.xy[1]))  for c in c_polys_filt]\n",
    "\n",
    "        # -----Extract snow lines (sl) from contours\n",
    "        # filter contours using no data and AOI masks (i.e., along glacier outline or data gaps)\n",
    "        sl_est = [] # initialize list of snow lines\n",
    "        min_sl_length = 100 # minimum snow line length\n",
    "        for c in polys_coords:\n",
    "            # create array of points\n",
    "            c_points =  [Point(x,y) for x,y in c]\n",
    "            # loop through points \n",
    "            line_points = [] # initialize list of points to use in snow line\n",
    "            for point in c_points:\n",
    "                # calculate distance from the point to the no data polygons and the AOI boundary\n",
    "                distance_no_data = no_data_polygons.distance(point)\n",
    "                distance_AOI = AOI_UTM.boundary[0].distance(point)\n",
    "                # only include points 100 m from both\n",
    "                if (distance_no_data >= 100) and (distance_AOI >=100):\n",
    "                    line_points = line_points + [point]\n",
    "            # print(len(line_points))\n",
    "            if line_points: # if list of line points is not empty\n",
    "                if len(line_points) > 1: # must have at least two points to create a LineString\n",
    "                    line = LineString([(p.xy[0][0], p.xy[1][0]) for p in line_points])\n",
    "                    if line.length > min_sl_length:\n",
    "                        sl_est = sl_est + [line]\n",
    "        \n",
    "        # NOT HELPFUL: remove points mostly surrounded by snow\n",
    "        # sl_est_filt = []\n",
    "        # for line in sl_est:\n",
    "        #     coords = list(line.coords)\n",
    "        #     coords_list = []\n",
    "        #     for c in coords:\n",
    "        #         x_samples = np.linspace(c[0]-100, c[0]+100, num=67)\n",
    "        #         y_samples = np.linspace(c[1]-100, c[1]+100, num=67)\n",
    "        #         median_sample = im_classified_adj.sel(x=x_samples, y=y_samples, method='nearest').median().data\n",
    "        #         if median_sample!=1:\n",
    "        #             coords_list = coords_list + [c]\n",
    "        #     if coords_list:\n",
    "        #         if len(coords_list) > 1:\n",
    "        #             line = LineString([(p.xy[0][0], p.xy[1][0]) for p in line_points])\n",
    "        #             if line.length > min_sl_length:\n",
    "        #                 sl_est_filt = sl_est_filt + [line]\n",
    "\n",
    "        # -----Split lines with points more than 100 m apart and filter by length\n",
    "        sl_est_split = [] # initialize list of filtered snow lines\n",
    "        min_sl_length = 200\n",
    "        for line in sl_est:\n",
    "            # extract line x and y coordinates\n",
    "            coords = list(line.coords)\n",
    "            # initialize binary array of where to split\n",
    "            split_list = np.zeros(len(line.coords)) \n",
    "            # loop through points\n",
    "            for i in np.arange(1,len(coords)):\n",
    "                if i!=0:\n",
    "                    point = Point(coords[i])\n",
    "                    # calculate distance between point and previous point\n",
    "                    distance = point.distance(Point(coords[i-1]))\n",
    "                    # set split to 1 if distance is greater than 100 m\n",
    "                    if distance > 100:\n",
    "                        split_list[i] = 1\n",
    "            if np.any(split_list==1):\n",
    "                # initialize binary list of where to split the line\n",
    "                isplit = np.ravel(np.where(split_list==1))\n",
    "                for i in np.arange(0,len(isplit)):\n",
    "                    if i==0:\n",
    "                        if len(coords[:isplit[i]+1]) > 1: # must have at least 2 points in LineString\n",
    "                            line_split = LineString(coords[:isplit[i]+1])\n",
    "                        else:\n",
    "                            line_split = None\n",
    "                    else:\n",
    "                        if len(coords[isplit[i-1]+1:isplit[i]]) > 1: # must have at least 2 points in LineString\n",
    "                            line_split = LineString(coords[isplit[i-1]+1:isplit[i]+1])\n",
    "                        else:\n",
    "                            line_split = None\n",
    "                    # concatenate split line to sl_est_filt if greater than min_sl_length\n",
    "                    if line_split is not None:\n",
    "                        if line_split.length > min_sl_length:\n",
    "                            sl_est_split = sl_est_split + [line_split] \n",
    "            else:\n",
    "                # concatenate line to sl_est_filt if greater than min_sl_length\n",
    "                if line.length > min_sl_length:\n",
    "                    sl_est_split = sl_est_split + [line]   \n",
    "\n",
    "        # -----Interpolate elevation at snow line points\n",
    "        # compile all line coordinates into arrays of x- and y-coordinates \n",
    "        xpts, ypts = [], []\n",
    "        for line in sl_est_split:\n",
    "            xpts = xpts + [x for x in line.coords.xy[0]]\n",
    "            ypts = ypts + [y for y in line.coords.xy[1]]  \n",
    "        xpts, ypts = np.array(xpts).flatten(), np.array(ypts).flatten()\n",
    "        # interpolate elevation at snow line points\n",
    "        sl_est_elev = [DEM.sel(x=x, y=y, method='nearest').elevation.data[0] \n",
    "                       for x, y in list(zip(xpts, ypts))]\n",
    "        # calculate median snow line elevation\n",
    "        sl_est_elev_median = np.nanmedian(sl_est_elev)\n",
    "        sl_est_elev_medians = sl_est_elev_medians + [sl_est_elev_median] # add to list\n",
    "        \n",
    "        # -----Plot results\n",
    "        contour = None\n",
    "        fig, ax, sl_points_AOI = f.plot_im_classified_histogram_contour(im_adj, im_classified_adj, DEM, AOI_UTM, contour)\n",
    "        # plot observed snow line elevation\n",
    "        ax[0].plot([x/1e3 for x in sl_obs_UTM.geometry[0].xy[0]], \n",
    "                   [y/1e3 for y in sl_obs_UTM.geometry[0].xy[1]], \n",
    "                   '.', color='#ae017e', label='sl$_{observed}$', markersize=3) \n",
    "        ax[1].plot([x/1e3 for x in sl_obs_UTM.geometry[0].xy[0]], \n",
    "                   [y/1e3 for y in sl_obs_UTM.geometry[0].xy[1]], \n",
    "                   '.', color='#ae017e', label='_nolegend_', markersize=3) \n",
    "        for line, i  in list(zip( sl_est_split, np.arange(0,len(sl_est_split)) )):\n",
    "            if i==0:\n",
    "                ax[0].plot([x/1e3 for x in line.coords.xy[0]], \n",
    "                           [y/1e3 for y in line.coords.xy[1]],\n",
    "                           '-', color='#f768a1', label='sl$_{estimated}$') \n",
    "            else: \n",
    "                ax[0].plot([x/1e3 for x in line.coords.xy[0]], \n",
    "                           [y/1e3 for y in line.coords.xy[1]],\n",
    "                           '-', color='#f768a1', label='_nolegend_')                 \n",
    "            ax[1].plot([x/1e3 for x in line.coords.xy[0]], \n",
    "                       [y/1e3 for y in line.coords.xy[1]],\n",
    "                       '-', color='#f768a1', label='_nolegend_') \n",
    "        # add legends\n",
    "        ax[0].legend(loc='best')\n",
    "        ax[1].legend(loc='best')\n",
    "        if contour is not None:\n",
    "            ax[3].set_title('Contour = ' + str(np.round(contour,1)) + ' m')\n",
    "        fig.suptitle(date)\n",
    "        plt.show()\n",
    "\n",
    "# -----Plot median snow line elevation\n",
    "fig2, ax2 = plt.subplots(figsize=(8,6))\n",
    "ax2.plot(np.arange(0, len(sl_obs_elev_medians)), \n",
    "         [x-y for x, y in list(zip(sl_est_elev_medians, sl_obs_elev_medians))], \n",
    "         '.b', label='observed')\n",
    "ax2.set_xlabel('observation #')\n",
    "ax2.set_ylabel('Misfit [m]')\n",
    "ax2.set_title('Median snow line elevation misfit (est. - obs.)')\n",
    "ax2.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80ffb6b-dc55-4acc-b2b3-56f4e85d6a44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### MAKE POLYGONS OF NOT SNOW-COVERED AREAS\n",
    "\n",
    "# ----Initialize observed and estimated snow line median elevation\n",
    "sl_obs_elev_medians = []\n",
    "sl_est_elev_medians = []\n",
    "\n",
    "### MAKE POLYGONS OF NOT SNOW-COVERED AREAS\n",
    "\n",
    "# ----Initialize observed and estimated snow line median elevation\n",
    "sl_obs_elev_medians = []\n",
    "sl_est_elev_medians = []\n",
    "\n",
    "# -----Loop through sites\n",
    "for site_name in site_names:  \n",
    "    \n",
    "    # define path to classified snow images\n",
    "    im_path = base_path + '../study-sites/' + site_name + '/imagery/PlanetScope/'\n",
    "\n",
    "    # define path to digitized snow lines\n",
    "    sl_obs_path = base_path + '../snowline-package/' + site_name + '/snowlines/'\n",
    "    \n",
    "    # load AOI as gpd.GeoDataFrame\n",
    "    AOI_fn = im_path + '../../glacier_outlines/' + site_name + '_USGS_*.shp'\n",
    "    AOI_fn = glob.glob(AOI_fn)[0]\n",
    "    AOI = gpd.read_file(AOI_fn)\n",
    "    \n",
    "    # solve for optimal UTM zone\n",
    "    AOI_WGS = AOI.to_crs(4326)\n",
    "    AOI_WGS_centroid = [AOI_WGS.geometry[0].centroid.xy[0][0],\n",
    "                        AOI_WGS.geometry[0].centroid.xy[1][0]]\n",
    "    epsg_UTM = f.convert_wgs_to_utm(AOI_WGS_centroid[0], AOI_WGS_centroid[1])\n",
    "    \n",
    "    # reproject AOI to UTM\n",
    "    AOI_UTM = AOI.to_crs(str(epsg_UTM))\n",
    "    \n",
    "    # load DEM as xarray DataSet\n",
    "    DEM_fn = im_path + '../../DEMs/' + site_name + '*_DEM/' + site_name + '*_DEM_filled.tif'\n",
    "    DEM_fn = glob.glob(DEM_fn)[0]\n",
    "    DEM_rio = rio.open(DEM_fn)\n",
    "    DEM = xr.open_dataset(DEM_fn)\n",
    "    DEM = DEM.rename({'band_data': 'elevation'})\n",
    "\n",
    "    # load snow line shapefile names\n",
    "    sl_obs_fns = glob.glob(sl_obs_path + '*.shp')\n",
    "    sl_obs_fns.sort() # sort chronologically\n",
    "\n",
    "    # initialize variables\n",
    "    sl_obs_elevs = [None]*len(sl_obs_fns) # observed snow elevations\n",
    "    datetimes = [None]*len(sl_obs_fns) # image datetimes\n",
    "\n",
    "    # loop through observed snow lines\n",
    "    for sl_obs_fn in sl_obs_fns:\n",
    "\n",
    "        # -----Load datasets\n",
    "        # load snow line\n",
    "        sl_obs = gpd.read_file(sl_obs_fn)\n",
    "        # extract date from filename\n",
    "        date = sl_obs_fn.split('/'+site_name+'_')[1][0:8]\n",
    "        # reproject snow line to UTM\n",
    "        sl_obs_UTM = sl_obs.to_crs(str(AOI_UTM.crs.to_epsg()))\n",
    "        # open adjusted image of the same date\n",
    "        im_adj_fn = glob.glob(im_path + 'adjusted-filtered/*' + date + '*.tif')[0] # define file name\n",
    "        im_adj = rxr.open_rasterio(im_adj_fn) # open image as xarray.DataArray\n",
    "        im_adj = im_adj / 1e4\n",
    "         # open classified image from the same date\n",
    "        im_classified_fn = glob.glob(im_path + 'classified/*' + date + '*.tif')[0] # define file name\n",
    "        im_classified = rxr.open_rasterio(im_classified_fn) # open image as xarray.DataArray\n",
    "        # create no data mask\n",
    "        no_data_mask = xr.where(im_classified==-9999, 1, 0).data[0]\n",
    "        # convert to polygons\n",
    "        no_data_polygons = []\n",
    "        for s, value in rio.features.shapes(no_data_mask.astype(np.int16), \n",
    "                                            mask=(no_data_mask >0), transform=rio.open(im_adj_fn).transform):\n",
    "            no_data_polygons.append(shape(s))\n",
    "        no_data_polygons = MultiPolygon(no_data_polygons)\n",
    "        # mask no data points in classified image\n",
    "        im_classified = im_classified.where(im_classified!=-9999) # now, remove no data values\n",
    "\n",
    "        # -----Interpolate elevations at observed snow line\n",
    "        sl_obs_elev = np.array([DEM.sel(x=x, y=y, method='nearest').elevation.data \n",
    "                                for x, y in list(zip(sl_obs_UTM.geometry[0].xy[0], \n",
    "                                                     sl_obs_UTM.geometry[0].xy[1]))])\n",
    "        # calculate median snow line elevation\n",
    "        sl_obs_elev_median = np.nanmedian(sl_obs_elev)\n",
    "        sl_obs_elev_medians = sl_obs_elev_medians + [sl_obs_elev_median] # add to list\n",
    "       \n",
    "        # -----Mask the DEM using the AOI\n",
    "        # create AOI mask with DEM cordinates\n",
    "        mask = rio.features.geometry_mask(AOI_UTM.geometry,\n",
    "                                          out_shape=(len(DEM.y), len(DEM.x)),\n",
    "                                          transform=DEM_rio.transform,\n",
    "                                          invert=True)\n",
    "        # convert mask to xarray DataArray\n",
    "        mask = xr.DataArray(mask , dims=(\"y\", \"x\"))\n",
    "        # mask DEM values outside the AOI\n",
    "        DEM_AOI = DEM.where(mask == True)\n",
    "\n",
    "        # -----Interpolate DEM at the image coordinates\n",
    "        # grab image indices\n",
    "        band, x, y = im_classified.indexes.values() \n",
    "        # interpolate DEM at image coordinates\n",
    "        DEM_AOI_interp = DEM_AOI.interp(x=x, y=y, method=\"nearest\") \n",
    "\n",
    "        # -----Determine snow covered elevations\n",
    "        DEM_AOI_interp_snow = DEM_AOI_interp.where(im_classified<=2) # mask pixels not classified as snow\n",
    "        snow_est_elev = DEM_AOI_interp_snow.elevation.data.flatten() # create array of snow-covered pixel elevations\n",
    "\n",
    "        # -----Calculate elevation histograms\n",
    "        # determine histogram bins using DEM elevation range\n",
    "        elev_min = np.fix(np.nanmin(DEM_AOI_interp.elevation.data.flatten())/10)*10\n",
    "        elev_max = np.round(np.nanmax(DEM_AOI_interp.elevation.data.flatten())/10)*10\n",
    "        bin_edges = np.linspace(elev_min, elev_max, num=int((elev_max-elev_min)/10 + 1))\n",
    "        bin_centers = (bin_edges[1:] + bin_edges[0:-1]) / 2\n",
    "        # calculate elevation histograms\n",
    "        H_DEM = np.histogram(DEM_AOI_interp.elevation.data.flatten(), bins=bin_edges)[0]\n",
    "        H_snow_est_elev = np.histogram(snow_est_elev, bins=bin_edges)[0]\n",
    "        H_snow_est_elev_norm = H_snow_est_elev / H_DEM\n",
    "\n",
    "        # -----All pixels above the first elevation band with > 75% snow-coverage = snow\n",
    "        # determine elevation with > 75% snow coverage\n",
    "        if len(np.where(H_snow_est_elev_norm > 0.75))>1:\n",
    "            elev_75_snow = bin_centers[np.where(H_snow_est_elev_norm > 0.75)[0][0]]\n",
    "            # set all pixels above the elev_75_snow to snow (1)\n",
    "            im_classified_adj = xr.where(DEM_AOI_interp.elevation > elev_75_snow, 1, im_classified) # set all values above elev_75_snow to snow (1)\n",
    "            im_classified_adj = im_classified_adj.squeeze(drop=True) # drop unecessary dimensions\n",
    "        else:\n",
    "            im_classified_adj = im_classified.squeeze(drop=True)\n",
    "\n",
    "        # -----Generate and filter binary snow matrix\n",
    "        # create binary snow matrix\n",
    "        im_binary = xr.where(im_classified_adj > 2, 1, 0).data\n",
    "        # apply median filter to binary image with kernel_size of 33 pixels (~99 m)\n",
    "        im_binary_filt = medfilt(im_binary, kernel_size=33)\n",
    "        # fill holes in binary image (0s within 1s = 1)\n",
    "        im_binary_filt_no_holes = binary_fill_holes(im_binary_filt)\n",
    "\n",
    "        # -----Find contours between 0 and 1 in binary image\n",
    "        # Find contours at a constant value of 0.5 (between 0 and 1)\n",
    "        contours = find_contours(im_binary_filt_no_holes, 0.5)\n",
    "        # convert contour points to image coordinates\n",
    "        contours_coords = []\n",
    "        for contour in contours: \n",
    "            ix = np.round(contour[:,1]).astype(int)\n",
    "            iy = np.round(contour[:,0]).astype(int)\n",
    "            coords = (im_adj.isel(x=ix, y=iy).x.data, # image x coordinates\n",
    "                      im_adj.isel(x=ix, y=iy).y.data) # image y coordinates\n",
    "            # zip points together\n",
    "            xy = list(zip([x for x in coords[0]], \n",
    "                          [y for y in coords[1]]))\n",
    "            contours_coords = contours_coords + [xy]\n",
    "\n",
    "        # -----Create no-snow polygons\n",
    "        c_polys = []\n",
    "        for c in contours_coords:\n",
    "            c_points = [Point(x,y) for x,y in c]\n",
    "            c_poly = Polygon([[p.x, p.y] for p in c_points])\n",
    "            c_polys = c_polys + [c_poly]\n",
    "        # only save the largest polygon by area\n",
    "        if len(c_polys) > 1:\n",
    "            # calculate polygon areas\n",
    "            areas = np.array([poly.area for poly in c_polys])\n",
    "            # grab maximum area with its polygon index\n",
    "            areas_max = sorted(zip(areas, np.arange(0,len(c_polys))), reverse=True)[:1]\n",
    "            # grab indices\n",
    "            ic_polys_filt = [x[1] for x in areas_max]\n",
    "            # grab polygons at indices\n",
    "            c_polys_filt = [c_polys[i] for i in ic_polys_filt]\n",
    "        else:\n",
    "            c_polys_filt = c_polys\n",
    "        # extract coordinates in polygon\n",
    "        polys_coords = [list(zip(c.exterior.coords.xy[0], c.exterior.coords.xy[1]))  \n",
    "                        for c in c_polys_filt]\n",
    "\n",
    "        # -----Extract snow lines (sl) from contours\n",
    "        # filter contours using no data and AOI masks (i.e., along glacier outline or data gaps)\n",
    "        sl_est = [] # initialize list of snow lines\n",
    "        min_sl_length = 200 # minimum snow line length\n",
    "        for c in polys_coords:\n",
    "            # create array of points\n",
    "            c_points =  [Point(x,y) for x,y in c]\n",
    "            # loop through points \n",
    "            line_points = [] # initialize list of points to use in snow line\n",
    "            for point in c_points:\n",
    "                # calculate distance from the point to the no data polygons and the AOI boundary\n",
    "                distance_no_data = no_data_polygons.distance(point)\n",
    "                distance_AOI = AOI_UTM.boundary[0].distance(point)\n",
    "                # only include points 100 m from both\n",
    "                if (distance_no_data >= 100) and (distance_AOI >=100):\n",
    "                    line_points = line_points + [point]\n",
    "            if line_points: # if list of line points is not empty\n",
    "                if len(line_points) > 1: # must have at least two points to create a LineString\n",
    "                    line = LineString([(p.xy[0][0], p.xy[1][0]) for p in line_points])\n",
    "                    if line.length > min_sl_length:\n",
    "                        sl_est = sl_est + [line]\n",
    "\n",
    "        # -----Check if any snowlines exist\n",
    "        if sl_est:\n",
    "            # split line depending on distance between points\n",
    "            sl_est = sl_est[0]\n",
    "            max_dist = 100 # m\n",
    "            first_point = Point(sl_est.coords.xy[0][0], sl_est.coords.xy[1][0])\n",
    "            points = [Point(sl_est.coords.xy[0][i], sl_est.coords.xy[1][i]) \n",
    "                      for i in np.arange(0,len(sl_est.coords.xy[0]))]\n",
    "            isplit = [0] # point indices where to split the line\n",
    "            for i, p in enumerate(points):\n",
    "                if i!=0:\n",
    "                    dist = p.distance(points[i-1])\n",
    "                    if dist > max_dist:\n",
    "                        isplit.append(i)\n",
    "            isplit.append(len(points)) # add ending point to complete the last line\n",
    "            sl_est_split = [] # initialize split lines\n",
    "            # loop through split indices\n",
    "            if isplit:\n",
    "                for i, p in enumerate(isplit[:-1]):\n",
    "                    if isplit[i+1]-isplit[i] > 1: # must have at least two points to make a line\n",
    "                        line = LineString(points[isplit[i]:isplit[i+1]])\n",
    "                        if line.length > min_sl_length:\n",
    "                            sl_est_split = sl_est_split + [line]\n",
    "            else:\n",
    "                sl_est_split = [sl_est]\n",
    " \n",
    "            # interpolate elevation at snow line points\n",
    "            # compile all line coordinates into arrays of x- and y-coordinates \n",
    "            xpts, ypts = [], []\n",
    "            for line in sl_est_split:\n",
    "                xpts = xpts + [x for x in line.coords.xy[0]]\n",
    "                ypts = ypts + [y for y in line.coords.xy[1]]  \n",
    "            xpts, ypts = np.array(xpts).flatten(), np.array(ypts).flatten()\n",
    "            # interpolate elevation at snow line points\n",
    "            sl_est_elev = [DEM.sel(x=x, y=y, method='nearest').elevation.data[0] \n",
    "                           for x, y in list(zip(xpts, ypts))]\n",
    "            # calculate median snow line elevation\n",
    "            sl_est_elev_median = np.nanmedian(sl_est_elev)\n",
    "            sl_est_elev_medians = sl_est_elev_medians + [sl_est_elev_median] # add to list\n",
    "        \n",
    "            # -----Plot results\n",
    "            contour = None\n",
    "            fig, ax, sl_points_AOI = f.plot_im_classified_histogram_contour(im_adj, im_classified_adj, DEM, DEM_rio, AOI_UTM, contour)\n",
    "            # plot estimated snow line coordinates\n",
    "            for i, line  in enumerate(sl_est_split):\n",
    "                if i==0:\n",
    "                    ax[0].plot([x/1e3 for x in line.coords.xy[0]], \n",
    "                               [y/1e3 for y in line.coords.xy[1]],\n",
    "                               '-', color='#f768a1', label='sl$_{estimated}$') \n",
    "                else: \n",
    "                    ax[0].plot([x/1e3 for x in line.coords.xy[0]], \n",
    "                               [y/1e3 for y in line.coords.xy[1]],\n",
    "                               '-', color='#f768a1', label='_nolegend_')                 \n",
    "                ax[1].plot([x/1e3 for x in line.coords.xy[0]], \n",
    "                           [y/1e3 for y in line.coords.xy[1]],\n",
    "                           '-', color='#f768a1', label='_nolegend_')\n",
    "            # plot observed snow line coordinates\n",
    "            ax[0].plot([x/1e3 for x in sl_obs_UTM.geometry[0].xy[0]], \n",
    "                       [y/1e3 for y in sl_obs_UTM.geometry[0].xy[1]], \n",
    "                       '.', color='#ae017e', label='sl$_{observed}$', markersize=2) \n",
    "            ax[1].plot([x/1e3 for x in sl_obs_UTM.geometry[0].xy[0]], \n",
    "                       [y/1e3 for y in sl_obs_UTM.geometry[0].xy[1]], \n",
    "                       '.', color='#ae017e', label='_nolegend_', markersize=2) \n",
    "            # add legends\n",
    "            ax[0].legend(loc='best')\n",
    "            ax[1].legend(loc='best')\n",
    "            if contour is not None:\n",
    "                ax[3].set_title('Contour = ' + str(np.round(contour,1)) + ' m')\n",
    "            fig.suptitle(date)\n",
    "            plt.show()\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            sl_est_elev_medians = sl_est_elev_medians + [np.nan]\n",
    "            print('no snow lines found, continuing...')\n",
    "        \n",
    " # -----Plot median snow line elevation\n",
    "fig2, ax2 = plt.subplots(figsize=(8,6))\n",
    "ax2.plot(np.arange(0, len(sl_obs_elev_medians)), \n",
    "         [x-y for x, y in list(zip(sl_est_elev_medians, sl_obs_elev_medians))], \n",
    "         '.b', label='observed')\n",
    "ax2.set_xlabel('observation #')\n",
    "ax2.set_ylabel('Misfit [m]')\n",
    "ax2.set_title('Median snow line elevation misfit (est. - obs.)')\n",
    "ax2.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4d39b2-2524-4583-b2bf-cfa24fe7cfaf",
   "metadata": {},
   "source": [
    "### Testing for one snow line..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6492ab-999a-4f41-b6df-9eab446c03e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAKE POLYGONS OF NOT SNOW-COVERED AREAS\n",
    "\n",
    "# ----Initialize observed and estimated snow line median elevation\n",
    "sl_obs_elev_medians = []\n",
    "sl_est_elev_medians = []\n",
    "\n",
    "# -----Loop through sites\n",
    "# for site_name in site_names:  \n",
    "site_name = site_names[0]\n",
    "    \n",
    "# define path to classified snow images\n",
    "im_path = base_path + '../study-sites/' + site_name + '/imagery/PlanetScope/'\n",
    "\n",
    "# define path to digitized snow lines\n",
    "sl_obs_path = base_path + '../snowline-package/' + site_name + '/snowlines/'\n",
    "\n",
    "# load AOI as gpd.GeoDataFrame\n",
    "AOI_fn = im_path + '../../glacier_outlines/' + site_name + '_USGS_*.shp'\n",
    "AOI_fn = glob.glob(AOI_fn)[0]\n",
    "AOI = gpd.read_file(AOI_fn)\n",
    "AOI_UTM = AOI\n",
    "\n",
    "# load DEM as xarray DataSet\n",
    "DEM_fn = im_path + '../../DEMs/' + site_name + '*_DEM/' + site_name + '*_DEM_filled.tif'\n",
    "DEM_fn = glob.glob(DEM_fn)[0]\n",
    "DEM_rio = rio.open(DEM_fn)\n",
    "DEM = xr.open_dataset(DEM_fn)\n",
    "DEM = DEM.rename({'band_data': 'elevation'})\n",
    "\n",
    "# load snow line shapefile names\n",
    "sl_obs_fns = glob.glob(sl_obs_path + '*.shp')\n",
    "sl_obs_fns.sort() # sort chronologically\n",
    "\n",
    "# initialize variables\n",
    "sl_obs_elevs = [None]*len(sl_obs_fns) # observed snow elevations\n",
    "datetimes = [None]*len(sl_obs_fns) # image datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c44d250-024e-41ea-8aaa-6e4a22ab5487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through observed snow lines\n",
    "# for sl_obs_fn in sl_obs_fns:\n",
    "sl_obs_fn = sl_obs_fns[1]\n",
    "\n",
    "# -----Load datasets\n",
    "# load snow line\n",
    "sl_obs = gpd.read_file(sl_obs_fn)\n",
    "# extract date from filename\n",
    "date = sl_obs_fn.split('/'+site_name+'_')[1][0:8]\n",
    "# reproject snow line to UTM\n",
    "sl_obs_UTM = sl_obs.to_crs(str(AOI_UTM.crs.to_epsg()))\n",
    "# open adjusted image of the same date\n",
    "im_adj_fn = glob.glob(im_path + 'adjusted-filtered/*' + date + '*.tif')[0] # define file name\n",
    "im_adj = rxr.open_rasterio(im_adj_fn) # open image as xarray.DataArray\n",
    "im_adj = im_adj / 1e4\n",
    " # open classified image from the same date\n",
    "im_classified_fn = glob.glob(im_path + 'classified/*' + date + '*.tif')[0] # define file name\n",
    "im_classified = rxr.open_rasterio(im_classified_fn) # open image as xarray.DataArray\n",
    "# create no data mask\n",
    "no_data_mask = xr.where(im_classified==-9999, 1, 0).data[0]\n",
    "# convert to polygons\n",
    "no_data_polygons = []\n",
    "for s, value in rio.features.shapes(no_data_mask.astype(np.int16), \n",
    "                                    mask=(no_data_mask >0), transform=rio.open(im_adj_fn).transform):\n",
    "    no_data_polygons.append(shape(s))\n",
    "no_data_polygons = MultiPolygon(no_data_polygons)\n",
    "# mask no data points in classified image\n",
    "im_classified = im_classified.where(im_classified!=-9999) # now, remove no data values\n",
    "\n",
    "# -----Interpolate elevations at observed snow line\n",
    "sl_obs_elev = np.array([DEM.sel(x=x, y=y, method='nearest').elevation.data \n",
    "                        for x, y in list(zip(sl_obs_UTM.geometry[0].xy[0], \n",
    "                                             sl_obs_UTM.geometry[0].xy[1]))])\n",
    "# calculate median snow line elevation\n",
    "sl_obs_elev_median = np.nanmedian(sl_obs_elev)\n",
    "sl_obs_elev_medians = sl_obs_elev_medians + [sl_obs_elev_median] # add to list\n",
    "       \n",
    "# -----Mask the DEM using the AOI\n",
    "# create AOI mask with DEM cordinates\n",
    "mask = rio.features.geometry_mask(AOI_UTM.geometry,\n",
    "                                  out_shape=(len(DEM.y), len(DEM.x)),\n",
    "                                  transform=DEM_rio.transform,\n",
    "                                  invert=True)\n",
    "# convert mask to xarray DataArray\n",
    "mask = xr.DataArray(mask , dims=(\"y\", \"x\"))\n",
    "# mask DEM values outside the AOI\n",
    "DEM_AOI = DEM.where(mask == True)\n",
    "\n",
    "# -----Interpolate DEM at the image coordinates\n",
    "# grab image indices\n",
    "band, x, y = im_classified.indexes.values() \n",
    "# interpolate DEM at image coordinates\n",
    "DEM_AOI_interp = DEM_AOI.interp(x=x, y=y, method=\"nearest\") \n",
    "\n",
    "# -----Determine snow covered elevations\n",
    "DEM_AOI_interp_snow = DEM_AOI_interp.where(im_classified<=2) # mask pixels not classified as snow\n",
    "snow_est_elev = DEM_AOI_interp_snow.elevation.data.flatten() # create array of snow-covered pixel elevations\n",
    "\n",
    "# -----Calculate elevation histograms\n",
    "# determine histogram bins using DEM elevation range\n",
    "elev_min = np.fix(np.nanmin(DEM_AOI_interp.elevation.data.flatten())/10)*10\n",
    "elev_max = np.round(np.nanmax(DEM_AOI_interp.elevation.data.flatten())/10)*10\n",
    "bin_edges = np.linspace(elev_min, elev_max, num=int((elev_max-elev_min)/10 + 1))\n",
    "bin_centers = (bin_edges[1:] + bin_edges[0:-1]) / 2\n",
    "# calculate elevation histograms\n",
    "H_DEM = np.histogram(DEM_AOI_interp.elevation.data.flatten(), bins=bin_edges)[0]\n",
    "H_snow_est_elev = np.histogram(snow_est_elev, bins=bin_edges)[0]\n",
    "H_snow_est_elev_norm = H_snow_est_elev / H_DEM\n",
    "\n",
    "# -----All pixels above the first elevation band with > 75% snow-coverage = snow\n",
    "# determine elevation with > 75% snow coverage\n",
    "if len(np.where(H_snow_est_elev_norm > 0.75))>1:\n",
    "    elev_75_snow = bin_centers[np.where(H_snow_est_elev_norm > 0.75)[0][0]]\n",
    "    # set all pixels above the elev_75_snow to snow (1)\n",
    "    im_classified_adj = xr.where(DEM_AOI_interp.elevation > elev_75_snow, 1, im_classified) # set all values above elev_75_snow to snow (1)\n",
    "    im_classified_adj = im_classified_adj.squeeze(drop=True) # drop unecessary dimensions\n",
    "else:\n",
    "    im_classified_adj = im_classified.squeeze(drop=True)\n",
    "\n",
    "# -----Generate and filter binary snow matrix\n",
    "# create binary snow matrix\n",
    "im_binary = xr.where(im_classified_adj > 2, 1, 0).data\n",
    "# apply median filter to binary image with kernel_size of 33 pixels (~99 m)\n",
    "im_binary_filt = medfilt(im_binary, kernel_size=33)\n",
    "# fill holes in binary image (0s within 1s = 1)\n",
    "im_binary_filt_no_holes = binary_fill_holes(im_binary_filt)\n",
    "\n",
    "# -----Find contours between 0 and 1 in binary image\n",
    "# Find contours at a constant value of 0.5 (between 0 and 1)\n",
    "contours = find_contours(im_binary_filt_no_holes, 0.5)\n",
    "# convert contour points to image coordinates\n",
    "contours_coords = []\n",
    "for contour in contours: \n",
    "    ix = np.round(contour[:,1]).astype(int)\n",
    "    iy = np.round(contour[:,0]).astype(int)\n",
    "    coords = (im_adj.isel(x=ix, y=iy).x.data, # image x coordinates\n",
    "              im_adj.isel(x=ix, y=iy).y.data) # image y coordinates\n",
    "    # zip points together\n",
    "    xy = list(zip([x for x in coords[0]], \n",
    "                  [y for y in coords[1]]))\n",
    "    contours_coords = contours_coords + [xy]\n",
    "\n",
    "# -----Create no-snow polygons\n",
    "c_polys = []\n",
    "for c in contours_coords:\n",
    "    c_points = [Point(x,y) for x,y in c]\n",
    "    c_poly = Polygon([[p.x, p.y] for p in c_points])\n",
    "    c_polys = c_polys + [c_poly]\n",
    "# only save the largest polygon by area\n",
    "if len(c_polys) > 1:\n",
    "    # calculate polygon areas\n",
    "    areas = np.array([poly.area for poly in c_polys])\n",
    "    # grab maximum area with its polygon index\n",
    "    areas_max = sorted(zip(areas, np.arange(0,len(c_polys))), reverse=True)[:1]\n",
    "    # grab indices\n",
    "    ic_polys_filt = [x[1] for x in areas_max]\n",
    "    # grab polygons at indices\n",
    "    c_polys_filt = [c_polys[i] for i in ic_polys_filt]\n",
    "else:\n",
    "    c_polys_filt = c_polys\n",
    "# extract coordinates in polygon\n",
    "polys_coords = [list(zip(c.exterior.coords.xy[0], c.exterior.coords.xy[1]))  for c in c_polys_filt]\n",
    "\n",
    "# -----Extract snow lines (sl) from contours\n",
    "# filter contours using no data and AOI masks (i.e., along glacier outline or data gaps)\n",
    "sl_est = [] # initialize list of snow lines\n",
    "min_sl_length = 200 # minimum snow line length\n",
    "for c in polys_coords:\n",
    "    # create array of points\n",
    "    c_points =  [Point(x,y) for x,y in c]\n",
    "    # loop through points \n",
    "    line_points = [] # initialize list of points to use in snow line\n",
    "    for point in c_points:\n",
    "        # calculate distance from the point to the no data polygons and the AOI boundary\n",
    "        distance_no_data = no_data_polygons.distance(point)\n",
    "        distance_AOI = AOI_UTM.boundary[0].distance(point)\n",
    "        # only include points 100 m from both\n",
    "        if (distance_no_data >= 100) and (distance_AOI >=100):\n",
    "            line_points = line_points + [point]\n",
    "    # print(len(line_points))\n",
    "    if line_points: # if list of line points is not empty\n",
    "        if len(line_points) > 1: # must have at least two points to create a LineString\n",
    "            line = LineString([(p.xy[0][0], p.xy[1][0]) for p in line_points])\n",
    "            if line.length > min_sl_length:\n",
    "                sl_est = sl_est + [line]\n",
    "\n",
    "    # -----Split line depending on distance between points\n",
    "    sl_est = sl_est[0]\n",
    "    max_dist = 100 # m\n",
    "    first_point = Point(sl_est.coords.xy[0][0], sl_est.coords.xy[1][0])\n",
    "    points = [Point(sl_est.coords.xy[0][i], sl_est.coords.xy[1][i]) \n",
    "              for i in np.arange(0,len(sl_est.coords.xy[0]))]\n",
    "    isplit = [0] # point indices where to split the line\n",
    "    for i, p in enumerate(points):\n",
    "        if i!=0:\n",
    "            dist = p.distance(points[i-1])\n",
    "            if dist > max_dist:\n",
    "                isplit.append(i)\n",
    "    isplit.append(len(points)) # add ending point to complete the last line\n",
    "    sl_est_split = [] # initialize split lines\n",
    "    # loop through split indices\n",
    "    if isplit:\n",
    "        for i, p in enumerate(isplit[:-1]):\n",
    "            if isplit[i+1]-isplit[i] > 1: # must have at least two points to make a line\n",
    "                line = LineString(points[isplit[i]:isplit[i+1]])\n",
    "                if line.length > min_sl_length:\n",
    "                    sl_est_split = sl_est_split + [line]\n",
    "    else:\n",
    "        sl_est_split = [sl_est]\n",
    " \n",
    " # -----Interpolate elevation at snow line points\n",
    "# compile all line coordinates into arrays of x- and y-coordinates \n",
    "xpts, ypts = [], []\n",
    "for line in sl_est_split:\n",
    "    xpts = xpts + [x for x in line.coords.xy[0]]\n",
    "    ypts = ypts + [y for y in line.coords.xy[1]]  \n",
    "xpts, ypts = np.array(xpts).flatten(), np.array(ypts).flatten()\n",
    "# interpolate elevation at snow line points\n",
    "sl_est_elev = [DEM.sel(x=x, y=y, method='nearest').elevation.data[0] \n",
    "               for x, y in list(zip(xpts, ypts))]\n",
    "# calculate median snow line elevation\n",
    "sl_est_elev_median = np.nanmedian(sl_est_elev)\n",
    "sl_est_elev_medians = sl_est_elev_medians + [sl_est_elev_median] # add to list\n",
    "        \n",
    "# -----Plot results\n",
    "contour = None\n",
    "fig, ax, sl_points_AOI = f.plot_im_classified_histogram_contour(im_adj, im_classified_adj, DEM, DEM_rio, AOI_UTM, contour)\n",
    "# plot estimated snow line coordinates\n",
    "for i, line  in enumerate(sl_est_split):\n",
    "    if i==0:\n",
    "        ax[0].plot([x/1e3 for x in line.coords.xy[0]], \n",
    "                   [y/1e3 for y in line.coords.xy[1]],\n",
    "                   '-', color='#f768a1', label='sl$_{estimated}$') \n",
    "    else: \n",
    "        ax[0].plot([x/1e3 for x in line.coords.xy[0]], \n",
    "                   [y/1e3 for y in line.coords.xy[1]],\n",
    "                   '-', color='#f768a1', label='_nolegend_')                 \n",
    "    ax[1].plot([x/1e3 for x in line.coords.xy[0]], \n",
    "               [y/1e3 for y in line.coords.xy[1]],\n",
    "               '-', color='#f768a1', label='_nolegend_')\n",
    "# plot observed snow line coordinates\n",
    "ax[0].plot([x/1e3 for x in sl_obs_UTM.geometry[0].xy[0]], \n",
    "           [y/1e3 for y in sl_obs_UTM.geometry[0].xy[1]], \n",
    "           '.', color='#ae017e', label='sl$_{observed}$', markersize=2) \n",
    "ax[1].plot([x/1e3 for x in sl_obs_UTM.geometry[0].xy[0]], \n",
    "           [y/1e3 for y in sl_obs_UTM.geometry[0].xy[1]], \n",
    "           '.', color='#ae017e', label='_nolegend_', markersize=2) \n",
    "# add legends\n",
    "ax[0].legend(loc='best')\n",
    "ax[1].legend(loc='best')\n",
    "if contour is not None:\n",
    "    ax[3].set_title('Contour = ' + str(np.round(contour,1)) + ' m')\n",
    "fig.suptitle(date)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80decfc-f5bc-44c4-a90f-2bb38de4b2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Interpolate elevation at snow line points\n",
    "# compile all line coordinates into arrays of x- and y-coordinates \n",
    "xpts, ypts = [], []\n",
    "for line in sl_est_split:\n",
    "    xpts = xpts + [x for x in line.coords.xy[0]]\n",
    "    ypts = ypts + [y for y in line.coords.xy[1]]  \n",
    "xpts, ypts = np.array(xpts).flatten(), np.array(ypts).flatten()\n",
    "# interpolate elevation at snow line points\n",
    "sl_est_elev = [DEM.sel(x=x, y=y, method='nearest').elevation.data[0] \n",
    "               for x, y in list(zip(xpts, ypts))]\n",
    "# calculate median snow line elevation\n",
    "sl_est_elev_median = np.nanmedian(sl_est_elev)\n",
    "sl_est_elev_medians = sl_est_elev_medians + [sl_est_elev_median] # add to list\n",
    "        \n",
    "# -----Plot results\n",
    "contour = None\n",
    "fig, ax, sl_points_AOI = f.plot_im_classified_histogram_contour(im_adj, im_classified_adj, DEM, DEM_rio, AOI_UTM, contour)\n",
    "# plot estimated snow line coordinates\n",
    "for i, line  in enumerate(sl_est_split):\n",
    "    if i==0:\n",
    "        ax[0].plot([x/1e3 for x in line.coords.xy[0]], \n",
    "                   [y/1e3 for y in line.coords.xy[1]],\n",
    "                   '-', color='#f768a1', label='sl$_{estimated}$') \n",
    "    else: \n",
    "        ax[0].plot([x/1e3 for x in line.coords.xy[0]], \n",
    "                   [y/1e3 for y in line.coords.xy[1]],\n",
    "                   '-', color='#f768a1', label='_nolegend_')                 \n",
    "    ax[1].plot([x/1e3 for x in line.coords.xy[0]], \n",
    "               [y/1e3 for y in line.coords.xy[1]],\n",
    "               '-', color='#f768a1', label='_nolegend_')\n",
    "# plot observed snow line coordinates\n",
    "ax[0].plot([x/1e3 for x in sl_obs_UTM.geometry[0].xy[0]], \n",
    "           [y/1e3 for y in sl_obs_UTM.geometry[0].xy[1]], \n",
    "           '.', color='#ae017e', label='sl$_{observed}$', markersize=2) \n",
    "ax[1].plot([x/1e3 for x in sl_obs_UTM.geometry[0].xy[0]], \n",
    "           [y/1e3 for y in sl_obs_UTM.geometry[0].xy[1]], \n",
    "           '.', color='#ae017e', label='_nolegend_', markersize=2) \n",
    "# add legends\n",
    "ax[0].legend(loc='best')\n",
    "ax[1].legend(loc='best')\n",
    "if contour is not None:\n",
    "    ax[3].set_title('Contour = ' + str(np.round(contour,1)) + ' m')\n",
    "fig.suptitle(date)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9da3ac-d2be-4010-96ab-bda8a9ddb5c3",
   "metadata": {},
   "source": [
    "## 2. Solve for the optimal snow elevation percentile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee322bb-261c-4046-b3aa-2c4154d5bf3e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Loop through sites\n",
    "percentiles = [] # initialize optimal percentiles for estimating snow line elevation\n",
    "misfits = [] # initialize misfits between estimated and observed snow line elevation\n",
    "for site_name in site_names:\n",
    "    \n",
    "    # define path to classified snow images\n",
    "    im_path = base_path + '../study-sites/' + site_name + '/imagery/PlanetScope/'\n",
    "    \n",
    "    # define path to digitized snow lines\n",
    "    sl_path = base_path + '../snowline-package/' + site_name + '/snowlines/'\n",
    "    \n",
    "    # load AOI as geopandas DataFrame\n",
    "    AOI_fn = base_path + '../../GIS_data/RGI_outlines/' + site_name + '_RGI.shp'\n",
    "    AOI = gpd.read_file(AOI_fn)\n",
    "    \n",
    "    # create a mask using the AOI\n",
    "    mask = rio.features.geometry_mask(AOI_UTM.geometry,\n",
    "                                      out_shape=(len(DEM.y), len(DEM.x)),\n",
    "                                      transform=DEM.transform,\n",
    "                                      invert=True)\n",
    "    # convert to xarray DataArray\n",
    "    mask = xr.DataArray(mask , dims=(\"y\", \"x\"))\n",
    "    # mask the DEM outside the AOI\n",
    "    DEM_AOI = DEM.where(mask == True)\n",
    "\n",
    "    # load snow line file names\n",
    "    sl_obs_fns = glob.glob(sl_path+'*.shp')\n",
    "    sl_obs_fns.sort() # sort chronologically\n",
    "    \n",
    "    # initialize variables\n",
    "    sl_obs_elev_medians = [None]*len(sl_obs_fns) # median observed snow line elevations\n",
    "    sl_est_elevs = [None]*len(sl_obs_fns) # median estimated snow line elevations\n",
    "    # loop through observed snow lines\n",
    "    for sl_obs_fn in sl_obs_fns:\n",
    "        \n",
    "        # load snow line\n",
    "        sl_obs = gpd.read_file(sl_obs_fn)\n",
    "        # extract date from filename\n",
    "        date = sl_obs_fn.split('/'+site_name+'_')[1][0:8]\n",
    "        # reproject snow line to UTM\n",
    "        sl_obs_UTM = sl_obs.to_crs(str(AOI_UTM.crs.to_epsg()))\n",
    "        # interpolate elevations at snow line\n",
    "        sl_obs_elev = np.array([DEM.sel(x=x, y=y, method='nearest').elevation.data \n",
    "                                for x, y in list(zip(sl_obs_UTM.geometry[0].xy[0], \n",
    "                                                     sl_obs_UTM.geometry[0].xy[1]))])\n",
    "        # calculate median snow line elevation\n",
    "        sl_obs_elev_median = np.nanmedian(sl_obs_elev)\n",
    "        sl_obs_elev_medians = sl_obs_elev_medians + [sl_obs_elev_median] # add to list\n",
    "\n",
    "        # open adjusted image of the same date\n",
    "        im_adj_fn = glob.glob(im_path + 'adjusted-filtered/*' + date + '*.tif')[0] # define file name\n",
    "        im_adj = rxr.open_rasterio(im_adj_fn) # open image as xarray.DataArray\n",
    "        im_adj = im_adj.where(im_adj!=-9999) # remove no data values\n",
    "        im_adj = im_adj / 10000 # account for surface reflectance scalar multiplier\n",
    "\n",
    "        # open classified image from the same date\n",
    "        im_classified_fn = glob.glob(im_path + 'classified/*' + date + '*.tif')[0] # define file name\n",
    "        im_classified = rxr.open_rasterio(im_classified_fn) # open image as xarray.DataArray\n",
    "        im_classified = im_classified.where(im_classified!=-9999) # remove no data values\n",
    "\n",
    "        # mask the DEM using the AOI\n",
    "        mask = rio.features.geometry_mask(AOI_UTM.geometry,\n",
    "                                          out_shape=(len(DEM.y), len(DEM.x)),\n",
    "                                          transform=DEM.transform,\n",
    "                                          invert=True)\n",
    "        # convert mask to xarray DataArray\n",
    "        mask = xr.DataArray(mask , dims=(\"y\", \"x\"))\n",
    "        # mask DEM values outside the AOI\n",
    "        DEM_AOI = DEM.where(mask == True)\n",
    "        \n",
    "        # interpolate DEM to the image coordinates\n",
    "        band, x, y = im_classified.indexes.values() # grab indices of image\n",
    "        DEM_AOI_interp = DEM_AOI.interp(x=x, y=y, method=\"nearest\") # interpolate DEM to image coordinates\n",
    "\n",
    "        # determine snow covered elevations\n",
    "        DEM_AOI_interp_snow = DEM_AOI_interp.where(im_classified<=2) # mask pixels not classified as snow\n",
    "        snow_est_elev = DEM_AOI_interp_snow.elevation.data.flatten() # create array of snow-covered pixel elevations\n",
    "\n",
    "        # determine bins to use in histograms\n",
    "        elev_min = np.fix(np.nanmin(DEM_AOI_interp.elevation.data.flatten())/10)*10\n",
    "        elev_max = np.round(np.nanmax(DEM_AOI_interp.elevation.data.flatten())/10)*10\n",
    "        bin_edges = np.linspace(elev_min, elev_max, num=int((elev_max-elev_min)/10 + 1))\n",
    "        bin_centers = (bin_edges[1:] + bin_edges[0:-1]) / 2\n",
    "\n",
    "        # calculate elevation histograms\n",
    "        H_DEM = np.histogram(DEM_AOI_interp.elevation.data.flatten(), bins=bin_edges)[0]\n",
    "        H_snow_est_elev = np.histogram(snow_est_elev, bins=bin_edges)[0]\n",
    "        H_snow_est_elev_norm = H_snow_est_elev / H_DEM\n",
    "\n",
    "        # determine elevation with > 75% snow coverage\n",
    "        elev_75_snow = bin_centers[np.where(H_snow_est_elev_norm > 0.75)[0][0]]\n",
    "\n",
    "        # set all pixels above the elev_75_snow to snow (1)\n",
    "        im_classified_adj = xr.where(DEM_AOI_interp.elevation > elev_75_snow, 1, im_classified) # set all values above elev_75_snow to snow (1)\n",
    "        im_classified_adj = im_classified_adj.squeeze(drop=True) # drop unecessary dimensions\n",
    "        \n",
    "        # re-determine snow-covered elevations using adjusted image\n",
    "        DEM_AOI_interp_snow_adj = DEM_AOI_interp.where(im_classified_adj<=2) # mask pixels not classified as snow\n",
    "        snow_est_elev_adj = DEM_AOI_interp_snow_adj.elevation.data.flatten() # create array of snow-covered pixel elevations\n",
    "        snow_est_elev_adj = snow_est_elev_adj[~np.isnan(snow_est_elev_adj)] # remove NaN values\n",
    "        \n",
    "        # determine the optimal percentile for estimating snow line elevation in classified image\n",
    "        # using the median observed snow line elevation\n",
    "        percentile = stats.percentileofscore(snow_est_elev_adj, sl_obs_elev_median, kind='rank')\n",
    "        percentiles = percentiles + [percentile] # concatenate to list\n",
    "        \n",
    "        # extract snow line elevation in the classified image using the optimal percentile\n",
    "        sl_est_elev = np.nanpercentile(snow_est_elev_adj, percentile)\n",
    "        sl_est_elevs = sl_est_elevs + [sl_est_elev]\n",
    "        \n",
    "        # plot results\n",
    "        fig, ax, sl_points_AOI = f.plot_im_classified_histogram_contour(im_adj, im_classified_adj, DEM, AOI_UTM, sl_est_elev)\n",
    "        # plot snow observed snow line elevation\n",
    "        ax[0].plot([x/1e3 for x in sl_obs_UTM.geometry[0].xy[0]], \n",
    "                   [y/1e3 for y in sl_obs_UTM.geometry[0].xy[1]], \n",
    "                   '.', color='#ae017e', label='sl$_{observed}$', markersize=3) \n",
    "        ax[1].plot([x/1e3 for x in sl_obs_UTM.geometry[0].xy[0]], \n",
    "                   [y/1e3 for y in sl_obs_UTM.geometry[0].xy[1]], \n",
    "                   '.', color='#ae017e', label='_nolegend_', markersize=3)  \n",
    "        # add legends\n",
    "        ax[0].legend(loc='best')\n",
    "        ax[1].legend(loc='best')\n",
    "        ax[3].set_title('Contour = P'+str(np.round(percentile,2)))\n",
    "        fig.suptitle(date)\n",
    "        plt.show()\n",
    "\n",
    "# Display boxplot of percentiles\n",
    "plt.boxplot(percentiles)\n",
    "plt.title('Percentiles distribution')\n",
    "plt.show()\n",
    "\n",
    "# Determine median optimal snow elevation percentile\n",
    "percentiles_median = np.median(percentiles)\n",
    "print('Median optimal snow elevation percentile = ', np.round(percentiles_median,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a41b435-0a48-45ab-8edd-3aa06232a62d",
   "metadata": {},
   "source": [
    "#### Plot classified images with optimal snow elevation percentile and observed snow line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad1007f-a52b-4297-8702-7f1e6e539256",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Loop through sites\n",
    "misfits = []    # initialize list of misfits between \n",
    "                # observed and estimated snow line elevation\n",
    "for site_name in site_names:\n",
    "    \n",
    "    # define path to classified snow images\n",
    "    im_path = base_path + '../study-sites/' + site_name + '/imagery/PlanetScope/'\n",
    "    \n",
    "    # define path to digitized snow lines\n",
    "    sl_path = base_path + '../snowline-package/' + site_name + '/snowlines/'\n",
    "    \n",
    "    # load AOI as gpd.GeoDataFrame\n",
    "    AOI_fn = base_path + '../../GIS_data/RGI_outlines/' + site_name + '_RGI.shp'\n",
    "    AOI = gpd.read_file(AOI_fn)\n",
    "    \n",
    "    # query GEE for DEM\n",
    "    DEM, AOI_UTM = f.query_GEE_for_DEM(AOI)\n",
    "\n",
    "    # load snow line shapefile names\n",
    "    sl_obs_fns = glob.glob(sl_path+'*.shp')\n",
    "    sl_obs_fns.sort() # sort chronologically\n",
    "    \n",
    "    # initialize variables\n",
    "    sl_obs_elev_medians = [None]*len(sl_obs_fns) # median observed snow line elevations\n",
    "    sl_est_elevs = [None]*len(sl_obs_fns) # estimated snow line elevation (i.e., optimal percentile of snow elevations)\n",
    "    # loop through observed snow lines\n",
    "    for sl_obs_fn in sl_obs_fns:\n",
    "        \n",
    "        # load snow line\n",
    "        sl_obs = gpd.read_file(sl_obs_fn)\n",
    "        # extract date from filename\n",
    "        date = sl_obs_fn.split('/'+site_name+'_')[1][0:8]\n",
    "        # reproject snow line to UTM\n",
    "        sl_obs_UTM = sl_obs.to_crs(str(AOI_UTM.crs.to_epsg()))\n",
    "        # interpolate elevations at snow line\n",
    "        sl_obs_elev = np.array([DEM.sel(time=DEM.time.data[0], x=x, y=y, method='nearest').elevation.data \n",
    "                                for x, y in list(zip(sl_obs_UTM.geometry[0].xy[0], \n",
    "                                                     sl_obs_UTM.geometry[0].xy[1]))])\n",
    "        # calculate median snow line elevation\n",
    "        sl_obs_elev_median = np.nanmedian(sl_obs_elev)\n",
    "        sl_obs_elev_medians = sl_obs_elev_medians + [sl_obs_elev_median] # add to list\n",
    "\n",
    "        # open adjusted image of the same date\n",
    "        im_adj_fn = glob.glob(im_path + 'adjusted-filtered/*' + date + '*.tif')[0] # define file name\n",
    "        im_adj = rxr.open_rasterio(im_adj_fn) # open image as xarray.DataArray\n",
    "        im_adj = im_adj.where(im_adj!=-9999) # remove no data values\n",
    "        im_adj = im_adj / 10000 # account for surface reflectance scalar multiplier\n",
    "\n",
    "        # open classified image from the same date\n",
    "        im_classified_fn = glob.glob(im_path + 'classified/*' + date + '*.tif')[0] # define file name\n",
    "        im_classified = rxr.open_rasterio(im_classified_fn) # open image as xarray.DataArray\n",
    "        im_classified = im_classified.where(im_classified!=-9999) # remove no data values\n",
    "\n",
    "        # mask the DEM using the AOI\n",
    "        mask = rio.features.geometry_mask(AOI_UTM.geometry,\n",
    "                                          out_shape=(len(DEM.y), len(DEM.x)),\n",
    "                                          transform=DEM.transform,\n",
    "                                          invert=True)\n",
    "        # convert mask to xarray DataArray\n",
    "        mask = xr.DataArray(mask , dims=(\"y\", \"x\"))\n",
    "        # mask DEM values outside the AOI\n",
    "        DEM_AOI = DEM.where(mask == True)\n",
    "        \n",
    "        # interpolate DEM to the image coordinates\n",
    "        band, x, y = im_classified.indexes.values() # grab indices of image\n",
    "        DEM_AOI_interp = DEM_AOI.interp(x=x, y=y, method=\"nearest\") # interpolate DEM to image coordinates\n",
    "\n",
    "        # determine snow covered elevations\n",
    "        DEM_AOI_interp_snow = DEM_AOI_interp.where(im_classified<=2) # mask pixels not classified as snow\n",
    "        snow_est_elev = DEM_AOI_interp_snow.elevation.data.flatten() # create array of snow-covered pixel elevations\n",
    "\n",
    "        # determine bins to use in histograms\n",
    "        elev_min = np.fix(np.nanmin(DEM_AOI_interp.elevation.data.flatten())/10)*10\n",
    "        elev_max = np.round(np.nanmax(DEM_AOI_interp.elevation.data.flatten())/10)*10\n",
    "        bin_edges = np.linspace(elev_min, elev_max, num=int((elev_max-elev_min)/10 + 1))\n",
    "        bin_centers = (bin_edges[1:] + bin_edges[0:-1]) / 2\n",
    "\n",
    "        # calculate elevation histograms\n",
    "        H_DEM = np.histogram(DEM_AOI_interp.elevation.data.flatten(), bins=bin_edges)[0]\n",
    "        H_snow_est_elev = np.histogram(snow_est_elev, bins=bin_edges)[0]\n",
    "        H_snow_est_elev_norm = H_snow_est_elev / H_DEM\n",
    "\n",
    "        # determine elevation with > 75% snow coverage\n",
    "        elev_75_snow = bin_centers[np.where(H_snow_est_elev_norm > 0.75)[0][0]]\n",
    "\n",
    "        # set all pixels above the elev_75_snow to snow (1)\n",
    "        im_classified_adj = xr.where(DEM_AOI_interp.elevation > elev_75_snow, 1, im_classified) # set all values above elev_75_snow to snow (1)\n",
    "        im_classified_adj = im_classified_adj.squeeze(drop=True) # drop unecessary dimensions\n",
    "        \n",
    "        # re-determine snow-covered elevations using adjusted image\n",
    "        DEM_AOI_interp_snow_adj = DEM_AOI_interp.where(im_classified_adj<=2) # mask pixels not classified as snow\n",
    "        snow_est_elev_adj = DEM_AOI_interp_snow_adj.elevation.data.flatten() # create array of snow-covered pixel elevations\n",
    "        snow_est_elev_adj = snow_est_elev_adj[~np.isnan(snow_est_elev_adj)] # remove NaN values\n",
    "        \n",
    "        # extract snow line elevation in the classified image using the optimal percentile\n",
    "        sl_est_elev = np.nanpercentile(snow_est_elev_adj, percentiles_median)\n",
    "        sl_est_elevs = sl_est_elevs + [sl_est_elev]\n",
    "        print('Observed snow line elevation: ', sl_obs_elev_median, ' m')\n",
    "        print('Estimated snow line elevation: ', sl_est_elev, ' m')\n",
    "        \n",
    "        # estimate misfit between observed and estimated snow line elevation\n",
    "        # misfit = estimated - observed\n",
    "        misfit =  sl_est_elev - sl_obs_elev_median\n",
    "        print('Misfit: ' + str(misfit) + ' m')\n",
    "        misfits = misfits + [misfit]\n",
    "        \n",
    "        # plot results\n",
    "        fig, ax, sl_points_AOI = f.plot_im_classified_histogram_contour(im_adj, im_classified_adj, DEM, AOI_UTM, sl_est_elev)\n",
    "        # plot snow observed snow line elevation\n",
    "        ax[0].plot([x/1e3 for x in sl_obs_UTM.geometry[0].xy[0]], \n",
    "                   [y/1e3 for y in sl_obs_UTM.geometry[0].xy[1]], \n",
    "                   '.', color='#ae017e', label='sl$_{observed}$', markersize=3) \n",
    "        ax[1].plot([x/1e3 for x in sl_obs_UTM.geometry[0].xy[0]], \n",
    "                   [y/1e3 for y in sl_obs_UTM.geometry[0].xy[1]], \n",
    "                   '.', color='#ae017e', label='_nolegend_', markersize=3)  \n",
    "        # add legends\n",
    "        ax[0].legend(loc='best')\n",
    "        ax[1].legend(loc='best')\n",
    "        ax[3].set_title('Contour = P'+str(np.round(percentiles_median,2)))\n",
    "        fig.suptitle(date)\n",
    "        plt.show()\n",
    "\n",
    "# Display boxplot of snow line elevation misfits\n",
    "plt.boxplot(misfits)\n",
    "plt.title('Snow line elevation misfits')\n",
    "plt.ylabel('Misfit [m]')\n",
    "plt.show()\n",
    "print('Median misfit magnitude: ' + str(np.round(np.nanmedian(np.abs(misfits)),2)), ' m')\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a564c3-4a1c-4a8a-8e5c-9116894d06dd",
   "metadata": {},
   "source": [
    "## 3. Edge detection in classified images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40b2c7a-b41d-49f8-9285-2231fea4df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = im_classified_adj.data #skimage.io.imread(fname=im_classified_fn, as_gray=True)\n",
    "image[image!=1] = 0\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "skimage.io.imshow(image, cmap='Greys')\n",
    "plt.show()\n",
    "\n",
    "# Canny edge detection\n",
    "edges = feature.canny(image=image, low_threshold=2, high_threshold=5)\n",
    "# display edges\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "skimage.io.imshow(edges)\n",
    "plt.show()\n",
    "\n",
    "# Other edge filters\n",
    "from skimage import filters\n",
    "edge_roberts = filters.roberts(image)\n",
    "edge_sobel = filters.sobel(image)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, sharex=True, sharey=True,\n",
    "                         figsize=(16, 8))\n",
    "\n",
    "axes[0].imshow(edge_roberts, cmap='Greys')\n",
    "axes[0].set_title('Roberts Edge Detection')\n",
    "\n",
    "axes[1].imshow(edge_sobel, cmap='Greys')\n",
    "axes[1].set_title('Sobel Edge Detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc67be27-f1d7-4805-b292-960b3c0fc099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chan-Vese Segementation\n",
    "\n",
    "from skimage.filters import gaussian\n",
    "from skimage.segmentation import chan_vese\n",
    "\n",
    "im = im_classified.data[0]\n",
    "im[im!=1] = 0\n",
    "\n",
    "cv = chan_vese(im, mu=0.25, lambda1=1, lambda2=1, tol=1e-3,\n",
    "               max_num_iter=200, dt=0.5, init_level_set=\"checkerboard\",\n",
    "               extended_output=True)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(7, 12))\n",
    "ax[0].imshow(im_classified.data[0], cmap=plt.cm.gray)\n",
    "ax[1].imshow(cv[0], cmap='Greys')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d41a1ed-f1bb-464f-86fc-b8ad775a1ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planet-snow",
   "language": "python",
   "name": "planet-snow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
