{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bab1b699",
   "metadata": {},
   "source": [
    "# Classify snow-covered area (SCA) in Sentinel-2, Landsat 8/9, and PlanetScope imagery: full pipelines\n",
    "\n",
    "Rainey Aberle\n",
    "\n",
    "Department of Geosciences, Boise State University\n",
    "\n",
    "2022\n",
    "\n",
    "### Requirements:\n",
    "- Area of Interest (AOI) shapefile: where snow will be classified in all available images. \n",
    "- Google Earth Engine (GEE) account: used to pull DEM over the AOI. Sign up for a free account [here](https://earthengine.google.com/new_signup/). \n",
    "- Digital elevation model (DEM) (_optional_): used to extract elevations over the AOI and for each snowline. If no DEM is provided, the ASTER Global DEM will be loaded through GEE. \n",
    "\n",
    "### Outline:\n",
    "__0. Setup__ paths in directory, file locations, authenticate GEE - _modify this section!_\n",
    "\n",
    "__1. Sentinel-2 Top of Atmosphere (TOA) imagery:__ full pipeline\n",
    "\n",
    "__2. Sentinel-2 Surface Reflectance (SR) imagery:__ full pipeline\n",
    "\n",
    "__3. Landsat 8/9 Surface Reflectance (SR) imagery:__ full pipeline\n",
    "\n",
    "__4. PlanetScope Surface Reflectance (SR) imagery:__ full pipeline\n",
    "\n",
    "-------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e4e890",
   "metadata": {},
   "source": [
    "### 0. Setup\n",
    "\n",
    "#### Define paths in directory and desired settings. \n",
    "Modify lines located within the following:\n",
    "\n",
    "`#### MODIFY HERE ####`  \n",
    "\n",
    "`#####################`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337a86c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MODIFY HERE #####\n",
    "\n",
    "# -----Paths in directory\n",
    "site_name = 'Wolverine'\n",
    "# path to snow-cover-mapping/ - Make sure you include a \"/\" at the end\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/'\n",
    "# path to AOI including the name of the shapefile\n",
    "AOI_path = base_path + '../study-sites/' + site_name + '/AOIs/'\n",
    "# AOI file name\n",
    "AOI_fn =  'Wolverine_USGS_glacier_outline_2020.shp'\n",
    "# path to DEM including the name of the tif file\n",
    "# Note: set DEM_path==None and DEM_fn=None if you want to use the ASTER GDEM via Google Earth Engine\n",
    "DEM_path = base_path + '../study-sites/' + site_name + '/DEMs/'\n",
    "# DEM file name\n",
    "DEM_fn = 'Wolverine_20200502_DEM_filled.tif'\n",
    "# path for output images\n",
    "out_path = base_path + '../study-sites/' + site_name + '/imagery/'\n",
    "# path to PlanetScope images\n",
    "# Note: set PS_im_path=None if not using PlanetScope\n",
    "PS_im_path = out_path + 'PlanetScope/raw_images/'\n",
    "# path for output figures\n",
    "figures_out_path = base_path + '../study-sites/' + site_name + '/figures/'\n",
    "\n",
    "# -----Define image search filters\n",
    "date_start = '2013-05-01'\n",
    "date_end = '2023-01-01'\n",
    "month_start = 5\n",
    "month_end = 11\n",
    "cloud_cover_max = 100\n",
    "\n",
    "# -----Determine settings\n",
    "plot_results = True # = True to plot figures of results for each image where applicable\n",
    "skip_clipped = False # = True to skip images where bands appear \"clipped\", i.e. max(blue) < 0.8\n",
    "crop_to_AOI = True # = True to crop images to AOI before calculating SCA\n",
    "save_outputs = True # = True to save SCAs and snowlines to file\n",
    "save_figures = True # = True to save SCA output figures to file\n",
    "\n",
    "#######################\n",
    "\n",
    "# -----Import packages\n",
    "import xarray as xr\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from matplotlib import pyplot as plt, dates\n",
    "import matplotlib\n",
    "import rasterio as rio\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import sys\n",
    "import ee\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# -----Set paths for output files\n",
    "PS_im_masked_path = out_path + 'PlanetScope/masked/'\n",
    "PS_im_mosaics_path = out_path + 'PlanetScope/mosaics/'\n",
    "im_classified_path = out_path + 'classified/'\n",
    "snowlines_path = out_path + 'snowlines/'\n",
    "\n",
    "# -----Add path to functions\n",
    "sys.path.insert(1, base_path+'functions/')\n",
    "import pipeline_utils as f\n",
    "\n",
    "# -----Load dataset dictionary\n",
    "with open(base_path + 'inputs-outputs/datasets_characteristics.pkl', 'rb') as fn:\n",
    "    dataset_dict = pickle.load(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7e78f7",
   "metadata": {},
   "source": [
    "#### Authenticate and initialize Google Earth Engine (GEE). \n",
    "\n",
    "__Note:__ The first time you run the following cell, you will be asked to authenticate your GEE account for use in this notebook. This will send you to an external web page, where you will walk through the GEE authentication workflow and copy an authentication code back into the space below this cell when prompted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d23d4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ee.Initialize()\n",
    "except: \n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ca0891",
   "metadata": {},
   "source": [
    "#### Load AOI and DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4807ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Load AOI as gpd.GeoDataFrame\n",
    "AOI = gpd.read_file(AOI_path + AOI_fn)\n",
    "# reproject the AOI to WGS to solve for the optimal UTM zone\n",
    "AOI_WGS = AOI.to_crs(4326)\n",
    "AOI_WGS_centroid = [AOI_WGS.geometry[0].centroid.xy[0][0],\n",
    "                    AOI_WGS.geometry[0].centroid.xy[1][0]]\n",
    "# grab the optimal UTM zone EPSG code\n",
    "epsg_UTM = f.convert_wgs_to_utm(AOI_WGS_centroid[0], AOI_WGS_centroid[1])\n",
    "    \n",
    "# -----Load DEM as Xarray DataSet\n",
    "if DEM_fn==None:\n",
    "    # query GEE for DEM\n",
    "    DEM, AOI_UTM = f.query_GEE_for_DEM(AOI)\n",
    "else:\n",
    "    # reproject AOI to UTM\n",
    "    AOI_UTM = AOI.to_crs('EPSG:'+str(epsg_UTM))\n",
    "    # load DEM as xarray DataSet\n",
    "    DEM = xr.open_dataset(DEM_path + DEM_fn)\n",
    "    DEM = DEM.rename({'band_data': 'elevation'})\n",
    "    # reproject the DEM to the optimal UTM zone\n",
    "    DEM = DEM.rio.reproject(str('EPSG:'+epsg_UTM))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aa368c-873b-416d-8edd-2939c456fee0",
   "metadata": {},
   "source": [
    "## 1. Sentinel-2 TOA imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c441a7-c6ac-430d-b75b-3ae03fea11ea",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Sentinel-2 TOA')\n",
    "print('----------')\n",
    "\n",
    "# -----Load trained classifier and feature columns\n",
    "clf_fn = base_path+'inputs-outputs/S2_TOA_classifier_all_sites.sav'\n",
    "clf = pickle.load(open(clf_fn, 'rb'))\n",
    "feature_cols_fn = base_path+'inputs-outputs/S2_TOA_feature_cols.pkl'\n",
    "feature_cols = pickle.load(open(feature_cols_fn,'rb'))\n",
    "\n",
    "# -----Query GEE for imagery\n",
    "dataset = 'Sentinel2_TOA'\n",
    "im_list = f.query_GEE_for_Sentinel2(dataset, dataset_dict, site_name, \n",
    "                                       AOI_UTM, date_start, date_end, month_start, \n",
    "                                       month_end, cloud_cover_max)\n",
    "im_list_size = im_list.size().getInfo()\n",
    "\n",
    "# -----Loop through images\n",
    "if im_list_size==0: # check that images were found\n",
    "    print('No images found to classify, quiting...')\n",
    "else:\n",
    "    \n",
    "    for i in range(0, im_list_size):\n",
    "        \n",
    "        # -----Select image by index\n",
    "        im = ee.Image(ee.List(im_list).get(i))\n",
    "        # get image time\n",
    "        im_date = im.date().format(None, 'GMT').getInfo()\n",
    "        print(' ')\n",
    "        print(str(i+1)+'/'+str(im_list_size))\n",
    "        print(im_date)\n",
    "                \n",
    "            \n",
    "        # -----Check if classified image and snowline already exists in file\n",
    "        im_classified_fn = im_date.replace('-','').replace(':','') + '_' + site_name + '_' + dataset + '_classified.nc'\n",
    "        snowline_fn = im_date.replace('-','').replace(':','') + '_' + site_name + '_' + dataset + '_snowline.pkl'\n",
    "        if os.path.exists(im_classified_path + im_classified_fn) & os.path.exists(snowlines_path + snowline_fn):\n",
    "            \n",
    "            print('Classified image already exists in file, loading...')\n",
    "            im_classified = xr.open_dataset(im_classified_path + im_classified_fn)\n",
    "            print('Snowline already exists in file, loading...')\n",
    "            snowline_df = pd.read_pickle(snowlines_path + snowline_fn)\n",
    "        \n",
    "        else:  \n",
    "            # -----Convert image to xarray.Dataset\n",
    "            if site_name=='Gulkana':\n",
    "                res = 12\n",
    "            else:\n",
    "                res = dataset_dict[dataset]['resolution_m']\n",
    "            im_xr = im.wx.to_xarray(scale=res, crs='EPSG:4326')\n",
    "            # reproject to UTM CRS\n",
    "            im_xr_UTM = im_xr.rio.reproject('EPSG:'+epsg_UTM)\n",
    "            # replace no data values with NaN and account for image scalar\n",
    "            bands = [band for band in dataset_dict[dataset]['bands'] if 'QA' not in band]\n",
    "            for band in bands:\n",
    "                im_xr_UTM[band] = xr.where(im_xr_UTM[band] != dataset_dict[dataset]['no_data_value'],\n",
    "                                           im_xr_UTM[band] / dataset_dict[dataset]['im_scalar'], np.nan)\n",
    "\n",
    "            # -----Add NDSI band\n",
    "            im_xr_UTM['NDSI'] = ((im_xr_UTM[dataset_dict[dataset]['NDSI'][0]] - im_xr_UTM[dataset_dict[dataset]['NDSI'][1]]) \n",
    "                                 / (im_xr_UTM[dataset_dict[dataset]['NDSI'][0]] + im_xr_UTM[dataset_dict[dataset]['NDSI'][1]]))\n",
    "\n",
    "            # -----Classify image\n",
    "            im_classified = f.classify_image(im_xr_UTM, clf, feature_cols, crop_to_AOI, \n",
    "                                             AOI_UTM, dataset, dataset_dict, site_name, \n",
    "                                             im_classified_fn, im_classified_path)\n",
    "            if type(im_classified)==str:\n",
    "                continue\n",
    "        \n",
    "            # -----Delineate snowline(s)\n",
    "            plot_results = True\n",
    "            snowline_df = f.delineate_im_snowline(im_xr_UTM, im_classified, site_name, AOI_UTM, DEM, \n",
    "                                                  dataset_dict, dataset, im_date, snowline_fn, \n",
    "                                                  snowlines_path, figures_out_path, plot_results)\n",
    "            \n",
    "        print('Median snowline elevation: ' + str(snowline_df['snowlines_elevs_median'][0]) + ' m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa58f9f3-f049-4fdd-9475-8d873d808456",
   "metadata": {},
   "source": [
    "## 2. Sentinel-2 SR imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4daa67-5289-4fad-b26d-fdb60e94b9fe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Load trained classifier and feature columns\n",
    "clf_fn = base_path+'inputs-outputs/S2_SR_classifier_all_sites.sav'\n",
    "clf = pickle.load(open(clf_fn, 'rb'))\n",
    "feature_cols_fn = base_path+'inputs-outputs/S2_SR_feature_cols.pkl'\n",
    "feature_cols = pickle.load(open(feature_cols_fn,'rb'))\n",
    "\n",
    "# -----Query GEE for imagery\n",
    "dataset = 'Sentinel2_SR'\n",
    "im_list = f.query_GEE_for_Sentinel2(dataset, dataset_dict, site_name, \n",
    "                                       AOI_UTM, date_start, date_end, month_start, \n",
    "                                       month_end, cloud_cover_max)\n",
    "im_list_size = im_list.size().getInfo()\n",
    "\n",
    "# -----Loop through images\n",
    "if im_list_size==0: # check that images were found\n",
    "    print('No images found to classify, quiting...')\n",
    "else:\n",
    "    \n",
    "    for i in range(0, im_list_size):\n",
    "        \n",
    "        # -----Select image by index\n",
    "        im = ee.Image(ee.List(im_list).get(i))\n",
    "        # get image time\n",
    "        im_date = im.date().format(None, 'GMT').getInfo()\n",
    "        print(' ')\n",
    "        print(str(i+1)+'/'+str(im_list_size))\n",
    "        print(im_date)\n",
    "                \n",
    "            \n",
    "        # -----Check if classified image and snowline already exists in file\n",
    "        im_classified_fn = im_date.replace('-','').replace(':','') + '_' + site_name + '_' + dataset + '_classified.nc'\n",
    "        snowline_fn = im_date.replace('-','').replace(':','') + '_' + site_name + '_' + dataset + '_snowline.pkl'\n",
    "        if os.path.exists(im_classified_path + im_classified_fn) & os.path.exists(snowlines_path + snowline_fn):\n",
    "            \n",
    "            print('Classified image already exists in file, loading...')\n",
    "            im_classified = xr.open_dataset(im_classified_path + im_classified_fn)\n",
    "            print('Snowline already exists in file, loading...')\n",
    "            snowline_df = pd.read_pickle(snowlines_path + snowline_fn)\n",
    "        \n",
    "        else:  \n",
    "            # -----Convert image to xarray.Dataset\n",
    "            if site_name=='Gulkana':\n",
    "                res = 12\n",
    "            else:\n",
    "                res = dataset_dict[dataset]['resolution_m']\n",
    "            im_xr = im.wx.to_xarray(scale=res, crs='EPSG:4326')\n",
    "            # reproject to UTM CRS\n",
    "            im_xr_UTM = im_xr.rio.reproject('EPSG:'+epsg_UTM)\n",
    "            # replace no data values with NaN and account for image scalar\n",
    "            bands = [band for band in dataset_dict[dataset]['bands'] if 'QA' not in band]\n",
    "            for band in bands:\n",
    "                im_xr_UTM[band] = xr.where(im_xr_UTM[band] != dataset_dict[dataset]['no_data_value'],\n",
    "                                           im_xr_UTM[band] / dataset_dict[dataset]['im_scalar'], np.nan)\n",
    "\n",
    "            # -----Add NDSI band\n",
    "            im_xr_UTM['NDSI'] = ((im_xr_UTM[dataset_dict[dataset]['NDSI'][0]] - im_xr_UTM[dataset_dict[dataset]['NDSI'][1]]) \n",
    "                                 / (im_xr_UTM[dataset_dict[dataset]['NDSI'][0]] + im_xr_UTM[dataset_dict[dataset]['NDSI'][1]]))\n",
    "\n",
    "            # -----Classify image\n",
    "            im_classified = f.classify_image(im_xr_UTM, clf, feature_cols, crop_to_AOI, \n",
    "                                             AOI_UTM, dataset, dataset_dict, site_name, \n",
    "                                             im_classified_fn, im_classified_path)\n",
    "            if type(im_classified)==str:\n",
    "                continue\n",
    "        \n",
    "            # -----Delineate snowline(s)\n",
    "            plot_results = True\n",
    "            snowline_df = f.delineate_im_snowline(im_xr_UTM, im_classified, site_name, AOI_UTM, DEM, \n",
    "                                                  dataset_dict, dataset, im_date, snowline_fn, \n",
    "                                                  snowlines_path, figures_out_path, plot_results)\n",
    "            \n",
    "        print('Median snowline elevation: ' + str(snowline_df['snowlines_elevs_median'][0]) + ' m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa90e231-d187-486d-bbaa-4edb581d0042",
   "metadata": {},
   "source": [
    "## 3. Landsat 8/9 SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e3ec2e-53a6-4981-a04d-f6f87e954dc8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Load trained classifier and feature columns\n",
    "clf_fn = base_path+'inputs-outputs/L_classifier_all_sites.sav'\n",
    "clf = pickle.load(open(clf_fn, 'rb'))\n",
    "feature_cols_fn = base_path+'inputs-outputs/L_feature_cols.pkl'\n",
    "feature_cols = pickle.load(open(feature_cols_fn,'rb'))\n",
    "\n",
    "# -----Query GEE for imagery\n",
    "dataset = 'Landsat'\n",
    "im_list = f.query_GEE_for_Landsat_SR(AOI_UTM, date_start, date_end, month_start, month_end, \n",
    "                                    cloud_cover_max, site_name, dataset, dataset_dict, out_path)\n",
    "im_list_size = im_list.size().getInfo()\n",
    "\n",
    "# -----Loop through images\n",
    "if im_list_size==0: # check that images were found\n",
    "    print('No images found to classify, quiting...')\n",
    "else:\n",
    "    \n",
    "    for i in range(0, im_list_size):\n",
    "        \n",
    "        # -----Select image by index\n",
    "        im = ee.Image(ee.List(im_list).get(i))\n",
    "        # get image time\n",
    "        im_date = im.date().format(None, 'GMT').getInfo()\n",
    "        print(' ')\n",
    "        print(str(i+1)+'/'+str(im_list_size))\n",
    "        print(im_date)\n",
    "                \n",
    "            \n",
    "        # -----Check if classified image and snowline already exists in file\n",
    "        im_classified_fn = im_date.replace('-','').replace(':','') + '_' + site_name + '_' + dataset + '_classified.nc'\n",
    "        snowline_fn = im_date.replace('-','').replace(':','') + '_' + site_name + '_' + dataset + '_snowline.pkl'\n",
    "        if os.path.exists(im_classified_path + im_classified_fn) & os.path.exists(snowlines_path + snowline_fn):\n",
    "            \n",
    "            print('Classified image already exists in file, loading...')\n",
    "            im_classified = xr.open_dataset(im_classified_path + im_classified_fn)\n",
    "            print('Snowline already exists in file, loading...')\n",
    "            snowline_df = pd.read_pickle(snowlines_path + snowline_fn)\n",
    "        \n",
    "        else:  \n",
    "            # -----Convert image to xarray.Dataset\n",
    "            res = dataset_dict[dataset]['resolution_m']\n",
    "            im_xr = im.wx.to_xarray(scale=res, crs='EPSG:4326')\n",
    "            # reproject to UTM CRS\n",
    "            im_xr_UTM = im_xr.rio.reproject('EPSG:'+epsg_UTM)\n",
    "            # replace no data values with NaN and account for image scalar\n",
    "            bands = [band for band in dataset_dict[dataset]['bands'] if 'QA' not in band]\n",
    "            for band in bands:\n",
    "                im_xr_UTM[band] = xr.where(im_xr_UTM[band] != dataset_dict[dataset]['no_data_value'],\n",
    "                                           im_xr_UTM[band] / dataset_dict[dataset]['im_scalar'], np.nan)\n",
    "\n",
    "            # -----Add NDSI band\n",
    "            im_xr_UTM['NDSI'] = ((im_xr_UTM[dataset_dict[dataset]['NDSI'][0]] - im_xr_UTM[dataset_dict[dataset]['NDSI'][1]]) \n",
    "                                 / (im_xr_UTM[dataset_dict[dataset]['NDSI'][0]] + im_xr_UTM[dataset_dict[dataset]['NDSI'][1]]))\n",
    "\n",
    "            # -----Classify image\n",
    "            im_classified = f.classify_image(im_xr_UTM, clf, feature_cols, crop_to_AOI, \n",
    "                                             AOI_UTM, dataset, dataset_dict, site_name, \n",
    "                                             im_classified_fn, im_classified_path)\n",
    "            if type(im_classified)==str:\n",
    "                continue\n",
    "        \n",
    "            # -----Delineate snowline(s)\n",
    "            plot_results = True\n",
    "            snowline_df = f.delineate_im_snowline(im_xr_UTM, im_classified, site_name, AOI_UTM, DEM, \n",
    "                                                  dataset_dict, dataset, im_date, snowline_fn, \n",
    "                                                  snowlines_path, figures_out_path, plot_results)\n",
    "            \n",
    "        print('Median snowline elevation: ' + str(snowline_df['snowlines_elevs_median'][0]) + ' m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b99890d-b4fb-4309-b4b5-a8012796fdf2",
   "metadata": {},
   "source": [
    "## 4. PlanetScope SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c9018d-9780-45ad-8aca-3bbe2f715911",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Load trained classifier and feature columns\n",
    "clf_fn = base_path+'inputs-outputs/PS_classifier_all_sites.sav'\n",
    "clf = pickle.load(open(clf_fn, 'rb'))\n",
    "feature_cols_fn = base_path+'inputs-outputs/PS_feature_cols.pkl'\n",
    "feature_cols = pickle.load(open(feature_cols_fn,'rb'))\n",
    "dataset = 'PlanetScope'\n",
    "\n",
    "# -----Read surface reflectance file names\n",
    "os.chdir(PS_im_path)\n",
    "im_fns = glob.glob('*SR*.tif')\n",
    "im_fns = sorted(im_fns) # sort chronologically\n",
    "plot_results = False\n",
    "\n",
    "# ----Mask clouds and cloud shadows in all images\n",
    "print('Masking images using cloud bitmask...')\n",
    "for i, im_fn in enumerate(im_fns):\n",
    "    f.PS_mask_im_pixels(PS_im_path, im_fn, PS_im_masked_path, save_outputs, plot_results)\n",
    "\n",
    "# -----Mosaic images captured within same hour\n",
    "print('Mosaicking images captured in the same hour...')\n",
    "# read masked image file names\n",
    "os.chdir(PS_im_masked_path)\n",
    "im_masked_fns = glob.glob('*_mask.tif')\n",
    "im_masked_fns = sorted(im_masked_fns) # sort chronologically\n",
    "# mosaic images by date\n",
    "f.PS_mosaic_ims_by_date(PS_im_masked_path, im_masked_fns, PS_im_mosaics_path, AOI_UTM, plot_results)\n",
    "print(' ')\n",
    "\n",
    "# -----Adjust image radiometry\n",
    "# read mosaicked image file names\n",
    "os.chdir(PS_im_mosaics_path)\n",
    "im_mosaic_fns = glob.glob('*.tif')\n",
    "im_mosaic_fns = sorted(im_mosaic_fns)\n",
    "# create a polygon(s) of the top 20th percentile elevations within the AOI\n",
    "plot_results=False \n",
    "polygon_top, polygon_bottom, im_mosaic_fn, im_mosaic = f.create_AOI_elev_polys(AOI_UTM, PS_im_mosaics_path, im_mosaic_fns, DEM)\n",
    "# loop through images\n",
    "for i, im_mosaic_fn in enumerate(im_mosaic_fns):\n",
    "    print(' ')\n",
    "    print(str(i+1)+'/'+str(len(im_mosaic_fns)))\n",
    "\n",
    "    # adjust radiometry\n",
    "    im_adj, im_adj_method = f.PS_adjust_image_radiometry(im_mosaic_fn, PS_im_mosaics_path, polygon_top, \n",
    "                                                         polygon_bottom, AOI_UTM, dataset_dict, dataset, \n",
    "                                                         site_name, skip_clipped, plot_results)\n",
    "    if type(im_adj)==str: # skip if there was an error in adjustment\n",
    "        continue\n",
    "    \n",
    "    # -----Determiine image date\n",
    "    im_date = im_mosaic_fn[0:8] + 'T' + im_mosaic_fn[9:11] + ':00:00'\n",
    "    \n",
    "    # -----Classify image\n",
    "    im_classified_fn = im_date.replace('-','').replace(':','') + '_' + site_name + '_' + dataset + '_classified.nc'\n",
    "    if os.path.exists(im_classified_path + im_classified_fn):\n",
    "        print('Classified image already exists in file, loading...')\n",
    "        im_classified = xr.open_dataset(im_classified_path + im_classified_fn)\n",
    "    else:\n",
    "        im_classified = f.classify_image(im_adj, clf, feature_cols, crop_to_AOI, \n",
    "                                         AOI_UTM, dataset, dataset_dict, site_name, \n",
    "                                         im_classified_fn, im_classified_path)\n",
    "\n",
    "    if type(im_classified)==str:\n",
    "        continue    \n",
    "    \n",
    "    # -----Delineate snowline(s)\n",
    "    plot_results=True\n",
    "    snowline_fn = im_date.replace('-','').replace(':','') + '_' + site_name + '_' + dataset + '_snowline.pkl'\n",
    "    if os.path.exists(snowlines_path + snowline_fn):\n",
    "        print('Snowline already exists in file, loading...')\n",
    "        snowline_df = pd.read_pickle(snowlines_path + snowline_fn)\n",
    "    else:\n",
    "        snowline_df = f.delineate_im_snowline(im_adj, im_classified, site_name, AOI_UTM, DEM, \n",
    "                                              dataset_dict, dataset, im_date, snowline_fn, \n",
    "                                              snowlines_path, figures_out_path, plot_results)\n",
    "    print('Median snowline elevation: ' + str(snowline_df['snowlines_elevs_median'][0]) + ' m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48e2cda-9766-4c2b-b269-39ea55ea3362",
   "metadata": {},
   "source": [
    "## _Optional:_ Compile snowlines into single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2f69a2-9597-48a2-b876-cfa58df684a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3e3b36d-8ef4-4c27-a65c-e023d016846b",
   "metadata": {},
   "source": [
    "## _Optional:_ Compile figures into single .gif file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e9f23d-a96f-4cdf-9499-091f64cbf5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ef4ce9-5975-4b2c-8619-03e4e3507b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import shape, Polygon, MultiPolygon, Point\n",
    "def plot_snowline_image(im, im_classified, snowline_df, AOI, DEM, site_name, im_date, figures_out_path, dataset, dataset_dict):\n",
    "\n",
    "    # -----Make directory for snowlines (if it does not already exist in file)\n",
    "    if os.path.exists(figures_out_path)==False:\n",
    "        os.mkdir(figures_out_path)\n",
    "        print(\"Made directory for figures:\" + figures_out_path)\n",
    "\n",
    "    # -----Subset dataset_dict to dataset\n",
    "    ds_dict = dataset_dict[dataset]\n",
    "\n",
    "    # -----Define image bands\n",
    "    bands = [x for x in im.data_vars]\n",
    "    bands = [band for band in bands if 'QA' not in band]\n",
    "\n",
    "    # -----Remove time dimension\n",
    "    im = im.isel(time=0)\n",
    "    im_classified = im_classified.isel(time=0)\n",
    "\n",
    "    # -----Create no data mask\n",
    "    no_data_mask = xr.where(np.isnan(im_classified), 1, 0).to_array().data[0]\n",
    "    # convert to polygons\n",
    "    no_data_polygons = []\n",
    "    for s, value in rio.features.shapes(no_data_mask.astype(np.int16),\n",
    "                                        mask=(no_data_mask > 0),\n",
    "                                        transform=im.rio.transform()):\n",
    "        no_data_polygons.append(shape(s))\n",
    "    no_data_polygons = MultiPolygon(no_data_polygons)\n",
    "\n",
    "    # -----Mask the DEM using the AOI\n",
    "    # create AOI mask\n",
    "    mask_AOI = rio.features.geometry_mask(AOI.geometry,\n",
    "                                      out_shape=(len(DEM.y), len(DEM.x)),\n",
    "                                      transform=DEM.rio.transform(),\n",
    "                                      invert=True)\n",
    "    # convert mask to xarray DataArray\n",
    "    mask_AOI = xr.DataArray(mask_AOI , dims=(\"y\", \"x\"))\n",
    "    # mask DEM values outside the AOI\n",
    "    DEM_AOI = DEM.copy(deep=True)\n",
    "    DEM_AOI['elevation'].data = np.where(mask_AOI==True, DEM_AOI['elevation'].data, np.nan)\n",
    "\n",
    "    # -----Interpolate DEM to the image coordinates\n",
    "    DEM_AOI_interp = DEM_AOI.interp(x=im_classified.x.data,\n",
    "                                    y=im_classified.y.data,\n",
    "                                    method=\"nearest\")\n",
    "\n",
    "    # -----Determine snow covered elevations\n",
    "    # create array of elevation for all un-masked pixels\n",
    "    all_elev = np.ravel(np.where(~np.isnan(im_classified.classified.data), DEM_AOI_interp.elevation.data, np.nan))\n",
    "    all_elev = all_elev[~np.isnan(all_elev)] # remove NaNs\n",
    "    # create array of snow-covered pixel elevations\n",
    "    snow_est_elev = np.ravel(np.where(im_classified.classified.data <= 2, DEM_AOI_interp.elevation.data, np.nan))\n",
    "    snow_est_elev = snow_est_elev[~np.isnan(snow_est_elev)] # remove NaNs\n",
    "\n",
    "    # -----Create elevation histograms\n",
    "    # determine bins to use in histograms\n",
    "    elev_min = np.fix(np.nanmin(DEM_AOI_interp.elevation.data.flatten())/10)*10\n",
    "    elev_max = np.round(np.nanmax(DEM_AOI_interp.elevation.data.flatten())/10)*10\n",
    "    bin_edges = np.linspace(elev_min, elev_max, num=int((elev_max-elev_min)/10 + 1))\n",
    "    bin_centers = (bin_edges[1:] + bin_edges[0:-1]) / 2\n",
    "    # calculate elevation histograms\n",
    "    H_elev = np.histogram(all_elev, bins=bin_edges)[0]\n",
    "    H_snow_est_elev = np.histogram(snow_est_elev, bins=bin_edges)[0]\n",
    "    H_snow_est_elev_norm = H_snow_est_elev / H_elev\n",
    "\n",
    "    # -----Make all pixels at elevations >75% snow coverage = snow\n",
    "    # determine elevation with > 75% snow coverage\n",
    "    if len(np.where(H_snow_est_elev_norm > 0.75)[0]) > 1:\n",
    "        elev_75_snow = bin_centers[np.where(H_snow_est_elev_norm > 0.75)[0][0]]\n",
    "        # set all pixels above the elev_75_snow to snow (1)\n",
    "        im_classified_adj = xr.where(DEM_AOI_interp['elevation'].isel(band=0) > elev_75_snow, 1, im_classified) # set all values above elev_75_snow to snow (1)\n",
    "        im_classified_adj = im_classified_adj.squeeze(drop=True) # drop unecessary dimensions\n",
    "        H_snow_est_elev_norm[bin_centers >= elev_75_snow] = 1\n",
    "    else:\n",
    "        im_classified_adj = im_classified.squeeze(drop=True)\n",
    "\n",
    "    # -----Plot results\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(12,8), gridspec_kw={'height_ratios': [3, 1]})\n",
    "    ax = ax.flatten()\n",
    "    # define x and y limits\n",
    "    xmin, xmax = AOI.geometry[0].buffer(500).bounds[0]/1e3, AOI.geometry[0].buffer(500).bounds[2]/1e3\n",
    "    ymin, ymax = AOI.geometry[0].buffer(500).bounds[1]/1e3, AOI.geometry[0].buffer(500).bounds[3]/1e3\n",
    "    # define colors for plotting\n",
    "    color_snow = '#4eb3d3'\n",
    "    color_ice = '#084081'\n",
    "    color_rock = '#fdbb84'\n",
    "    color_water = '#bdbdbd'\n",
    "    # create colormap\n",
    "    colors = [color_snow, color_snow, color_ice, color_rock, color_water]\n",
    "    cmp = matplotlib.colors.ListedColormap(colors)\n",
    "    # RGB image\n",
    "    ax[0].imshow(np.dstack([im[ds_dict['RGB_bands'][0]].data,\n",
    "                            im[ds_dict['RGB_bands'][1]].data,\n",
    "                            im[ds_dict['RGB_bands'][2]].data]),\n",
    "                 extent=(np.min(im.x.data)/1e3, np.max(im.x.data)/1e3, np.min(im.y.data)/1e3, np.max(im.y.data)/1e3))\n",
    "    ax[0].set_xlabel('Easting [km]')\n",
    "    ax[0].set_ylabel('Northing [km]')\n",
    "    # classified image\n",
    "    ax[1].imshow(im_classified['classified'].data, cmap=cmp, vmin=1, vmax=5,\n",
    "                 extent=(np.min(im_classified.x.data)/1e3, np.max(im_classified.x.data)/1e3, np.min(im_classified.y.data)/1e3, np.max(im_classified.y.data)/1e3))\n",
    "    # plot dummy points for legend\n",
    "    ax[1].scatter(0, 0, color=color_snow, s=50, label='snow')\n",
    "    ax[1].scatter(0, 0, color=color_ice, s=50, label='ice')\n",
    "    ax[1].scatter(0, 0, color=color_rock, s=50, label='rock')\n",
    "    ax[1].scatter(0, 0, color=color_water, s=50, label='water')\n",
    "    ax[1].set_xlabel('Easting [km]')\n",
    "    # AOI\n",
    "    for j, geom in enumerate(AOI.geometry[0].boundary.geoms):\n",
    "        if j==0:\n",
    "            ax[0].plot([x/1e3 for x in geom.coords.xy[0]], [y/1e3 for y in geom.coords.xy[1]], '-k', linewidth=1, label='AOI')\n",
    "        else:\n",
    "            ax[0].plot([x/1e3 for x in geom.coords.xy[0]], [y/1e3 for y in geom.coords.xy[1]], '-k', linewidth=1, label='_nolegend_')\n",
    "        ax[1].plot([x/1e3 for x in geom.coords.xy[0]], [y/1e3 for y in geom.coords.xy[1]], '-k', linewidth=1, label='_nolegend_')\n",
    "    # reset x and y limits\n",
    "    ax[0].set_xlim(xmin, xmax)\n",
    "    ax[0].set_ylim(ymin, ymax)\n",
    "    ax[1].set_xlim(xmin, xmax)\n",
    "    ax[1].set_ylim(ymin, ymax)\n",
    "    # image bands histogram\n",
    "    h_b = ax[2].hist(im[ds_dict['RGB_bands'][0]].data.flatten(), color='blue', histtype='step', linewidth=2, bins=100, label=\"blue\")\n",
    "    h_g = ax[2].hist(im[ds_dict['RGB_bands'][1]].data.flatten(), color='green', histtype='step', linewidth=2, bins=100, label=\"green\")\n",
    "    h_r = ax[2].hist(im[ds_dict['RGB_bands'][2]].data.flatten(), color='red', histtype='step', linewidth=2, bins=100, label=\"red\")\n",
    "    ax[2].set_xlabel(\"Surface reflectance\")\n",
    "    ax[2].set_ylabel(\"Pixel counts\")\n",
    "    ax[2].legend(loc='best')\n",
    "    ax[2].grid()\n",
    "    # normalized snow elevations histogram\n",
    "    ax[3].bar(bin_centers, H_snow_est_elev_norm, width=(bin_centers[1]-bin_centers[0]), color=color_snow, align='center')\n",
    "    ax[3].set_xlabel(\"Elevation [m]\")\n",
    "    ax[3].set_ylabel(\"% snow-covered\")\n",
    "    ax[3].grid()\n",
    "    ax[3].set_xlim(elev_min-10, elev_max+10)\n",
    "    ax[3].set_ylim(0,1)\n",
    "    # plot estimated snow line coordinates\n",
    "    ax[0].plot([x/1e3 for x in snowline_df['snowlines_coords'][0].coords.xy[0]],\n",
    "               [y/1e3 for y in snowline_df['snowlines_coords'][0].coords.xy[1]], \n",
    "               '.', color='#f768a1', markersize=5, label='sl$_{estimated}$')\n",
    "    ax[1].plot([x/1e3 for x in snowline_df['snowlines_coords'][0].coords.xy[0]],\n",
    "               [y/1e3 for y in snowline_df['snowlines_coords'][0].coords.xy[1]], \n",
    "               '.', color='#f768a1', markersize=5, label='sl$_{estimated}$')   \n",
    "    # determine figure title and file name\n",
    "    title = im_date + '_' + site_name + '_' + dataset + '_snowline'\n",
    "    # add legends\n",
    "    ax[0].legend(loc='best')\n",
    "    ax[1].legend(loc='best')\n",
    "    fig.suptitle(title)\n",
    "    fig.tight_layout()\n",
    "    # save figure\n",
    "    fig_fn = figures_out_path + title + '.png'\n",
    "    fig.savefig(fig_fn, dpi=300, facecolor='white', edgecolor='none')\n",
    "    print('Figure saved to file:' + fig_fn)\n",
    "\n",
    "    return fig\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae1a880-444a-481a-990d-e433c86feacc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shapely \n",
    "\n",
    "# loop through images\n",
    "for i, im_mosaic_fn in enumerate(im_mosaic_fns):\n",
    "    print(' ')\n",
    "    print(str(i+1)+'/'+str(len(im_mosaic_fns)))\n",
    "\n",
    "    # adjust radiometry\n",
    "    im_adj, im_adj_method = f.PS_adjust_image_radiometry(im_mosaic_fn, PS_im_mosaics_path, polygon_top, \n",
    "                                                         polygon_bottom, AOI_UTM, dataset_dict, dataset, \n",
    "                                                         site_name, skip_clipped, plot_results)\n",
    "    if type(im_adj)==str: # skip if there was an error in adjustment\n",
    "        continue\n",
    "    \n",
    "    # -----Determiine image date\n",
    "    im_date = im_mosaic_fn[0:8] + 'T' + im_mosaic_fn[9:11] + ':00:00'\n",
    "    \n",
    "    # -----Classify image\n",
    "    im_classified_fn = im_date.replace('-','').replace(':','') + '_' + site_name + '_' + dataset + '_classified.nc'\n",
    "    if os.path.exists(im_classified_path + im_classified_fn):\n",
    "        print('Classified image already exists in file, loading...')\n",
    "        im_classified = xr.open_dataset(im_classified_path + im_classified_fn)\n",
    "    else:\n",
    "        im_classified = 'N/A'\n",
    "\n",
    "    if type(im_classified)==str:\n",
    "        continue    \n",
    "    \n",
    "    # -----Delineate snowline(s)\n",
    "    snowline_fn = im_date.replace('-','').replace(':','') + '_' + site_name + '_' + dataset + '_snowline.pkl'\n",
    "    if os.path.exists(snowlines_path + snowline_fn):\n",
    "        print('Snowline already exists in file, loading...')\n",
    "        snowline_df = pd.read_pickle(snowlines_path + snowline_fn)\n",
    "        if type(snowline_df['snowlines_coords'][0])==shapely.geometry.linestring.LineString:\n",
    "            fig = plot_snowline_image(im_adj, im_classified, snowline_df, AOI, DEM, site_name, im_date, \n",
    "                                      figures_out_path, dataset, dataset_dict)   \n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f17f52-96c9-4361-8c32-0358156f09f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce36cba8-7765-46f1-8c07-ccf232acddb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planet-snow",
   "language": "python",
   "name": "planet-snow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
