{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce5d338c",
   "metadata": {},
   "source": [
    "# Classify snow-covered area (SCA) in Sentinel-2, Landsat 8/9, and PlanetScope imagery: full pipelines\n",
    "\n",
    "Rainey Aberle\n",
    "\n",
    "Department of Geosciences, Boise State University\n",
    "\n",
    "2022\n",
    "\n",
    "### Requirements:\n",
    "- Area of Interest (AOI) shapefile: where snow will be classified in all available images. \n",
    "- Google Earth Engine (GEE) account: used to pull DEM over the AOI. Sign up for a free account [here](https://earthengine.google.com/new_signup/). \n",
    "- Digital elevation model (DEM) (_optional_): used to extract elevations over the AOI and for each snowline. If no DEM is provided, the ASTER Global DEM will be loaded through GEE. \n",
    "\n",
    "### Outline:\n",
    "__0. Setup__ paths in directory, file locations, authenticate GEE - _modify this section!_\n",
    "\n",
    "__1. Sentinel-2 Top of Atmosphere (TOA) imagery:__ full pipeline\n",
    "\n",
    "__2. Sentinel-2 Surface Reflectance (SR) imagery:__ full pipeline\n",
    "\n",
    "__3. Landsat 8/9 Surface Reflectance (SR) imagery:__ full pipeline\n",
    "\n",
    "__4. PlanetScope Surface Reflectance (SR) imagery:__ full pipeline\n",
    "\n",
    "-------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8979f4",
   "metadata": {},
   "source": [
    "### 0. Setup\n",
    "\n",
    "#### Define paths in directory and desired settings. \n",
    "Modify lines located within the following:\n",
    "\n",
    "`#### MODIFY HERE ####`  \n",
    "\n",
    "`#####################`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183ca5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MODIFY HERE #####\n",
    "\n",
    "# -----Paths in directory\n",
    "site_name = 'Gulkana'\n",
    "# path to snow-cover-mapping/ - Make sure you include a \"/\" at the end\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/'\n",
    "# path to folder containing AOI files\n",
    "AOI_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/snow_cover_mapping/snow_cover_mapping_application/study-sites/' + site_name + '/AOIs/'\n",
    "# AOI file name\n",
    "AOI_fn = 'Gulkana_USGS_glacier_outline_2021.shp' \n",
    "# path to folder containing DEM raster file\n",
    "# Note: set DEM_fn=None if you want to use the ArcticDEM or ASTER GDEM via Google Earth Engine\n",
    "DEM_path = AOI_path + '../DEMs/'\n",
    "# DEM file name\n",
    "DEM_fn = 'Gulkana_ArcticDEM_clip.tif'\n",
    "# path for output images\n",
    "out_path = AOI_path + '../imagery/'\n",
    "# path to PlanetScope images\n",
    "# Note: set PS_im_path=None if not using PlanetScope\n",
    "PS_im_path = out_path + 'PlanetScope/raw_images/'\n",
    "# path for output figures\n",
    "figures_out_path = AOI_path + '../figures/'\n",
    "\n",
    "# -----Define image search filters\n",
    "date_start = '2021-05-01'\n",
    "date_end = '2021-12-01'\n",
    "month_start = 5\n",
    "month_end = 10\n",
    "cloud_cover_max = 70\n",
    "\n",
    "# -----Determine whether to mask clouds using the respective cloud masking data products\n",
    "# NOTE: Cloud mask products anecdotally are less accurate over glacierized/snow-covered surfaces. \n",
    "# If the cloud masks are consistently masking large regions or your study site, I suggest setting mask_clouds = False\n",
    "mask_clouds = True\n",
    "\n",
    "# -----Determine image download, clipping & plotting settings\n",
    "# Note: if im_download = False, but images over the AOI exceed GEE limit,\n",
    "# images must be downloaded regardless.\n",
    "im_download = False  # = True to download all satellite images by default\n",
    "plot_results = True # = True to plot figures of results for each image where applicable\n",
    "skip_clipped = False # = True to skip images where bands appear \"clipped\", i.e. max(blue) < 0.8\n",
    "crop_to_AOI = True # = True to crop images to AOI before calculating SCA\n",
    "save_outputs = True # = True to save SCAs and snowlines to file\n",
    "save_figures = True # = True to save output figures to file\n",
    "\n",
    "#######################\n",
    "\n",
    "# -----Import packages\n",
    "import xarray as xr\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from matplotlib import pyplot as plt, dates\n",
    "import matplotlib\n",
    "import rasterio as rio\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import sys\n",
    "import ee\n",
    "import geedim as gd\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import dump, load\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "\n",
    "# -----Set paths for output files\n",
    "S2_TOA_im_path = out_path + 'Sentinel-2_TOA/'\n",
    "S2_SR_im_path = out_path + 'Sentinel-2_SR/'\n",
    "L_im_path = out_path + 'Landsat/'\n",
    "PS_im_masked_path = out_path + 'PlanetScope/masked/'\n",
    "PS_im_mosaics_path = out_path + 'PlanetScope/mosaics/'\n",
    "im_classified_path = out_path + 'classified/'\n",
    "snowlines_path = out_path + 'snowlines/'\n",
    "\n",
    "# -----Add path to functions\n",
    "sys.path.insert(1, base_path+'functions/')\n",
    "import pipeline_utils as f\n",
    "\n",
    "# -----Load dataset dictionary\n",
    "dataset_dict = json.load(open(base_path + 'inputs-outputs/datasets_characteristics.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaee83ad",
   "metadata": {},
   "source": [
    "#### Authenticate and initialize Google Earth Engine (GEE). \n",
    "\n",
    "__Note:__ The first time you run the following cell, you will be asked to authenticate your GEE account for use in this notebook. This will send you to an external web page, where you will walk through the GEE authentication workflow and copy an authentication code back into the space below this cell when prompted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ec405",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com')\n",
    "except: \n",
    "    ee.Authenticate()\n",
    "    ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b83185",
   "metadata": {},
   "source": [
    "#### Load AOI and DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959210a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Load AOI as gpd.GeoDataFrame\n",
    "AOI = gpd.read_file(AOI_path + AOI_fn)\n",
    "# reproject the AOI to WGS to solve for the optimal UTM zone\n",
    "AOI_WGS = AOI.to_crs('EPSG:4326')\n",
    "AOI_WGS_centroid = [AOI_WGS.geometry[0].centroid.xy[0][0],\n",
    "                    AOI_WGS.geometry[0].centroid.xy[1][0]]\n",
    "# grab the optimal UTM zone EPSG code\n",
    "epsg_UTM = f.convert_wgs_to_utm(AOI_WGS_centroid[0], AOI_WGS_centroid[1])\n",
    "print('Optimal UTM CRS = EPSG:' + str(epsg_UTM))\n",
    "# reproject AOI to the optimal UTM zone\n",
    "AOI_UTM = AOI.to_crs('EPSG:'+epsg_UTM)\n",
    "\n",
    "# -----Load DEM as Xarray DataSet\n",
    "if DEM_fn is None:\n",
    "    # query GEE for DEM\n",
    "    DEM = f.query_gee_for_dem(AOI_UTM, base_path, site_name, DEM_path)\n",
    "else:\n",
    "    # load DEM as xarray DataSet\n",
    "    DEM = xr.open_dataset(DEM_path + DEM_fn)\n",
    "    DEM = DEM.rename({'band_data': 'elevation'})\n",
    "    # reproject the DEM to the optimal UTM zone\n",
    "    DEM = DEM.rio.reproject('EPSG:'+str(epsg_UTM))\n",
    "    DEM = DEM.rio.write_crs('EPSG:'+str(epsg_UTM))\n",
    "# remove unnecessary data (possible extra bands from ArcticDEM or other DEM)\n",
    "if len(np.shape(DEM.elevation.data))>2:\n",
    "    DEM['elevation'] = DEM.elevation[0]\n",
    "    DEM = xr.where(DEM < -100, np.nan, DEM)\n",
    "    DEM = DEM.rio.write_crs('EPSG:'+str(epsg_UTM))\n",
    "\n",
    "# -----Plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,6))\n",
    "dem_im = ax.imshow(DEM.elevation.data, cmap='terrain', \n",
    "          extent=(np.min(DEM.x.data)/1e3, np.max(DEM.x.data)/1e3, np.min(DEM.y.data)/1e3, np.max(DEM.y.data)/1e3))\n",
    "if type(AOI_UTM.geometry[0])==Polygon:\n",
    "    ax.plot([x/1e3 for x in AOI_UTM.geometry[0].exterior.coords.xy[0]],\n",
    "            [y/1e3 for y in AOI_UTM.geometry[0].exterior.coords.xy[1]], '-k')\n",
    "elif type(AOI_UTM.geometry[0])==MultiPolygon:\n",
    "    [ax.plot([x/1e3 for x in geom.exterior.coords.xy[0]],\n",
    "            [y/1e3 for y in geom.exterior.coords.xy[1]], '-k') for geom in AOI_UTM.geometry[0].geoms]\n",
    "ax.grid()\n",
    "ax.set_xlabel('Easting [km]')\n",
    "ax.set_ylabel('Northing [km]')\n",
    "fig.colorbar(dem_im, ax=ax, shrink=0.5, label='Elevation [m]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94aa40c",
   "metadata": {},
   "source": [
    "## 1. Sentinel-2 TOA imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda52707",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Query GEE for imagery (and download to S2_TOA_im_path if necessary)\n",
    "dataset = 'Sentinel-2_TOA'\n",
    "im_list = f.query_gee_for_imagery(dataset_dict, dataset, AOI_UTM, date_start, date_end, month_start, \n",
    "                                  month_end, cloud_cover_max, mask_clouds, S2_TOA_im_path, im_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ba6a71",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Load trained classifier and feature columns\n",
    "clf_fn = base_path+'inputs-outputs/Sentinel-2_TOA_classifier_all_sites.joblib'\n",
    "clf = load(clf_fn)\n",
    "feature_cols_fn = base_path+'inputs-outputs/Sentinel-2_TOA_feature_columns.json'\n",
    "feature_cols = json.load(open(feature_cols_fn))\n",
    "\n",
    "# -----Loop through images\n",
    "if type(im_list)==str: # check that images were found\n",
    "    print('No images found to classify, quiting...')\n",
    "else:\n",
    "    \n",
    "    for i in tqdm(range(0, len(im_list))):\n",
    "        \n",
    "        # -----Subset image using loop index\n",
    "        im_xr = im_list[i]\n",
    "        im_date = str(im_xr.time.data[0])[0:19]\n",
    "        print(im_date)\n",
    "        \n",
    "        # -----Adjust image for image scalar and no data values\n",
    "        # replace no data values with NaN and account for image scalar\n",
    "        crs = im_xr.rio.crs.to_epsg()\n",
    "        if np.nanmean(im_xr['B2'])>1e3:\n",
    "            im_xr = xr.where(im_xr==dataset_dict[dataset]['no_data_value'], np.nan, \n",
    "                             im_xr / dataset_dict[dataset]['image_scalar'])\n",
    "        else:\n",
    "            im_xr = xr.where(im_xr==dataset_dict[dataset]['no_data_value'], np.nan, im_xr)\n",
    "        # add NDSI band\n",
    "        im_xr['NDSI'] = ((im_xr[dataset_dict[dataset]['NDSI_bands'][0]] - im_xr[dataset_dict[dataset]['NDSI_bands'][1]]) \n",
    "                             / (im_xr[dataset_dict[dataset]['NDSI_bands'][0]] + im_xr[dataset_dict[dataset]['NDSI_bands'][1]]))\n",
    "        im_xr.rio.write_crs('EPSG:'+str(crs), inplace=True)\n",
    "                \n",
    "        # -----Classify image\n",
    "        # check if classified image already exists in file\n",
    "        im_classified_fn = im_date.replace('-','').replace(':','') + '_' + site_name + '_' + dataset + '_classified.nc'\n",
    "        if os.path.exists(im_classified_path + im_classified_fn):\n",
    "            print('Classified image already exists in file, continuing...')\n",
    "            im_classified = xr.open_dataset(im_classified_path + im_classified_fn)\n",
    "            # remove no data values\n",
    "            im_classified = xr.where(im_classified==-9999, np.nan, im_classified)\n",
    "        else:  \n",
    "            # classify image\n",
    "            im_classified = f.classify_image(im_xr, clf, feature_cols, crop_to_AOI, AOI_UTM, DEM,\n",
    "                                             dataset_dict, dataset, im_classified_fn, im_classified_path)\n",
    "            if type(im_classified)==str: # skip if error in classification\n",
    "                continue\n",
    "        \n",
    "        # -----Delineate snowline(s)\n",
    "        # check if snowline already exists in file\n",
    "        snowline_fn = im_date.replace('-','').replace(':','') + '_' + site_name + '_' + dataset + '_snowline.csv'\n",
    "        if os.path.exists(snowlines_path + snowline_fn):\n",
    "            print('Snowline already exists in file, continuing...')\n",
    "            print(' ')\n",
    "            continue # no need to load snowline if it already exists\n",
    "        else:\n",
    "            plot_results = True\n",
    "            # create directory for figures if it doesn't already exist\n",
    "            if (not os.path.exists(figures_out_path)) & plot_results:\n",
    "                os.mkdir(figures_out_path)\n",
    "                print('Created directory for output figures: '+figures_out_path)\n",
    "            snowline_df = f.delineate_image_snowline(im_xr, im_classified, site_name, AOI_UTM, dataset_dict, dataset, \n",
    "                                                     im_date, snowline_fn, snowlines_path, figures_out_path, plot_results)\n",
    "            # plt.show()\n",
    "            print('Accumulation Area Ratio =  ' + str(snowline_df['AAR'][0]))\n",
    "        print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10df07b",
   "metadata": {},
   "source": [
    "## 2. Sentinel-2 SR imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7615f094",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Query GEE for imagery and download to S2_SR_im_path if necessary\n",
    "dataset = 'Sentinel-2_SR'\n",
    "im_list = f.query_gee_for_imagery(dataset_dict, dataset, AOI_UTM, date_start, date_end, month_start, \n",
    "                                  month_end, cloud_cover_max, mask_clouds, S2_SR_im_path, im_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fdaccf-bfaf-4eef-9b36-6696fe2b61a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import MultiPolygon, Polygon, MultiLineString, LineString, Point, shape\n",
    "from scipy.ndimage import binary_fill_holes, binary_dilation\n",
    "from skimage.measure import find_contours\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import iqr\n",
    "\n",
    "def delineate_snowline(im_xr, im_classified, site_name, aoi, dataset_dict, dataset, im_date, snowline_fn,\n",
    "                       out_path, figures_out_path, plot_results, verbose=False):\n",
    "    \"\"\"\n",
    "    Delineate the seasonal snowline in classified images. Snowlines will likely not be detected in images with nearly all or no snow.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    im_xr: xarray.Dataset\n",
    "        input image used for plotting\n",
    "    im_classified: xarray.Dataset\n",
    "        classified image used to delineate snowlines\n",
    "    site_name: str\n",
    "        name of study site used for output file names\n",
    "    aoi:  geopandas.geodataframe.GeoDataFrame\n",
    "        area of interest used to crop classified images\n",
    "    dataset_dict: dict\n",
    "        dictionary of dataset-specific parameters\n",
    "    dataset: str\n",
    "        name of dataset ('Landsat', 'Sentinel2', 'PlanetScope')\n",
    "    im_date: str\n",
    "        image capture datetime ('YYYYMMDDTHHmmss')\n",
    "    snowline_fn: str\n",
    "        file name of snowline to be saved in out_path\n",
    "    out_path: str\n",
    "        path in directory for output snowlines\n",
    "    figures_out_path: str\n",
    "        path in directory for figures\n",
    "    plot_results: bool\n",
    "        whether to plot RGB image, classified image, and resulting snowline and save figure to file\n",
    "    verbose: bool\n",
    "        whether to print details during the process\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    snowline_gdf: geopandas.GeoDataFrame\n",
    "        resulting study site name, image datetime, snowline coordinates, snowline elevations, and median snowline elevation\n",
    "    \"\"\"\n",
    "\n",
    "    # -----Make directory for snowlines (if it does not already exist)\n",
    "    if not os.path.exists(out_path):\n",
    "        os.mkdir(out_path)\n",
    "        print(\"Made directory for snowlines:\" + out_path)\n",
    "        \n",
    "    # -----Make directory for figures (if it does not already exist)\n",
    "    if (not os.path.exists(figures_out_path)) & plot_results:\n",
    "        os.mkdir(figures_out_path)\n",
    "        print('Made directory for output figures: ' + figures_out_path)\n",
    "\n",
    "    # -----Subset dataset_dict to dataset\n",
    "    ds_dict = dataset_dict[dataset]\n",
    "\n",
    "    # -----Remove time dimension\n",
    "    im_xr = im_xr.isel(time=0)\n",
    "    im_classified = im_classified.isel(time=0)\n",
    "\n",
    "    # -----Create no data mask\n",
    "    no_data_mask = xr.where(np.isnan(im_classified), 1, 0).classified.data\n",
    "    # dilate by two image pixels\n",
    "    dilated_mask = binary_dilation(no_data_mask, iterations=5)\n",
    "    no_data_mask = np.logical_not(dilated_mask)\n",
    "    # add no_data_mask variable classified image\n",
    "    im_classified = im_classified.assign(no_data_mask=([\"y\", \"x\"], no_data_mask))\n",
    "\n",
    "    # -----Determine snow covered elevations\n",
    "    all_elev = np.ravel(im_classified.elevation.data)\n",
    "    all_elev = all_elev[~np.isnan(all_elev)]  # remove NaNs\n",
    "    snow_est_elev = np.ravel(im_classified.where((im_classified.classified <= 2))\n",
    "                             .where(im_classified.classified != -9999).elevation.data)\n",
    "    snow_est_elev = snow_est_elev[~np.isnan(snow_est_elev)]  # remove NaNs\n",
    "\n",
    "    # -----Create elevation histograms\n",
    "    # determine bins to use in histograms\n",
    "    elev_min = np.fix(np.nanmin(np.ravel(im_classified.elevation.data)) / 10) * 10\n",
    "    elev_max = np.round(np.nanmax(np.ravel(im_classified.elevation.data)) / 10) * 10\n",
    "    bin_edges = np.linspace(elev_min, elev_max, num=int((elev_max - elev_min) / 10 + 1))\n",
    "    bin_centers = (bin_edges[1:] + bin_edges[0:-1]) / 2\n",
    "    # calculate elevation histograms\n",
    "    hist_elev = np.histogram(all_elev, bins=bin_edges)[0]\n",
    "    hist_snow_est_elev = np.histogram(snow_est_elev, bins=bin_edges)[0]\n",
    "    hist_snow_est_elev_norm = hist_snow_est_elev / hist_elev\n",
    "\n",
    "    # -----Make all pixels at elevation bins with >75% snow coverage = snow\n",
    "    # determine elevation with > 75% snow coverage\n",
    "    if np.any(hist_snow_est_elev_norm > 0.75):\n",
    "        elev_75_snow = bin_centers[np.argmax(hist_snow_est_elev_norm > 0.75)]\n",
    "        # make a copy of im_classified for adjusting\n",
    "        im_classified_adj = im_classified.copy()\n",
    "        # Fill gaps in elevation using linear interpolation along the spatial dimensions\n",
    "        im_classified_adj['elevation'] = im_classified['elevation'].interpolate_na(dim='x', method='linear')\n",
    "        # set all pixels above the elev_75_snow to snow (1)\n",
    "        im_classified_adj['classified'] = xr.where(im_classified_adj['elevation'] > elev_75_snow, 1,\n",
    "                                                   im_classified_adj['classified'])\n",
    "        # create a binary mask for everything above the first instance of 75% snow-covered\n",
    "        elevation_threshold_mask = xr.where(im_classified.elevation > elev_75_snow, 1, 0) \n",
    "        \n",
    "    else:\n",
    "        im_classified_adj = im_classified\n",
    "\n",
    "    # -----Delineate snow lines\n",
    "    # create binary snow matrix\n",
    "    im_binary = xr.where(im_classified_adj > 2, 1, 0).classified.data\n",
    "    # fill holes in binary image (0s within 1s = 1)\n",
    "    im_binary_no_holes = binary_fill_holes(im_binary)\n",
    "    # find contours at a constant value of 0.5 (between 0 and 1)\n",
    "    contours = find_contours(im_binary_no_holes, 0.5)    \n",
    "    # convert contour points to image coordinates\n",
    "    contours_coords = []\n",
    "    for contour in contours:\n",
    "        # convert image pixel coordinates to real coordinates\n",
    "        fx = interp1d(range(0, len(im_classified_adj.x.data)), im_classified_adj.x.data)\n",
    "        fy = interp1d(range(0, len(im_classified_adj.y.data)), im_classified_adj.y.data)\n",
    "        coords = (fx(contour[:, 1]), fy(contour[:, 0]))\n",
    "        # zip points together\n",
    "        xy = list(zip([x for x in coords[0]],\n",
    "                      [y for y in coords[1]]))\n",
    "        contours_coords.append(xy)\n",
    "            \n",
    "    # convert list of coordinates to list of LineStrings\n",
    "    # do not include points in the no data mask or points above the elevation threshold\n",
    "    contour_lines = []\n",
    "    for contour_coords in contours_coords:\n",
    "        points_real = [Point(x,y) for x,y in contour_coords\n",
    "                       if (im_classified.sel(x=x, y=y, method='nearest').no_data_mask.data.item()==True)\n",
    "                       and (elevation_threshold_mask.sel(x=x, y=y, method='nearest').data.item()==0)]\n",
    "        \n",
    "        if len(points_real)>2: # need at least 3 points for a LineString\n",
    "            contour_line = LineString([[point.x, point.y] for point in points_real])\n",
    "            contour_lines.append(contour_line)\n",
    "    \n",
    "    # proceed if lines were found after filtering\n",
    "    if len(contour_lines) > 0:\n",
    "        \n",
    "        # -----Use the longest line as the snowline\n",
    "        lengths = [line.length for line in contour_lines]\n",
    "        max_length_index = max(range(len(contour_lines)), key=lambda i: lengths[i])\n",
    "        snowline = contour_lines[max_length_index]\n",
    "        \n",
    "        # -----Interpolate elevations at snow line coordinates\n",
    "        # compile all line coordinates into arrays of x- and y-coordinates\n",
    "        xpts = np.ravel([x for x in snowline.coords.xy[0]])\n",
    "        ypts = np.ravel([y for y in snowline.coords.xy[1]])\n",
    "        # interpolate elevation at snow line points\n",
    "        snowline_elevs = [im_classified.sel(x=x, y=y, method='nearest').elevation.data.item()\n",
    "                          for x, y in list(zip(xpts, ypts))]\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        snowline = []\n",
    "        snowline_elevs = np.nan\n",
    "        \n",
    "    # -----If AOI is ~covered in snow, set snowline elevation to the minimum elevation in the AOI\n",
    "    if np.all(np.isnan(snowline_elevs)) and (np.nanmedian(hist_snow_est_elev_norm) > 0.5):\n",
    "        snowline_elevs = np.nanmin(np.ravel(im_classified.elevation.data))\n",
    "        \n",
    "    # -----Calculate snow-covered area (SCA) and accumulation area ratio (AAR)\n",
    "    # pixel resolution\n",
    "    dx = im_classified.x.data[1] - im_classified.x.data[0]\n",
    "    # snow-covered area\n",
    "    sca = len(np.ravel(im_classified.classified.data[im_classified.classified.data <= 2])) * (\n",
    "            dx ** 2)  # number of snow-covered pixels * pixel resolution [m^2]\n",
    "    # accumulation area ratio\n",
    "    total_area = len(np.ravel(im_classified.classified.data[~np.isnan(im_classified.classified.data)])) * (\n",
    "            dx ** 2)  # number of pixels * pixel resolution [m^2]\n",
    "    aar = sca / total_area\n",
    "    \n",
    "    # -----Compile results in dataframe\n",
    "    # calculate median snow line elevation\n",
    "    median_snowline_elev = np.nanmedian(snowline_elevs)\n",
    "    # compile results in df\n",
    "    if type(snowline)==LineString:\n",
    "        snowlines_coords_x = [ snowline.coords.xy[0] ]\n",
    "        snowlines_coords_y = [ snowline.coords.xy[1] ]\n",
    "    else:\n",
    "        snowlines_coords_x = [[]]\n",
    "        snowlines_coords_y = [[]]\n",
    "    snowline_df = pd.DataFrame({'site_name': [site_name],\n",
    "                                'datetime': [im_date],\n",
    "                                'snowlines_coords_X': snowlines_coords_x,\n",
    "                                'snowlines_coords_Y': snowlines_coords_y,\n",
    "                                'CRS': ['EPSG:' + str(im_xr.rio.crs.to_epsg())],\n",
    "                                'snowline_elevs_m': [snowline_elevs],\n",
    "                                'snowline_elevs_median_m': [median_snowline_elev],\n",
    "                                'SCA_m2': [sca],\n",
    "                                'AAR': [aar],\n",
    "                                'dataset': [dataset],\n",
    "                                'geometry': [snowline]\n",
    "                                })\n",
    "    \n",
    "     # -----Save snowline df to file\n",
    "    # reduce memory storage of dataframe\n",
    "    # snowline_df = f.reduce_memory_usage(snowline_df, verbose=False)\n",
    "    # save using user-specified file extension\n",
    "    # if 'pkl' in snowline_fn:\n",
    "    #     snowline_df.to_pickle(out_path + snowline_fn)\n",
    "    #     if verbose:\n",
    "    #         print('Snowline saved to file: ' + out_path + snowline_fn)\n",
    "    # elif 'csv' in snowline_fn:\n",
    "    #     snowline_df.to_csv(out_path + snowline_fn, index=False)\n",
    "    #     if verbose:\n",
    "    #         print('Snowline saved to file: ' + out_path + snowline_fn)\n",
    "    # else:\n",
    "    #     print('Please specify snowline_fn with extension .pkl or .csv. Exiting...')\n",
    "    #     return 'N/A'\n",
    "    \n",
    "    # -----Plot results\n",
    "    if plot_results:\n",
    "        fig, ax = plt.subplots(2, 2, figsize=(12, 8), gridspec_kw={'height_ratios': [3, 1]})\n",
    "        ax = ax.flatten()\n",
    "        # define x and y limits\n",
    "        xmin, xmax = aoi.geometry[0].buffer(100).bounds[0] / 1e3, aoi.geometry[0].buffer(100).bounds[2] / 1e3\n",
    "        ymin, ymax = aoi.geometry[0].buffer(100).bounds[1] / 1e3, aoi.geometry[0].buffer(100).bounds[3] / 1e3\n",
    "        # define colors for plotting\n",
    "        colors = list(dataset_dict['classified_image']['class_colors'].values())\n",
    "        cmp = matplotlib.colors.ListedColormap(colors)\n",
    "        # RGB image\n",
    "        ax[0].imshow(np.dstack([im_xr[ds_dict['RGB_bands'][0]].data,\n",
    "                                im_xr[ds_dict['RGB_bands'][1]].data,\n",
    "                                im_xr[ds_dict['RGB_bands'][2]].data]),\n",
    "                     extent=(np.min(im_xr.x.data) / 1e3, np.max(im_xr.x.data) / 1e3, np.min(im_xr.y.data) / 1e3,\n",
    "                             np.max(im_xr.y.data) / 1e3))\n",
    "        ax[0].set_xlabel('Easting [km]')\n",
    "        ax[0].set_ylabel('Northing [km]')\n",
    "        # classified image\n",
    "        ax[1].imshow(im_classified['classified'].data, cmap=cmp, clim=(1, 5),\n",
    "                     extent=(np.min(im_classified.x.data) / 1e3, np.max(im_classified.x.data) / 1e3,\n",
    "                             np.min(im_classified.y.data) / 1e3, np.max(im_classified.y.data) / 1e3))\n",
    "        # snowline coordinates\n",
    "        if type(snowline)==LineString:\n",
    "            ax[0].plot(np.divide(snowline.coords.xy[0], 1e3), np.divide(snowline.coords.xy[1], 1e3),\n",
    "                       '.', color='#f768a1', markersize=2)\n",
    "            ax[1].plot(np.divide(snowline.coords.xy[0], 1e3), np.divide(snowline.coords.xy[1], 1e3),\n",
    "                       '.', color='#f768a1', markersize=2)\n",
    "        # plot dummy points for legend\n",
    "        ax[1].scatter(0, 0, color=colors[0], s=50, label='Snow')\n",
    "        ax[1].scatter(0, 0, color=colors[1], s=50, label='Shadowed snow')\n",
    "        ax[1].scatter(0, 0, color=colors[2], s=50, label='Ice')\n",
    "        ax[1].scatter(0, 0, color=colors[3], s=50, label='Rock')\n",
    "        ax[1].scatter(0, 0, color=colors[4], s=50, label='Water')\n",
    "        if type(snowline)==LineString:\n",
    "            ax[0].scatter(0, 0, color='#f768a1', s=25, label='Snowline estimate')        \n",
    "            ax[1].scatter(0, 0, color='#f768a1', s=25, label='Snowline estimate')\n",
    "        ax[1].set_xlabel('Easting [km]')\n",
    "        # AOI\n",
    "        label='AOI'\n",
    "        if type(aoi.geometry[0].boundary) == MultiLineString:\n",
    "            for ii, geom in enumerate(aoi.geometry[0].boundary.geoms):\n",
    "                if ii >0 :\n",
    "                    label = '_nolegend_'\n",
    "                ax[0].plot(np.divide(geom.coords.xy[0], 1e3),\n",
    "                           np.divide(geom.coords.xy[1], 1e3), '-k', linewidth=1, label=label)\n",
    "                ax[1].plot(np.divide(geom.coords.xy[0], 1e3),\n",
    "                           np.divide(geom.coords.xy[1], 1e3), '-k', linewidth=1, label=label)\n",
    "        elif type(aoi.geometry[0].boundary) == LineString:\n",
    "            ax[0].plot(np.divide(aoi.geometry[0].boundary.coords.xy[0], 1e3),\n",
    "                       np.divide(aoi.geometry[0].boundary.coords.xy[1], 1e3), '-k', linewidth=1, label=label)\n",
    "            ax[1].plot(np.divide(aoi.geometry[0].boundary.coords.xy[0], 1e3),\n",
    "                       np.divide(aoi.geometry[0].boundary.coords.xy[1], 1e3), '-k', linewidth=1, label=label)\n",
    "                    \n",
    "        # reset x and y limits\n",
    "        ax[0].set_xlim(xmin, xmax)\n",
    "        ax[0].set_ylim(ymin, ymax)\n",
    "        ax[1].set_xlim(xmin, xmax)\n",
    "        ax[1].set_ylim(ymin, ymax)\n",
    "        # image bands histogram\n",
    "        ax[2].hist(im_xr[ds_dict['RGB_bands'][0]].data.flatten(), color='blue', histtype='step', linewidth=2,\n",
    "                   bins=100, label=\"blue\")\n",
    "        ax[2].hist(im_xr[ds_dict['RGB_bands'][1]].data.flatten(), color='green', histtype='step', linewidth=2,\n",
    "                   bins=100, label=\"green\")\n",
    "        ax[2].hist(im_xr[ds_dict['RGB_bands'][2]].data.flatten(), color='red', histtype='step', linewidth=2,\n",
    "                   bins=100, label=\"red\")\n",
    "        ax[2].set_xlabel(\"Surface reflectance\")\n",
    "        ax[2].set_ylabel(\"Pixel counts\")\n",
    "        ax[2].grid()\n",
    "        # normalized snow elevations histogram\n",
    "        ax[3].bar(bin_centers, hist_snow_est_elev_norm, width=(bin_centers[1] - bin_centers[0]), color=colors[0],\n",
    "                  align='center')\n",
    "        ax[3].plot([median_snowline_elev, median_snowline_elev], [0, 1], '-', color='#f768a1', \n",
    "                   linewidth=3, label='Median snowline elevation')\n",
    "        ax[3].set_xlabel(\"Elevation [m]\")\n",
    "        ax[3].set_ylabel(\"Fraction snow-covered\")\n",
    "        ax[3].grid()\n",
    "        ax[3].set_xlim(elev_min - 10, elev_max + 10)\n",
    "        ax[3].set_ylim(0, 1)\n",
    "        # determine figure title and file name\n",
    "        title = im_date.replace('-', '').replace(':', '') + '_' + site_name + '_' + dataset + '_snow-cover'\n",
    "        # add legends\n",
    "        ax[0].legend(loc='best')\n",
    "        ax[1].legend(loc='best')\n",
    "        ax[2].legend(loc='upper right')\n",
    "        ax[3].legend(loc='lower right')\n",
    "        fig.suptitle(title)\n",
    "        fig.tight_layout()\n",
    "        # save figure\n",
    "        fig_fn = figures_out_path + title + '.png'\n",
    "        fig.savefig(fig_fn, dpi=300, facecolor='white', edgecolor='none')\n",
    "        if verbose:\n",
    "            print('Figure saved to file:' + fig_fn)\n",
    "            \n",
    "    return snowline_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d677d866",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Load trained classifier and feature columns\n",
    "clf_fn = base_path+'inputs-outputs/Sentinel-2_SR_classifier_all_sites.joblib'\n",
    "clf = load(clf_fn)\n",
    "feature_cols_fn = base_path+'inputs-outputs/Sentinel-2_SR_feature_columns.json'\n",
    "feature_cols = json.load(open(feature_cols_fn))\n",
    "\n",
    "# -----Loop through images\n",
    "if type(im_list)==str: # check that images were found\n",
    "    print('No images found to classify, quiting...')\n",
    "else:\n",
    "    \n",
    "    for i in tqdm(range(0, len(im_list))):\n",
    "        \n",
    "        # -----Subset image using loop index\n",
    "        im_xr = im_list[i]\n",
    "        im_date = str(im_xr.time.data[0])[0:19]\n",
    "        print(im_date)\n",
    "        \n",
    "        # -----Adjust image for image scalar and no data values\n",
    "        # replace no data values with NaN and account for image scalar\n",
    "        crs = im_xr.rio.crs.to_epsg()\n",
    "        if np.nanmean(im_xr['B2'])>1e3:\n",
    "            im_xr = xr.where(im_xr==dataset_dict[dataset]['no_data_value'], np.nan, \n",
    "                             im_xr / dataset_dict[dataset]['image_scalar'])\n",
    "        else:\n",
    "            im_xr = xr.where(im_xr==dataset_dict[dataset]['no_data_value'], np.nan, im_xr)\n",
    "        # add NDSI band\n",
    "        im_xr['NDSI'] = ((im_xr[dataset_dict[dataset]['NDSI_bands'][0]] - im_xr[dataset_dict[dataset]['NDSI_bands'][1]]) \n",
    "                             / (im_xr[dataset_dict[dataset]['NDSI_bands'][0]] + im_xr[dataset_dict[dataset]['NDSI_bands'][1]]))\n",
    "        im_xr.rio.write_crs('EPSG:'+str(crs), inplace=True)\n",
    "                \n",
    "        # -----Classify image\n",
    "        # check if classified image already exists in file\n",
    "        im_classified_fn = im_date.replace('-','').replace(':','') + '_' + site_name + '_' + dataset + '_classified.nc'\n",
    "        if os.path.exists(im_classified_path + im_classified_fn):\n",
    "            print('Classified image already exists in file, continuing...')\n",
    "            im_classified = xr.open_dataset(im_classified_path + im_classified_fn)\n",
    "            # remove no data values\n",
    "            im_classified = xr.where(im_classified==-9999, np.nan, im_classified)\n",
    "        else:  \n",
    "            # classify image\n",
    "            im_classified = f.classify_image(im_xr, clf, feature_cols, crop_to_AOI, AOI_UTM, DEM,\n",
    "                                             dataset_dict, dataset, im_classified_fn, im_classified_path)\n",
    "            if type(im_classified)==str: # skip if error in classification\n",
    "                continue\n",
    "        \n",
    "        # -----Delineate snowline(s)\n",
    "        # check if snowline already exists in file\n",
    "        snowline_fn = im_date.replace('-','').replace(':','') + '_' + site_name + '_' + dataset + '_snowline.csv'\n",
    "        # if os.path.exists(snowlines_path + snowline_fn):\n",
    "        #     print('Snowline already exists in file, continuing...')\n",
    "        #     continue # no need to load snowline if it already exists\n",
    "        # else:\n",
    "        plot_results = True\n",
    "        # create directory for figures if it doesn't already exist\n",
    "        if (not os.path.exists(figures_out_path)) & plot_results:\n",
    "            os.mkdir(figures_out_path)\n",
    "            print('Created directory for output figures: ' + figures_out_path)\n",
    "        snowline_df = delineate_snowline(im_xr, im_classified, site_name, AOI_UTM, dataset_dict, dataset, \n",
    "                                         im_date, snowline_fn, snowlines_path, figures_out_path, plot_results)\n",
    "        plt.show()\n",
    "        print('Accumulation Area Ratio =  ' + str(snowline_df['AAR'][0]))\n",
    "        print(' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deef487",
   "metadata": {},
   "source": [
    "## 3. Landsat 8/9 SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d4b791",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Query GEE for imagery (and download to L_im_path if necessary)\n",
    "dataset = 'Landsat'\n",
    "im_list = f.query_gee_for_imagery(dataset_dict, dataset, AOI_UTM, date_start, date_end, month_start, month_end,\n",
    "                                  cloud_cover_max, mask_clouds, L_im_path, im_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637986b0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Load trained classifier and feature columns\n",
    "clf_fn = base_path+'inputs-outputs/Landsat_classifier_all_sites.joblib'\n",
    "clf = load(clf_fn)\n",
    "feature_cols_fn = base_path+'inputs-outputs/Landsat_feature_columns.json'\n",
    "feature_cols = json.load(open(feature_cols_fn))\n",
    "\n",
    "# -----Loop through images\n",
    "if type(im_list)==str: # check that images were found\n",
    "    print('No images found to classify, quitting...')\n",
    "else:\n",
    "    \n",
    "    for i in tqdm(range(0, len(im_list))):\n",
    "        \n",
    "        # -----Subset image using loop index\n",
    "        im_xr = im_list[i]\n",
    "        im_date = str(im_xr.time.data[0])[0:19]\n",
    "        print(im_date)\n",
    "        \n",
    "        # -----Adjust image for image scalar and no data values\n",
    "        # replace no data values with NaN and account for image scalar\n",
    "        crs = im_xr.rio.crs.to_epsg()\n",
    "        # add NDSI band\n",
    "        im_xr['NDSI'] = ((im_xr[dataset_dict[dataset]['NDSI_bands'][0]] - im_xr[dataset_dict[dataset]['NDSI_bands'][1]]) \n",
    "                         / (im_xr[dataset_dict[dataset]['NDSI_bands'][0]] + im_xr[dataset_dict[dataset]['NDSI_bands'][1]]))\n",
    "        im_xr.rio.write_crs('EPSG:'+str(crs), inplace=True)\n",
    "                \n",
    "        # -----Classify image\n",
    "        # check if classified image already exists in file\n",
    "        im_classified_fn = im_date.replace('-','').replace(':','') + '_' + site_name + '_' + dataset + '_classified.nc'\n",
    "        if os.path.exists(im_classified_path + im_classified_fn):\n",
    "            print('Classified image already exists in file, continuing...')\n",
    "            im_classified = xr.open_dataset(im_classified_path + im_classified_fn)\n",
    "            # remove no data values\n",
    "            im_classified = xr.where(im_classified==-9999, np.nan, im_classified)\n",
    "        else:  \n",
    "            # classify image\n",
    "            im_classified = f.classify_image(im_xr, clf, feature_cols, crop_to_AOI, AOI_UTM, DEM,\n",
    "                                             dataset_dict, dataset, im_classified_fn, im_classified_path)\n",
    "            if type(im_classified)==str: # skip if error in classification\n",
    "                continue\n",
    "        \n",
    "        # -----Delineate snowline(s)\n",
    "        # check if snowline already exists in file\n",
    "        snowline_fn = im_date.replace('-','').replace(':','') + '_' + site_name + '_' + dataset + '_snowline.csv'\n",
    "        if os.path.exists(snowlines_path + snowline_fn):\n",
    "            print('Snowline already exists in file, continuing...')\n",
    "            continue # no need to load snowline if it already exists\n",
    "        else:\n",
    "            plot_results = True\n",
    "            # create directory for figures if it doesn't already exist\n",
    "            if (not os.path.exists(figures_out_path)) & plot_results:\n",
    "                os.mkdir(figures_out_path)\n",
    "                print('Created directory for output figures: '+figures_out_path)\n",
    "            snowline_df = f.delineate_image_snowline(im_xr, im_classified, site_name, AOI_UTM, dataset_dict, dataset, \n",
    "                                                     im_date, snowline_fn, snowlines_path, figures_out_path, plot_results)\n",
    "            # plt.show()\n",
    "            print('Accumulation Area Ratio =  ' + str(snowline_df['AAR'][0]))\n",
    "        print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b5ce4c",
   "metadata": {},
   "source": [
    "## 4. PlanetScope SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2e4584",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Read surface reflectance image file names\n",
    "os.chdir(PS_im_path)\n",
    "im_fns = sorted(glob.glob('*SR*.tif'))\n",
    "\n",
    "# ----Mask clouds and cloud shadows in all images\n",
    "plot_results = False\n",
    "if mask_clouds:\n",
    "    print('Masking images using cloud bitmask...')\n",
    "    for im_fn in tqdm(im_fns):\n",
    "        f.planetscope_mask_image_pixels(PS_im_path, im_fn, PS_im_masked_path, save_outputs, plot_results)\n",
    "# read masked image file names\n",
    "os.chdir(PS_im_masked_path)\n",
    "im_masked_fns = sorted(glob.glob('*_mask.tif'))\n",
    "    \n",
    "# -----Mosaic images captured within same hour\n",
    "print('Mosaicking images captured in the same hour...')\n",
    "if mask_clouds: \n",
    "    f.planetscope_mosaic_images_by_date(PS_im_masked_path, im_masked_fns, PS_im_mosaics_path, AOI_UTM)\n",
    "    print(' ')\n",
    "else:\n",
    "    f.planetscope_mosaic_images_by_date(PS_im_path, im_fns, PS_im_mosaics_path, AOI_UTM)\n",
    "    print(' ')\n",
    "    \n",
    "# -----Load trained classifier and feature columns\n",
    "clf_fn = base_path+'inputs-outputs/PlanetScope_classifier_all_sites.joblib'\n",
    "clf = load(clf_fn)\n",
    "feature_cols_fn = base_path+'inputs-outputs/PlanetScope_feature_columns.json'\n",
    "feature_cols = json.load(open(feature_cols_fn))\n",
    "dataset = 'PlanetScope'\n",
    "\n",
    "# -----Adjust image radiometry\n",
    "# read mosaicked image file names\n",
    "os.chdir(PS_im_mosaics_path)\n",
    "im_mosaic_fns = sorted(glob.glob('*.tif'))\n",
    "# create polygon(s) of the top and bottom 20th percentile elevations within the AOI\n",
    "polygons_top, polygons_bottom = f.create_aoi_elev_polys(AOI_UTM, DEM)\n",
    "# loop through images\n",
    "for im_mosaic_fn in tqdm(im_mosaic_fns[104:]):\n",
    "    \n",
    "    # -----Open image mosaic\n",
    "    im_da = xr.open_dataset(PS_im_mosaics_path + im_mosaic_fn)\n",
    "    # determine image date from image mosaic file name\n",
    "    im_date = im_mosaic_fn[0:4] + '-' + im_mosaic_fn[4:6] + '-' + im_mosaic_fn[6:8] + 'T' + im_mosaic_fn[9:11] + ':00:00'\n",
    "    im_dt = np.datetime64(im_date)\n",
    "    print(im_date)\n",
    "    \n",
    "    # -----Adjust radiometry\n",
    "    im_adj, im_adj_method = f.planetscope_adjust_image_radiometry(im_da, im_dt, polygons_top, polygons_bottom, dataset_dict, skip_clipped)\n",
    "    if type(im_adj)==str: # skip if there was an error in adjustment\n",
    "        continue\n",
    "    \n",
    "    # -----Classify image\n",
    "    im_classified_fn = im_date.replace('-','').replace(':','') + '_' + site_name + '_' + dataset + '_classified.nc'\n",
    "    if os.path.exists(im_classified_path + im_classified_fn):\n",
    "        print('Classified image already exists in file, loading...')\n",
    "        im_classified = xr.open_dataset(im_classified_path + im_classified_fn)\n",
    "        # remove no data values\n",
    "        im_classified = xr.where(im_classified==-9999, np.nan, im_classified)\n",
    "    else:\n",
    "        im_classified = f.classify_image(im_adj, clf, feature_cols, crop_to_AOI, AOI_UTM, DEM,\n",
    "                                         dataset_dict, dataset, im_classified_fn, im_classified_path)\n",
    "    if type(im_classified)==str:\n",
    "        continue    \n",
    "    \n",
    "    # -----Delineate snowline(s)\n",
    "    plot_results=True\n",
    "    # create directory for figures if it doesn't already exist\n",
    "    if (not os.path.exists(figures_out_path)) & plot_results:\n",
    "        os.mkdir(figures_out_path)\n",
    "        print('Created directory for output figures: '+figures_out_path)\n",
    "    # check if snowline already exists in file\n",
    "    snowline_fn = im_date.replace('-','').replace(':','') + '_' + site_name + '_' + dataset + '_snowline.csv'\n",
    "    if os.path.exists(snowlines_path + snowline_fn):\n",
    "        print('Snowline already exists in file, skipping...')\n",
    "    else:\n",
    "        snowline_df = f.delineate_image_snowline(im_adj, im_classified, site_name, AOI_UTM, dataset_dict, dataset, \n",
    "                                                 im_date, snowline_fn, snowlines_path, figures_out_path, plot_results)\n",
    "        # plt.show()\n",
    "        print('Accumulation Area Ratio =  ' + str(snowline_df['AAR'][0]))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536857e1",
   "metadata": {},
   "source": [
    "## _Optional_: Compile individual figures into a single .gif file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3b4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Modify the strings below according to your file names ###\n",
    "\n",
    "# identify the string that is present in all filenames of the figures that you want to compile\n",
    "fig_fns_str = '*' + site_name + '_' + dataset + '_*snow-cover.png'\n",
    "# define the output .gif filename\n",
    "gif_fn = site_name + '_' + dataset + '_' + date_start.replace('-','') + '_' + date_end.replace('-','') + 'snow-cover.gif' \n",
    "\n",
    "# -----Make a .gif of output images\n",
    "from PIL import Image as PIL_Image\n",
    "from IPython.display import Image as IPy_Image\n",
    "os.chdir(figures_out_path)\n",
    "fig_fns = glob.glob(fig_fns_str) # load all output figure file names\n",
    "fig_fns = sorted(fig_fns) # sort chronologically\n",
    "\n",
    "# grab figures date range for .gif file name\n",
    "frames = [PIL_Image.open(im) for im in fig_fns]\n",
    "frame_one = frames[0]\n",
    "frame_one.save(figures_out_path + gif_fn, format=\"GIF\", append_images=frames, save_all=True, duration=2000, loop=0)\n",
    "print('GIF saved to file:' + figures_out_path + gif_fn)\n",
    "\n",
    "\n",
    "# -----Clean up: delete individual figure files\n",
    "for fn in fig_fns:\n",
    "    os.remove(os.path.join(figures_out_path, fn))\n",
    "print('Individual figure files deleted.')\n",
    "\n",
    "# -----Display .gif\n",
    "IPy_Image(filename = figures_out_path + gif_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
