{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0288858c",
   "metadata": {},
   "source": [
    "## Notebook to develop supervised classification algorithm for identifying snow in PlanetScope 4-band, Landsat 8, Sentinel-2, and MODIS imagery\n",
    "Rainey Aberle\n",
    "\n",
    "Adapted from the [SciKit Learn Classifier comparison tutorial](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n",
    "\n",
    "### Outline:\n",
    "1. Set up training data\n",
    "- PlanetScope\n",
    "- Landsat 8/9\n",
    "- Sentinel-2 SR\n",
    "- Sentinel-2 TOA\n",
    "- MODIS\n",
    "    \n",
    "2. Develop supervised classifiers for EACH site and ALL sites\n",
    "\n",
    "3. *Optional*\n",
    "- Test how the number of points used to train impacts model accuracies\n",
    "- Sentinel-1 (not recommended)\n",
    "\n",
    "### 0. Initial Set-up: \n",
    "\n",
    "#### Import packages, define paths in directory, authenticate Google Earth Engine (GEE), define classification settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a8ee8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Import packages\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import ee\n",
    "import richdem as rd\n",
    "import scipy\n",
    "import wxee as wx\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "from shapely.geometry import Polygon\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import sys\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Point, MultiPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b3c2b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Determine whether to save outputs to file\n",
    "save_outputs = False # = True to save output figures and best classifier \n",
    "\n",
    "# -----Define paths in directory\n",
    "# base directory (path to snow-cover-mapping/)\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/'\n",
    "# output folder for best classifier\n",
    "out_path = base_path + 'inputs-outputs/'\n",
    "# output folder for figures\n",
    "figures_out_path = base_path + 'figures/'\n",
    "# path to classified points used to train and test classifiers\n",
    "data_pts_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/classified-points/'\n",
    "\n",
    "# -----Determine settings\n",
    "terrain_parameters = False # whether to use terrain parameters (elevation, slope, aspect) in classification\n",
    "save_figures = True # whether to save output figures\n",
    "\n",
    "# -----Add path to functions\n",
    "sys.path.insert(1, base_path + 'functions/')\n",
    "import pipeline_utils as f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aba55cb",
   "metadata": {},
   "source": [
    "### Authenticate and initialize Google Earth Engine (GEE)\n",
    "\n",
    "__Note:__ The first time you run the following cell, you will be asked to authenticate your GEE account for use in this notebook. This will send you to an external web page, where you will walk through the GEE authentication workflow and copy an authentication code back in this notebook when prompted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2db7f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ee.Initialize()\n",
    "except: \n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07f42b5",
   "metadata": {},
   "source": [
    "### Create dictionary of dataset-specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f0f6f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_dict = {\n",
    "    'PlanetScope':{\n",
    "        'im_scalar': 1e4,\n",
    "        'bands': {\n",
    "            'blue': '0',\n",
    "            'green': '1',\n",
    "            'red': '2',\n",
    "            'NIR': '3'\n",
    "        },\n",
    "        'NDSI': ['green', 'NIR'],\n",
    "        'no_data_value': -9999,\n",
    "        'resolution_m': 3,\n",
    "        'RGB_bands': ['red', 'green', 'blue']\n",
    "    },\n",
    "    \n",
    "    'Landsat': {\n",
    "        'im_scalar': 1/2.75e-05,\n",
    "        'bands': {\n",
    "            # 'SR_B1': 'ultra_blue',\n",
    "            'SR_B2': 'blue',\n",
    "            'SR_B3': 'green',\n",
    "            'SR_B4': 'red',\n",
    "            'SR_B5': 'NIR',\n",
    "            'SR_B6': 'SWIR1',\n",
    "            'SR_B7': 'SWIR2',\n",
    "            'QA_PIXEL': 'pixel_quality'\n",
    "        },\n",
    "        'NDSI': ['SR_B3', 'SR_B6'],\n",
    "        'no_data_value': -32768,\n",
    "        'resolution_m': 30,\n",
    "        'RGB_bands': ['SR_B4', 'SR_B3', 'SR_B2']\n",
    "    },\n",
    "    \n",
    "    'Sentinel2_SR':{\n",
    "        'im_scalar': 1e4,\n",
    "        'bands': {\n",
    "            # 'B1': 'aerosols',\n",
    "            'B2': 'blue',\n",
    "            'B3': 'green',\n",
    "            'B4': 'red',\n",
    "            'B5': 'red_edge1',\n",
    "            'B6': 'red_edge2',\n",
    "            # 'B7': 'red_edge3',\n",
    "            'B8': 'NIR',\n",
    "            # 'B8A': 'red_edge4',\n",
    "            # 'B9': 'water_vapor',\n",
    "            'B11': 'SWIR1',\n",
    "            'B12': 'SWIR2',\n",
    "            'QA60': 'cloud_mask'\n",
    "        },\n",
    "        'NDSI': ['B3', 'B11'],\n",
    "        'no_data_value': -32768,\n",
    "        'resolution_m': 10,\n",
    "        'RGB_bands': ['B4', 'B3', 'B2']\n",
    "    },\n",
    "    \n",
    "    'Sentinel2_TOA':{\n",
    "        'im_scalar': 1e4,\n",
    "        'bands': {\n",
    "            # 'B1': 'aerosols',\n",
    "            'B2': 'blue',\n",
    "            'B3': 'green',\n",
    "            'B4': 'red',\n",
    "            'B5': 'red_edge1',\n",
    "            'B6': 'red_edge2',\n",
    "            # 'B7': 'red_edge3',\n",
    "            'B8': 'NIR',\n",
    "            # 'B8A': 'red_edge4',\n",
    "            # 'B9': 'water_vapor',\n",
    "            'B11': 'SWIR1',\n",
    "            'B12': 'SWIR2',\n",
    "            'QA60': 'cloud_mask'\n",
    "        },\n",
    "        'NDSI': ['B3', 'B11'],\n",
    "        'no_data_value': -32768,\n",
    "        'resolution_m': 10,\n",
    "        'RGB_bands': ['B4', 'B3', 'B2']\n",
    "    },\n",
    "    \n",
    "    'MODIS': {\n",
    "        'im_scalar': 1e4,\n",
    "        'bands': {\n",
    "            'sur_refl_b01': 'red',\n",
    "            'sur_refl_b02': 'NIR1',\n",
    "            'sur_refl_b03': 'blue',\n",
    "            'sur_refl_b04': 'green',\n",
    "            'sur_refl_b05': 'NIR2',\n",
    "            'sur_refl_b06': 'SWIR1',\n",
    "            'sur_refl_b07': 'SWIR2',\n",
    "            'sur_refl_qc_500m': 'pixel_quality',\n",
    "        },\n",
    "        'NDSI': ['sur_refl_b04', 'sur_refl_b06'],\n",
    "        'no_data_value': -28672,\n",
    "        'resolution_m': 1000,\n",
    "        'RGB_bands': ['sur_refl_b01', 'sur_refl_b04', 'sur_refl_b03']\n",
    "    }\n",
    "        \n",
    "}\n",
    "\n",
    "# save dictionary as pickle file\n",
    "# with open(out_path + 'datasets_characteristics.pkl', 'wb') as fn:\n",
    "#     pickle.dump(dataset_dict, fn)\n",
    "# print('dictionary saved to file: ' + out_path + 'datasets_characteristics.pkl')\n",
    "# # open dictionary and display\n",
    "# with open(out_path + 'datasets_characteristics.pkl', 'rb') as fn:\n",
    "#     dataset_dict = pickle.load(fn)\n",
    "# dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95831d5a",
   "metadata": {},
   "source": [
    "### Define supervised classification algorithms to test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e25dbb13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Classifier names\n",
    "names = [\n",
    "    # \"Gaussian Process\", # keeps crashing kernel when classifying images!\n",
    "    \"Nearest Neighbors\",\n",
    "    # \"Linear SVM\", # keeps crashing kernel \n",
    "    \"RBF SVM\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "    \"Logistic Regression\"\n",
    "]\n",
    "\n",
    "# -----Classifiers\n",
    "classifiers = [\n",
    "    # GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    KNeighborsClassifier(3),\n",
    "    # SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    LogisticRegression(random_state = 0)\n",
    "]\n",
    "\n",
    "# -----Define site names\n",
    "site_names = ['Gulkana', 'SouthCascade', 'Sperry', 'Wolverine']\n",
    "\n",
    "# -----Define number of folds to use in K-folds cross-validation\n",
    "num_folds = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225cc2f7",
   "metadata": {},
   "source": [
    "## 1. Set up training data\n",
    "\n",
    "### a. PlanetScope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0d10c56",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Gulkana\n",
      "----------\n",
      "Gulkana_20210615_16_snow.shp\n",
      "Gulkana_20210615_16_snow-shadowed.shp\n",
      "Gulkana_20210615_16_rock.shp\n",
      "Gulkana_20210806_19_snow.shp\n",
      "Gulkana_20210806_19_snow-shadowed.shp\n",
      "Gulkana_20210806_19_ice.shp\n",
      "Gulkana_20210806_19_rock.shp\n",
      "----------\n",
      "SouthCascade\n",
      "----------\n",
      "SouthCascade_20210703_19_snow.shp\n",
      "SouthCascade_20210703_19_rock.shp\n",
      "SouthCascade_20210703_19_water.shp\n",
      "SouthCascade_20210828_19_snow.shp\n",
      "SouthCascade_20210828_19_ice.shp\n",
      "SouthCascade_20210828_19_rock.shp\n",
      "SouthCascade_20210828_19_water.shp\n",
      "----------\n",
      "Sperry\n",
      "----------\n",
      "Sperry_20210627_14_snow.shp\n",
      "Sperry_20210627_14_snow-shadowed.shp\n",
      "Sperry_20210627_14_rock.shp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning 1: TIFFFetchNormalTag:Incorrect value for \"RichTIFFIPTC\"; tag ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sperry_20210801_17_snow.shp\n",
      "Sperry_20210801_17_ice.shp\n",
      "Sperry_20210801_17_rock.shp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning 1: TIFFFetchNormalTag:Incorrect value for \"RichTIFFIPTC\"; tag ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Wolverine\n",
      "----------\n",
      "Wolverine_20180904_19_snow.shp\n",
      "Wolverine_20180904_19_firn.shp\n",
      "Wolverine_20180905_19_snow.shp\n",
      "Wolverine_20180905_19_firn.shp\n",
      "Wolverine_20180912_19_snow.shp\n",
      "Wolverine_20180912_19_firn.shp\n",
      "Wolverine_20180930_19_snow.shp\n",
      "Wolverine_20180930_19_firn.shp\n",
      "Wolverine_20210613_19_snow.shp\n",
      "Wolverine_20210613_19_snow-shadowed.shp\n",
      "Wolverine_20210613_19_ice.shp\n",
      "Wolverine_20210613_19_rock.shp\n",
      "Wolverine_20210815_20_snow.shp\n",
      "Wolverine_20210815_20_snow-shadowed.shp\n",
      "Wolverine_20210815_20_ice.shp\n",
      "Wolverine_20210815_20_rock.shp\n",
      "Mem. usage decreased to 0.77 Mb (9.7% reduction)\n",
      "PS training data saved to file:/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/inputs-outputs/PS_training_data.pkl\n",
      "Feature columns saved to file:  /Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/inputs-outputs/PS_feature_cols.pkl\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 214.375x180 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "figure saved to file:/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/figures/spectral_pairplot_PS_training_data.png\n"
     ]
    }
   ],
   "source": [
    "# -----Define dataset for reading dataset dictionary and feature columns\n",
    "dataset = 'PlanetScope'\n",
    "feature_cols_PS = ['blue', 'green', 'red', 'NIR', 'NDSI']\n",
    "\n",
    "# -----Load classified points\n",
    "os.chdir(data_pts_path)\n",
    "data_pts_fns = sorted(glob.glob('*.shp'))\n",
    "\n",
    "# -----Check if training data exist in file\n",
    "PS_training_data_fn = 'PS_training_data.pkl'\n",
    "if os.path.exists(out_path + PS_training_data_fn):\n",
    "    data_pts_full_PS = pd.read_pickle(out_path + PS_training_data_fn)\n",
    "    print('training data already exist... loaded from file.')\n",
    "    \n",
    "else: \n",
    "    \n",
    "    # Initialize data_pts_full to save info for single classifier (next step)\n",
    "    data_pts_full_PS = gpd.GeoDataFrame()  \n",
    "    \n",
    "    # Loop through sites\n",
    "    for i, site_name in enumerate(site_names):\n",
    "\n",
    "        print('----------')\n",
    "        print(site_name)\n",
    "        print('----------')\n",
    "\n",
    "        # determine number of images used for classified points\n",
    "        num_images = len([s for s in data_pts_fns if (site_name in s) and ('snow.shp' in s)])\n",
    "        im_dates = [s[len(site_name)+1:len(site_name)+9] for s in data_pts_fns if (site_name in s) and ('snow.shp' in s)]\n",
    "        # loop through each image\n",
    "        for j in range(0, num_images):\n",
    "            # determine image date\n",
    "            im_date = im_dates[j]\n",
    "            # load classified points\n",
    "            data_pts = pd.DataFrame() # dataframe to hold applicable data classes\n",
    "            # snow\n",
    "            if len([s for s in data_pts_fns if (site_name in s) and ('snow.shp' in s) and (im_date in s)])>0: # check if class exists for site and date\n",
    "                data_pts_snow_fn = [s for s in data_pts_fns if (site_name in s) and ('snow.shp' in s) and (im_date in s)][0]\n",
    "                data_pts_snow = gpd.read_file(data_pts_path + data_pts_snow_fn) # read file\n",
    "                data_pts_snow['class'] = 1 # determine class ID\n",
    "                data_pts = pd.concat([data_pts, data_pts_snow], ignore_index=True) # concatenate to full data points df\n",
    "                print(data_pts_snow_fn)\n",
    "            # shadowed snow\n",
    "            if len([s for s in data_pts_fns if (site_name in s) and ('snow-shadowed.shp' in s)  and (im_date in s)])>0: # check if class exists for site and date\n",
    "                data_pts_snow_sh_fn = [s for s in data_pts_fns if (site_name in s) and ('snow-shadowed.shp' in s) and (im_date in s)][0]\n",
    "                data_pts_snow_sh = gpd.read_file(data_pts_path + data_pts_snow_sh_fn) # read file\n",
    "                data_pts_snow_sh['class'] = 2 # determine class ID\n",
    "                data_pts = pd.concat([data_pts, data_pts_snow_sh], ignore_index=True) # concatenate to full data points df\n",
    "                print(data_pts_snow_sh_fn)\n",
    "            # firn\n",
    "            if len([s for s in data_pts_fns if (site_name in s) and ('firn.shp' in s)  and (im_date in s)])>0: # check if class exists for site and date\n",
    "                data_pts_snow_sh_fn = [s for s in data_pts_fns if (site_name in s) and ('firn.shp' in s) and (im_date in s)][0]\n",
    "                data_pts_snow_sh = gpd.read_file(data_pts_path + data_pts_snow_sh_fn) # read file\n",
    "                data_pts_snow_sh['class'] = 3 # determine class ID\n",
    "                data_pts = pd.concat([data_pts, data_pts_snow_sh], ignore_index=True) # concatenate to full data points df\n",
    "                print(data_pts_snow_sh_fn)\n",
    "            # ice\n",
    "            if len([s for s in data_pts_fns if (site_name in s) and ('ice.shp' in s) and (im_date in s)])>0: # check if class exists for site and date\n",
    "                data_pts_ice_fn = [s for s in data_pts_fns if (site_name in s) and ('ice.shp' in s)  and (im_date in s)][0]\n",
    "                data_pts_ice = gpd.read_file(data_pts_path + data_pts_ice_fn)  # read file\n",
    "                data_pts_ice['class'] = 4 # determine class ID\n",
    "                data_pts = pd.concat([data_pts, data_pts_ice], ignore_index=True) # concatenate to full data points df\n",
    "                print(data_pts_ice_fn)\n",
    "            # rock\n",
    "            if len([s for s in data_pts_fns if (site_name in s) and ('rock.shp' in s) and (im_date in s)])>0: # check if class exists for site and date\n",
    "                data_pts_rock_fn = [s for s in data_pts_fns if (site_name in s) and ('rock.shp' in s)  and (im_date in s)][0]\n",
    "                data_pts_rock = gpd.read_file(data_pts_path + data_pts_rock_fn) # read file\n",
    "                data_pts_rock['class'] = 5 # determine class ID\n",
    "                data_pts = pd.concat([data_pts, data_pts_rock], ignore_index=True) # concatenate to full data points df\n",
    "                print(data_pts_rock_fn)\n",
    "            # water\n",
    "            if len([s for s in data_pts_fns if (site_name in s) and ('water.shp' in s)  and (im_date in s)])>0: # check if class exists for site and date\n",
    "                data_pts_water_fn = [s for s in data_pts_fns if (site_name in s) and ('water.shp' in s) and (im_date in s)][0]\n",
    "                data_pts_water = gpd.read_file(data_pts_path + data_pts_water_fn) # read file\n",
    "                data_pts_water['class'] = 6 # determine class ID\n",
    "                data_pts = pd.concat([data_pts, data_pts_water], ignore_index=True) # concatenate to full data points df\n",
    "                print(data_pts_water_fn)\n",
    "\n",
    "            # Load image\n",
    "            Idate = data_pts_snow_fn.index('_')+1\n",
    "            im_fn = site_name+'_'+data_pts_snow_fn[Idate:Idate+11]+'.tif' # image file name\n",
    "            im_date = im_fn.split(site_name+'_')[1][:-4] # image capture date\n",
    "            im = xr.open_dataset(data_pts_path+im_fn) # open image\n",
    "            # adjust image values\n",
    "            im = xr.where(im==-9999, np.nan, im)\n",
    "            epsg = str(rxr.open_rasterio(data_pts_path+im_fn).rio.crs.to_epsg()) # grab EPSG code\n",
    "            # load AOI\n",
    "            AOI_path = data_pts_path + '../study-sites/'+site_name+'/AOIs/'\n",
    "            AOI_fn = glob.glob(AOI_path+site_name+'_USGS*.shp')[0]\n",
    "            AOI = gpd.read_file(AOI_fn)\n",
    "            AOI = AOI.to_crs('EPSG:'+epsg)\n",
    "            # load DEM\n",
    "            DEM_path = AOI_path + '../DEMs/'\n",
    "            DEM_fn = glob.glob(DEM_path+site_name+'*_DEM*.tif')[0]\n",
    "            DEM = xr.open_dataset(DEM_fn)\n",
    "            DEM = DEM.rename({'band_data':'elevation'})\n",
    "            DEM = DEM.rio.reproject('EPSG:'+epsg)\n",
    "            # adjust image radiometry\n",
    "            polygon_top, polygon_bottom, im_fn, im_rxr = f.create_AOI_elev_polys(AOI, data_pts_path, [im_fn], DEM)\n",
    "            im_adj = f.PS_adjust_image_radiometry(im_fn, data_pts_path, polygon_top, polygon_bottom, AOI, dataset_dict, \n",
    "                                                  dataset, site_name, skip_clipped=False, plot_results=True)[0]\n",
    "            \n",
    "            # Reformat data points coordinates\n",
    "            # reproject to image epsg\n",
    "            data_pts = data_pts.to_crs('EPSG:'+epsg) \n",
    "            # remove \"id\" column\n",
    "            data_pts = data_pts.drop(columns=['id'])\n",
    "            # change MultiPoint objects to Point objects, remove Null values\n",
    "            # geoms_points = []\n",
    "            data_pts['geometry'] = [Point(x.geoms[0]) if type(x)==MultiPoint else None for x in data_pts['geometry'].values]\n",
    "            data_pts = data_pts.dropna(subset=['geometry'])\n",
    "            # remove rows containing NaN\n",
    "            data_pts = data_pts.dropna()\n",
    "            data_pts = data_pts.reset_index(drop=True)\n",
    "            # add site_name column\n",
    "            data_pts['site_name'] = site_name\n",
    "            # add image date column\n",
    "            data_pts['PS_im_date'] = im_date\n",
    "            # sample band values at points\n",
    "            data_pts['blue'] = [im_adj.sel(x=x.coords.xy[0][0], y=x.coords.xy[1][0], method='nearest').blue.values for x in data_pts['geometry'].values]\n",
    "            data_pts['green'] = [im_adj.sel(x=x.coords.xy[0][0], y=x.coords.xy[1][0], method='nearest').green.values for x in data_pts['geometry'].values]\n",
    "            data_pts['red'] = [im_adj.sel(x=x.coords.xy[0][0], y=x.coords.xy[1][0], method='nearest').red.values for x in data_pts['geometry'].values]\n",
    "            data_pts['NIR'] = [im_adj.sel(x=x.coords.xy[0][0], y=x.coords.xy[1][0], method='nearest').NIR.values for x in data_pts['geometry'].values]\n",
    "            # add NDSI column\n",
    "            data_pts['NDSI'] = ((data_pts[dataset_dict[dataset]['NDSI'][0]] - data_pts[dataset_dict[dataset]['NDSI'][1]]) \n",
    "                                / (data_pts[dataset_dict[dataset]['NDSI'][0]] + data_pts[dataset_dict[dataset]['NDSI'][1]]))\n",
    "            if terrain_parameters==True:\n",
    "                # Load DEM\n",
    "                x = im.bounds.left, im.bounds.right, im.bounds.right, im.bounds.left, im.bounds.left\n",
    "                y = im.bounds.bottom, im.bounds.bottom, im.bounds.top, im.bounds.top, im.bounds.bottom\n",
    "                coords = list(zip(x,y))\n",
    "                bb_gdf = gpd.GeoDataFrame({'geometry': [Polygon(coords)]}, crs=im.crs)\n",
    "                DEM, AOI_UTM = f.query_GEE_for_DEM(bb_gdf, im_path, im_fn)\n",
    "                # flatten DEM to 2D\n",
    "                DEM_rd = rd.rdarray(DEM.elevation.data, no_data=-9999) # rich DEM array of DEM\n",
    "                # calculate slope and aspect using DEM\n",
    "                slope = rd.TerrainAttribute(DEM_rd, attrib='slope_degrees')\n",
    "                aspect = rd.TerrainAttribute(DEM_rd, attrib='aspect')\n",
    "                # convert from rdarray to numpy array\n",
    "                slope, aspect = np.array(slope).astype(int), np.array(aspect).astype(int)\n",
    "                # interpolate elevation at coords\n",
    "                f_DEM = scipy.interpolate.interp2d(DEM.x.data, DEM.y.data, DEM)\n",
    "                data_pts['elevation'] = [f_DEM(x[0], x[1])[0] for x in data_pts['coords'].values]\n",
    "                # interpolate slope at coords\n",
    "                f_slope = scipy.interpolate.interp2d(DEM_x, DEM_y, slope) \n",
    "                data_pts['slope'] = [f_slope(x[0], x[1])[0] for x in data_pts['coords'].values] \n",
    "                # interpolate aspect at coords\n",
    "                f_aspect = scipy.interpolate.interp2d(DEM_x, DEM_y, aspect) \n",
    "                data_pts['aspect'] = [f_aspect(x[0], x[1])[0] for x in data_pts['coords'].values]\n",
    "                # add month-of-year (moy) column\n",
    "                data_pts['moy'] = float(im_fn[4:6]) \n",
    "            \n",
    "            # Reproject back to WGS84 for compatibility\n",
    "            data_pts_WGS = data_pts.to_crs(4326)\n",
    "            \n",
    "            # Concatenate to full DataFrame\n",
    "            data_pts_full_PS = pd.concat([data_pts_full_PS, data_pts_WGS], ignore_index=True)\n",
    "    \n",
    "    # Reduce memory usage in data pts\n",
    "    data_pts_full_PS = f.reduce_memory_usage(data_pts_full_PS)\n",
    "    \n",
    "    # Save training data to file\n",
    "    data_pts_full_PS.to_pickle(out_path + PS_training_data_fn)\n",
    "    print('PS training data saved to file:' + out_path + PS_training_data_fn)\n",
    "    \n",
    "    # Save feature columns\n",
    "    feature_cols_fn = out_path + 'PS_feature_cols.pkl'\n",
    "    pickle.dump(feature_cols_PS, open(feature_cols_fn, 'wb'))\n",
    "    print('Feature columns saved to file: ', feature_cols_fn)\n",
    "    \n",
    "    # Plot spectral pairplot for training data\n",
    "    df = data_pts_full_PS\n",
    "    df['Class'] = df['class'].astype(int)\n",
    "    df = df.sort_values(by='Class')\n",
    "    # Assign labels to each class\n",
    "    df.loc[df['Class']==1, 'Class'] = 'Snow'\n",
    "    df.loc[df['Class']==2, 'Class'] = 'Snow'\n",
    "    df.loc[df['Class']==3, 'Class'] = 'Firn'\n",
    "    df.loc[df['Class']==4, 'Class'] = 'Ice'\n",
    "    df.loc[df['Class']==5, 'Class'] = 'Bare rock'\n",
    "    df.loc[df['Class']==6, 'Class'] = 'Water'\n",
    "    df[feature_cols_PS] = df[feature_cols_PS].astype(object)\n",
    "    # Create colormap\n",
    "    color_snow = '#4eb3d3'\n",
    "    color_firn = '#756bb1'\n",
    "    color_ice = '#084081'\n",
    "    color_rock = '#fdbb84'\n",
    "    color_water = '#bdbdbd'\n",
    "    colors = [color_snow, color_firn, color_ice, color_rock, color_water]\n",
    "    # plot\n",
    "    fig = sns.pairplot(df, vars=df.columns[4:-1], corner=True, diag_kind='kde', hue='Class', palette=colors)\n",
    "    plt.show()\n",
    "    # save figure\n",
    "    if save_figures:\n",
    "        fig_fn = base_path + 'figures/spectral_pairplot_PS_training_data.png'\n",
    "        fig.savefig(fig_fn, facecolor='w', dpi=300)\n",
    "        print('figure saved to file:' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a40773-9621-4d11-b882-52a728a7b678",
   "metadata": {},
   "source": [
    "### b. Landsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bfa40d-72f1-411c-a9e2-4bb00045eb5b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = 'Landsat'\n",
    "ds_dict = dataset_dict[dataset]\n",
    "\n",
    "# -----Define bands and feature columns (predictors used in classification)\n",
    "# Landsat 8 bands: \n",
    "# SR_B2=Blue, SR_B3=Green, SR_B4=Red, SR_B5=NIR, SR_B6=SWIR1, SR_B7=SWIR2\n",
    "band_names = ['SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7', 'QA_PIXEL']\n",
    "feature_cols_L = band_names[0:-1] + ['NDSI']\n",
    "# buffer used for clipping images\n",
    "buffer = 4000 # [m]\n",
    "\n",
    "# -----Load classified points\n",
    "os.chdir(data_pts_path)\n",
    "data_pts_fns = glob.glob('*.shp')\n",
    "data_pts_fns.sort()\n",
    "\n",
    "# -----Check if training data exist in file\n",
    "L_training_data_fn = 'L_training_data.pkl'\n",
    "if os.path.exists(out_path + L_training_data_fn):\n",
    "    \n",
    "    data_pts_full_L = pd.read_pickle(out_path + L_training_data_fn)\n",
    "    print('Landsat training data already exist... loaded from file.')\n",
    "    \n",
    "else: \n",
    "    \n",
    "    # Initialize full data points dataframe (for use in next step)\n",
    "    data_pts_full_L = data_pts_full_PS.copy(deep=True)\n",
    "    # remove PS bands\n",
    "    data_pts_full_L = data_pts_full_L.drop(columns=['blue', 'green', 'red', 'NIR', 'NDSI'])\n",
    "    # initialize band columns\n",
    "    data_pts_full_L['L_im_date'] = ' '\n",
    "    data_pts_full_L[feature_cols_L] = 0\n",
    "\n",
    "    # Loop through sites\n",
    "    for i, site_name in enumerate(site_names):\n",
    "\n",
    "        print('----------')\n",
    "        print(site_name)\n",
    "        print('----------')\n",
    "\n",
    "        # Extract image dates from data point file names\n",
    "        im_dates = [s[len(site_name)+1:len(site_name)+9] for s in data_pts_fns if (site_name in s) and ('snow.shp' in s)]\n",
    "\n",
    "        # Load AOI\n",
    "        AOI_fn = AOI_path + site_name + '/glacier_outlines/' + site_name + '_USGS_*.shp'\n",
    "        AOI_fn = glob.glob(AOI_fn)[0]\n",
    "        AOI = gpd.read_file(AOI_fn)\n",
    "        # reproject AOI to WGS 84 for compatibility with images\n",
    "        AOI_WGS = AOI.to_crs(4326)\n",
    "        # reformat AOI_WGS bounding box as ee.Geometry for clipping images\n",
    "        AOI_WGS_bb_ee = ee.Geometry.Polygon(\n",
    "                                [[[AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.miny[0]],\n",
    "                                  [AOI_WGS.geometry.bounds.maxx[0], AOI_WGS.geometry.bounds.miny[0]],\n",
    "                                  [AOI_WGS.geometry.bounds.maxx[0], AOI_WGS.geometry.bounds.maxy[0]],\n",
    "                                  [AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.maxy[0]],\n",
    "                                  [AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.miny[0]]]\n",
    "                                ])\n",
    "\n",
    "        # Load images from Earth Engine\n",
    "        if site_name=='Gulkana':\n",
    "            im1_fn = 'LANDSAT/LC08/C02/T1_L2/LC08_067016_20210610'\n",
    "            im2_fn = 'LANDSAT/LC08/C02/T1_L2/LC08_068016_20210804'\n",
    "        elif site_name=='SouthCascade':\n",
    "            im1_fn= 'LANDSAT/LC08/C02/T1_L2/LC08_046026_20210709'\n",
    "            im2_fn = 'LANDSAT/LC08/C02/T1_L2/LC08_045026_20210819'\n",
    "        elif site_name=='Sperry':\n",
    "            im1_fn = 'LANDSAT/LC08/C02/T1_L2/LC08_041026_20210706'\n",
    "            im2_fn = 'LANDSAT/LC08/C02/T1_L2/LC08_041026_20210722'\n",
    "        elif site_name=='Wolverine':\n",
    "            im1_fn = 'LANDSAT/LC08/C02/T1_L2/LC08_067018_20220629'\n",
    "            im2_fn = 'LANDSAT/LC08/C02/T1_L2/LC08_067018_20210829'\n",
    "        im1, im2 = ee.Image(im1_fn), ee.Image(im2_fn)\n",
    "\n",
    "        # Clip images and select bands\n",
    "        im1_clip = im1.clip(AOI_WGS_bb_ee.buffer(buffer)).select(band_names)\n",
    "        im2_clip = im2.clip(AOI_WGS_bb_ee.buffer(buffer)).select(band_names)\n",
    "\n",
    "        # Determine optimal UTM zone EPSG code\n",
    "        epsg_UTM = f.convert_wgs_to_utm((AOI_WGS.geometry.bounds.maxx[0] - AOI_WGS.geometry.bounds.minx[0]) + AOI_WGS.geometry.bounds.minx[0],\n",
    "                                         (AOI_WGS.geometry.bounds.maxy[0] - AOI_WGS.geometry.bounds.miny[0]) + AOI_WGS.geometry.bounds.miny[0])\n",
    "\n",
    "        # Convert images to xarray Datasets\n",
    "        im1_xr = im1_clip.wx.to_xarray(scale=20, crs='EPSG:4326')\n",
    "        im2_xr = im2_clip.wx.to_xarray(scale=20, crs='EPSG:4326')\n",
    "        # Reproject to UTM\n",
    "        im1_xr = im1_xr.rio.reproject('EPSG:'+epsg_UTM)\n",
    "        im2_xr = im2_xr.rio.reproject('EPSG:'+epsg_UTM)\n",
    "        # Replace no data values with NaN and account for image scalar\n",
    "        im1_xr = xr.where(im1_xr!=ds_dict['no_data_value'],\n",
    "                          im1_xr / ds_dict['SR_scalar'],\n",
    "                          np.nan)\n",
    "        im2_xr = xr.where(im2_xr!=ds_dict['no_data_value'],\n",
    "                          im2_xr / ds_dict['SR_scalar'],\n",
    "                          np.nan)\n",
    "        # Create list of images\n",
    "        im_list = [im1_xr, im2_xr]\n",
    "\n",
    "        # Loop through image dates\n",
    "        for j, im_date in enumerate(im_dates):\n",
    "\n",
    "            im = im_list[j]\n",
    "\n",
    "            # mask clouds\n",
    "            bands = [str(x) for x in im.data_vars]\n",
    "            im_mask = f.Landsat_mask_clouds(im, bands, plot_results=False)\n",
    "\n",
    "            # select df columns for study site and image date\n",
    "            data_pts = data_pts_full_L.loc[(data_pts_full_L['site_name']==site_name) \n",
    "                                            & (data_pts_full_L['PS_im_date']==im_date[0:4]+'-'+im_date[4:6]+'-'+im_date[6:8])]\n",
    "            # add image date\n",
    "            data_pts['L_im_date'] = str(im.time.data[0])[0:10]\n",
    "            # reproject to UTM\n",
    "            data_pts = data_pts.to_crs(epsg_UTM)\n",
    "            \n",
    "            # grab x and y coordinates for data points at the site\n",
    "            data_pts_x = [data_pts['geometry'].reset_index(drop=True)[i].geoms[0].x\n",
    "                          for i in np.arange(0,len(data_pts))]\n",
    "            data_pts_y = [data_pts['geometry'].reset_index(drop=True)[i].geoms[0].y\n",
    "                          for i in np.arange(0,len(data_pts))]\n",
    "            # extract band values at data points \n",
    "            for band_name in band_names:\n",
    "                data_pts[band_name] = [im_mask.sel(x=x, y=y, method=\"nearest\")[band_name].data[0] \n",
    "                                       for x, y in list(zip(data_pts_x, data_pts_y))]\n",
    "\n",
    "            # add data_pts back to full df\n",
    "            data_pts_full_L.loc[(data_pts_full_L['site_name']==site_name) \n",
    "                                 & (data_pts_full_L['PS_im_date']==im_date[0:4]+'-'+im_date[4:6]+'-'+im_date[6:8])] = data_pts\n",
    "\n",
    "            # plot images and data points\n",
    "            fig1, ax1 = plt.subplots(1, 1, figsize=(16,16))\n",
    "            ax1.imshow(np.dstack([im_mask['SR_B4'].data[0], im_mask['SR_B3'].data[0], im_mask['SR_B2'].data[0]]),\n",
    "                      extent=(np.min(im_mask.x.data)/1e3, np.max(im_mask.x.data)/1e3, \n",
    "                              np.min(im_mask.y.data)/1e3, np.max(im_mask.y.data)/1e3))\n",
    "            ax1.scatter([x.geoms[0].x/1e3 for x in data_pts['geometry'].loc[data_pts['class']==1]], \n",
    "                        [x.geoms[0].y/1e3 for x in data_pts['geometry'].loc[data_pts['class']==1]], c='cyan', s=1)\n",
    "            ax1.scatter([x.geoms[0].x/1e3 for x in data_pts['geometry'].loc[data_pts['class']==3]], \n",
    "                        [x.geoms[0].y/1e3 for x in data_pts['geometry'].loc[data_pts['class']==3]], c='blue', s=1)\n",
    "            ax1.scatter([x.geoms[0].x/1e3 for x in data_pts['geometry'].loc[data_pts['class']==4]], \n",
    "                        [x.geoms[0].y/1e3 for x in data_pts['geometry'].loc[data_pts['class']==4]], c='orange', s=1)\n",
    "            ax1.scatter([x.geoms[0].x/1e3 for x in data_pts['geometry'].loc[data_pts['class']==5]], \n",
    "                        [x.geoms[0].y/1e3 for x in data_pts['geometry'].loc[data_pts['class']==5]], c='grey', s=1)\n",
    "            ax1.set_xlabel('Easting [km]')\n",
    "            ax1.set_ylabel('Northing [km]')\n",
    "            plt.show()\n",
    "\n",
    "    # Remove no data points\n",
    "    data_pts_full_L = data_pts_full_L.dropna().reset_index(drop=True)\n",
    "\n",
    "    # Add NDSI column (G-SWIR)/(G+SWIR)\n",
    "    NDSI_bands = dataset_dict[dataset]['NDSI']\n",
    "    data_pts_full_L['NDSI'] = ((data_pts_full_L[NDSI_bands[0]] - data_pts_full_L[NDSI_bands[1]]) \n",
    "                             / (data_pts_full_L[NDSI_bands[0]] + data_pts_full_L[NDSI_bands[1]]))\n",
    "\n",
    "    # don't use shadowed snow (where class==shadowed snow, make it snow)\n",
    "    data_pts_full_L.loc[data_pts_full_L['class']==2, 'class'] = 1\n",
    "                              \n",
    "    # Reduce memory usage in df\n",
    "    data_pts_full_L = f.reduce_memory_usage(data_pts_full_L)\n",
    "\n",
    "    # Save training data to file\n",
    "    data_pts_full_L.to_pickle(out_path + L_training_data_fn)\n",
    "    print('Landsat training data saved to file:' + out_path + L_training_data_fn)\n",
    "\n",
    "    # Save feature columns\n",
    "    feature_cols_fn = out_path + 'L_feature_cols.pkl'\n",
    "    pickle.dump(feature_cols_L, open(feature_cols_fn, 'wb'))\n",
    "    print('Feature columns saved to file: ', feature_cols_fn)\n",
    "    \n",
    "    # Plot spectral pairplot for training data\n",
    "    df = data_pts_full_L\n",
    "    df = df.sort_values(by='class')\n",
    "    df['Class'] = df['class'].astype(object)\n",
    "    # Assign labels to each class\n",
    "    df.loc[df['Class']==1, 'Class'] = 'Snow'\n",
    "    df.loc[df['Class']==2, 'Class'] = 'Snow'\n",
    "    df.loc[df['Class']==3, 'Class'] = 'Ice'\n",
    "    df.loc[df['Class']==4, 'Class'] = 'Bare rock'\n",
    "    df.loc[df['Class']==5, 'Class'] = 'Water'\n",
    "    # Create colormap\n",
    "    color_snow = '#4eb3d3'\n",
    "    color_ice = '#084081'\n",
    "    color_rock = '#fdbb84'\n",
    "    color_water = '#bdbdbd'\n",
    "    color_contour = '#f768a1'\n",
    "    colors = [color_snow, color_ice, color_rock, color_water]\n",
    "    # plot\n",
    "    fig = sns.pairplot(df[['Class'] + feature_cols_L], corner=True, diag_kind='kde', hue='Class', palette=colors)\n",
    "    plt.show()\n",
    "    # save figure\n",
    "    if save_figures:\n",
    "        fig_fn = base_path + 'figures/spectral_pairplot_L_training_data.png'\n",
    "        fig.savefig(fig_fn, facecolor='w', dpi=300)\n",
    "        print('figure saved to file:' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8da88b-822c-4e10-87e8-133f78872be3",
   "metadata": {},
   "source": [
    "### c. Sentinel-2 SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674dde0a-cd6e-4f62-aede-7706b8595f18",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset='Sentinel2_SR'\n",
    "ds_dict = dataset_dict[dataset]\n",
    "\n",
    "# -----Define bands and feature columns (predictors used in classification)\n",
    "# Sentinel-2 bands: \n",
    "# B2=Blue, B3=Green, B4=Red, B5=Red Edge 1, B6=Red Edge 2, B7=Red Edge 3, B8=NIR, \n",
    "# B8A=Red Edge 4, B9=Water vapor, B11=SWIR1, B12=SWIR2\n",
    "band_names = [band for band in ds_dict['bands'] if 'QA' not in band]\n",
    "feature_cols_S2 = band_names + ['NDSI']\n",
    "# buffer used for clipping images\n",
    "buffer = [1000, 3000, 1000, 2500] # [m]\n",
    "\n",
    "# -----Check if training data exist in file\n",
    "S2_SR_training_data_fn = 'S2_SR_training_data.pkl'\n",
    "if os.path.exists(out_path + S2_SR_training_data_fn):\n",
    "    \n",
    "    data_pts_full_S2_SR = pd.read_pickle(out_path + S2_SR_training_data_fn)\n",
    "    print('S2 SR training data already exist... loaded from file.')\n",
    "    \n",
    "else: \n",
    "    \n",
    "    # Initialize full data points dataframe (for use in next step)\n",
    "    data_pts_full_S2_SR = data_pts_full_PS.copy(deep=True)\n",
    "    data_pts_full_S2_SR = data_pts_full_S2_SR.drop(columns=['blue', 'red', 'green', 'NIR', 'NDSI'])\n",
    "    data_pts_full_S2_SR[band_names] = \" \" # initialize band columns\n",
    "\n",
    "    # Loop through sites\n",
    "    for i, site_name in enumerate(site_names):\n",
    "    \n",
    "        print('----------')\n",
    "        print(site_name)\n",
    "        print('----------')\n",
    "\n",
    "        # Extract image dates from data point file names\n",
    "        im_dates = [s[len(site_name)+1:len(site_name)+9] for s in data_pts_fns if (site_name in s) and ('snow.shp' in s)]\n",
    "\n",
    "        # Load AOI\n",
    "        AOI_fn = AOI_path + site_name + '/glacier_outlines/' + site_name + '_USGS_*.shp'\n",
    "        AOI_fn = glob.glob(AOI_fn)[0]\n",
    "        AOI = gpd.read_file(AOI_fn)\n",
    "        # reproject AOI to WGS 84 for compatibility with images\n",
    "        AOI_WGS = AOI.to_crs(4326)\n",
    "        # reformat AOI_WGS bounding box as ee.Geometry for clipping images\n",
    "        AOI_WGS_bb_ee = ee.Geometry.Polygon(\n",
    "                                [[[AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.miny[0]],\n",
    "                                  [AOI_WGS.geometry.bounds.maxx[0], AOI_WGS.geometry.bounds.miny[0]],\n",
    "                                  [AOI_WGS.geometry.bounds.maxx[0], AOI_WGS.geometry.bounds.maxy[0]],\n",
    "                                  [AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.maxy[0]],\n",
    "                                  [AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.miny[0]]]\n",
    "                                ])\n",
    "\n",
    "        # Load images from Earth Engine\n",
    "        if site_name=='Gulkana':\n",
    "            im1_fn = '20210615T211519_20210615T211514_T06VWR'\n",
    "            im2_fn = '20210802T212531_20210802T212655_T06VWR'\n",
    "        elif site_name=='SouthCascade':\n",
    "            im1_fn= '20210704T190921_20210704T191755_T10UFU'\n",
    "            im2_fn = '20210825T185919_20210825T190431_T10UFU'\n",
    "        elif site_name=='Sperry':\n",
    "            im1_fn = '20210630T183919_20210630T184143_T11UQP'\n",
    "            im2_fn = '20210730T183919_20210730T184810_T11UQP'\n",
    "        elif site_name=='Wolverine':\n",
    "            im1_fn = '20210615T211519_20210615T211514_T06VUN'\n",
    "            im2_fn = '20210829T211521_20210829T211714_T06VUN'\n",
    "        im1, im2 = ee.Image('COPERNICUS/S2_SR_HARMONIZED/'+im1_fn), ee.Image('COPERNICUS/S2_SR_HARMONIZED/'+im2_fn)\n",
    "        # Clip images and select bands\n",
    "        im1_clip = im1.clip(AOI_WGS_bb_ee.buffer(buffer[i])).select(band_names)\n",
    "        im2_clip = im2.clip(AOI_WGS_bb_ee.buffer(buffer[i])).select(band_names)\n",
    "        \n",
    "        # Convert images to xarray Datasets\n",
    "        im1_xr = im1_clip.wx.to_xarray(scale=20, crs='EPSG:4326')\n",
    "        im2_xr = im2_clip.wx.to_xarray(scale=20, crs='EPSG:4326')\n",
    "        # Determine optimal UTM zone EPSG code\n",
    "        epsg_UTM = f.convert_wgs_to_utm((AOI_WGS.geometry.bounds.maxx[0] - AOI_WGS.geometry.bounds.minx[0]) + AOI_WGS.geometry.bounds.minx[0],\n",
    "                                         (AOI_WGS.geometry.bounds.maxy[0] - AOI_WGS.geometry.bounds.miny[0]) + AOI_WGS.geometry.bounds.miny[0])\n",
    "        # Reproject to UTM\n",
    "        im1_xr = im1_xr.rio.reproject('EPSG:'+epsg_UTM)\n",
    "        im2_xr = im2_xr.rio.reproject('EPSG:'+epsg_UTM)\n",
    "        # Replace no data values with NaN, account for image scalar\n",
    "        im1_xr = xr.where(im1_xr!=ds_dict['no_data_value'], \n",
    "                          im1_xr / ds_dict['SR_scalar'], \n",
    "                          np.nan)\n",
    "        im2_xr = xr.where(im2_xr!=ds_dict['no_data_value'], \n",
    "                          im2_xr / ds_dict['SR_scalar'], \n",
    "                          np.nan)\n",
    "        # Create list of images\n",
    "        im_list = [im1_xr, im2_xr]\n",
    "        \n",
    "        # Loop through image dates\n",
    "        for j, im_date in enumerate(im_dates):\n",
    "\n",
    "            im = im_list[j]\n",
    "            \n",
    "            # select df columns for study site and image date\n",
    "            data_pts = data_pts_full_S2_SR.loc[(data_pts_full_S2_SR['site_name']==site_name) \n",
    "                                            & (data_pts_full_S2_SR['PS_im_date']==im_date[0:4]+'-'+im_date[4:6]+'-'+im_date[6:8])]\n",
    "            # reproject to UTM\n",
    "            data_pts = data_pts.to_crs(epsg_UTM)\n",
    "            \n",
    "            # grab x and y coordinates for data points at the site\n",
    "            data_pts_x = [data_pts['geometry'].reset_index(drop=True)[i].geoms[0].x\n",
    "                          for i in np.arange(0,len(data_pts))]\n",
    "            data_pts_y = [data_pts['geometry'].reset_index(drop=True)[i].geoms[0].y\n",
    "                          for i in np.arange(0,len(data_pts))]\n",
    "            \n",
    "            # extract band values at data points \n",
    "            for band_name in band_names:\n",
    "                data_pts[band_name] = [im.sel(x=x, y=y, method=\"nearest\")[band_name].data[0] \n",
    "                                       for x, y in list(zip(data_pts_x, data_pts_y))]\n",
    "\n",
    "            # plot images and data points\n",
    "            fig1, ax1 = plt.subplots(1, 1, figsize=(6,6))\n",
    "            ax1.imshow(np.dstack([im['B4'].data[0], im['B3'].data[0], im['B2'].data[0]]),\n",
    "                      extent=(np.min(im.x.data)/1e3, np.max(im.x.data)/1e3, \n",
    "                              np.min(im.y.data)/1e3, np.max(im.y.data)/1e3))\n",
    "            # ax1.scatter([x.geoms[0].x/1e3 for x in data_pts['geometry'].loc[data_pts['class']==1]], \n",
    "            #             [x.geoms[0].y/1e3 for x in data_pts['geometry'].loc[data_pts['class']==1]], c='cyan', s=2)\n",
    "            ax1.plot([x/1e3 for x in data_pts_x], [y/1e3 for y in data_pts_y], '.m', markersize=5)\n",
    "            ax1.set_xlabel('Easting [km]')\n",
    "            ax1.set_ylabel('Northing [km]')\n",
    "            plt.show()\n",
    "\n",
    "            # add data_pts back to full df\n",
    "            data_pts_full_S2_SR.loc[(data_pts_full_S2_SR['site_name']==site_name) \n",
    "                                 & (data_pts_full_S2_SR['PS_im_date']==im_date[0:4]+'-'+im_date[4:6]+'-'+im_date[6:8])] = data_pts\n",
    "\n",
    "    # Add NDSI column\n",
    "    data_pts_full_S2_SR['NDSI'] = ((data_pts_full_S2_SR[ds_dict['NDSI'][0]] - data_pts_full_S2_SR[ds_dict['NDSI'][1]]) / \n",
    "                                (data_pts_full_S2_SR[ds_dict['NDSI'][0]] + data_pts_full_S2_SR[ds_dict['NDSI'][1]]))\n",
    "    \n",
    "    # Remove no data points\n",
    "    data_pts_full_S2_SR = data_pts_full_S2_SR.dropna().reset_index(drop=True)\n",
    "    \n",
    "    # don't use shadowed snow (where class==shadowed snow, make it snow)\n",
    "    data_pts_full_S2_SR.loc[data_pts_full_S2_SR['class']==2, 'class'] = 1\n",
    "    \n",
    "    # Reduce memory usage in df\n",
    "    data_pts_full_S2_SR = f.reduce_memory_usage(data_pts_full_S2_SR)\n",
    "    \n",
    "    # Save training data to file\n",
    "    data_pts_full_S2_SR.to_pickle(out_path + S2_SR_training_data_fn)\n",
    "    print('S2 SR training data saved to file:' + out_path + S2_SR_training_data_fn)\n",
    "    \n",
    "    # Save feature columns\n",
    "    feature_cols_fn = out_path + 'S2_SR_feature_cols.pkl'\n",
    "    pickle.dump(feature_cols_S2, open(feature_cols_fn, 'wb'))\n",
    "    print('Feature columns saved to file: ', feature_cols_fn)\n",
    "    \n",
    "    # Plot spectral pairplot for training data\n",
    "    df = data_pts_full_S2_SR\n",
    "    df = df.sort_values(by='class')\n",
    "    df['Class'] = df['class'].astype(object)\n",
    "    # Assign labels to each class\n",
    "    df.loc[df['Class']==1, 'Class'] = 'Snow'\n",
    "    df.loc[df['Class']==2, 'Class'] = 'Snow'\n",
    "    df.loc[df['Class']==3, 'Class'] = 'Ice'\n",
    "    df.loc[df['Class']==4, 'Class'] = 'Bare rock'\n",
    "    df.loc[df['Class']==5, 'Class'] = 'Water'\n",
    "    # Create colormap\n",
    "    color_snow = '#4eb3d3'\n",
    "    color_ice = '#084081'\n",
    "    color_rock = '#fdbb84'\n",
    "    color_water = '#bdbdbd'\n",
    "    color_contour = '#f768a1'\n",
    "    colors = [color_snow, color_ice, color_rock, color_water]\n",
    "    # plot\n",
    "    fig = sns.pairplot(df[['Class'] + feature_cols_S2], corner=True, diag_kind='kde', hue='Class', palette=colors)\n",
    "    plt.show()\n",
    "    # save figure\n",
    "    if save_figures:\n",
    "        fig_fn = base_path + 'figures/spectral_pairplot_S2_SR_training_data.png'\n",
    "        fig.savefig(fig_fn, facecolor='w', dpi=300)\n",
    "        print('figure saved to file:' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a423df3-ffee-4dee-b7be-1dfda487f2fb",
   "metadata": {},
   "source": [
    "### d. Sentinel-2 TOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f75b6a5-21a9-417a-9ebf-14640851d872",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset='Sentinel2-TOA'\n",
    "ds_dict = dataset_dict['Sentinel2_TOA']\n",
    "\n",
    "# -----Define bands and feature columns (predictors used in classification)\n",
    "# Sentinel-2 bands: \n",
    "# B2=Blue, B3=Green, B4=Red, B5=Red Edge 1, B6=Red Edge 2, B7=Red Edge 3, B8=NIR, \n",
    "# B8A=Red Edge 4, B9=Water vapor, B11=SWIR1, B12=SWIR2\n",
    "band_names = [band for band in ds_dict['bands'] if 'QA' not in band]\n",
    "feature_cols_S2_TOA = band_names + ['NDSI']\n",
    "# buffer used for clipping images\n",
    "buffer = [1000, 3000, 1000, 2500] # [m]\n",
    "\n",
    "# -----Check if training data exist in file\n",
    "S2_TOA_training_data_fn = 'S2_TOA_training_data.pkl'\n",
    "if os.path.exists(out_path + S2_TOA_training_data_fn):\n",
    "    \n",
    "    data_pts_full_S2_TOA = pd.read_pickle(out_path + S2_TOA_training_data_fn)\n",
    "    print('S2-TOA training data already exist... loaded from file.')\n",
    "    \n",
    "else: \n",
    "    \n",
    "    # Initialize full data points dataframe (for use in next step)\n",
    "    data_pts_full_S2_TOA = data_pts_full_PS.copy(deep=True)\n",
    "    data_pts_full_S2_TOA = data_pts_full_S2_TOA.drop(columns=['blue', 'red', 'green', 'NIR', 'NDSI'])\n",
    "    data_pts_full_S2_TOA[band_names] = \" \" # initialize band columns\n",
    "\n",
    "    # Loop through sites\n",
    "    for i, site_name in enumerate(site_names):\n",
    "    \n",
    "        print('----------')\n",
    "        print(site_name)\n",
    "        print('----------')\n",
    "\n",
    "        # Extract image dates from data point file names\n",
    "        im_dates = [s[len(site_name)+1:len(site_name)+9] for s in data_pts_fns if (site_name in s) and ('snow.shp' in s)]\n",
    "\n",
    "        # Load AOI\n",
    "        AOI_fn = AOI_path + site_name + '/glacier_outlines/' + site_name + '_USGS_*.shp'\n",
    "        AOI_fn = glob.glob(AOI_fn)[0]\n",
    "        AOI = gpd.read_file(AOI_fn)\n",
    "        # reproject AOI to WGS 84 for compatibility with images\n",
    "        AOI_WGS = AOI.to_crs(4326)\n",
    "        # reformat AOI_WGS bounding box as ee.Geometry for clipping images\n",
    "        AOI_WGS_bb_ee = ee.Geometry.Polygon(\n",
    "                                [[[AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.miny[0]],\n",
    "                                  [AOI_WGS.geometry.bounds.maxx[0], AOI_WGS.geometry.bounds.miny[0]],\n",
    "                                  [AOI_WGS.geometry.bounds.maxx[0], AOI_WGS.geometry.bounds.maxy[0]],\n",
    "                                  [AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.maxy[0]],\n",
    "                                  [AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.miny[0]]]\n",
    "                                ])\n",
    "\n",
    "        # Load images from Earth Engine\n",
    "        if site_name=='Gulkana':\n",
    "            im1_fn = '20210615T211519_20210615T211514_T06VWR'\n",
    "            im2_fn = '20210802T212531_20210802T212655_T06VWR'\n",
    "        elif site_name=='SouthCascade':\n",
    "            im1_fn= '20210704T190921_20210704T191755_T10UFU'\n",
    "            im2_fn = '20210825T185919_20210825T190431_T10UFU'\n",
    "        elif site_name=='Sperry':\n",
    "            im1_fn = '20210630T183919_20210630T184143_T11UQP'\n",
    "            im2_fn = '20210730T183919_20210730T184810_T11UQP'\n",
    "        elif site_name=='Wolverine':\n",
    "            im1_fn = '20210615T211519_20210615T211514_T06VUN'\n",
    "            im2_fn = '20210829T211521_20210829T211714_T06VUN'\n",
    "        im1, im2 = ee.Image('COPERNICUS/S2_HARMONIZED/'+im1_fn), ee.Image('COPERNICUS/S2_HARMONIZED/'+im2_fn)\n",
    "        # Clip images and select bands\n",
    "        im1_clip = im1.clip(AOI_WGS_bb_ee.buffer(buffer[i])).select(band_names)\n",
    "        im2_clip = im2.clip(AOI_WGS_bb_ee.buffer(buffer[i])).select(band_names)\n",
    "        \n",
    "        # Convert images to xarray Datasets\n",
    "        im1_xr = im1_clip.wx.to_xarray(scale=20, crs='EPSG:4326')\n",
    "        im2_xr = im2_clip.wx.to_xarray(scale=20, crs='EPSG:4326')\n",
    "        # Determine optimal UTM zone EPSG code\n",
    "        epsg_UTM = f.convert_wgs_to_utm((AOI_WGS.geometry.bounds.maxx[0] - AOI_WGS.geometry.bounds.minx[0]) + AOI_WGS.geometry.bounds.minx[0],\n",
    "                                         (AOI_WGS.geometry.bounds.maxy[0] - AOI_WGS.geometry.bounds.miny[0]) + AOI_WGS.geometry.bounds.miny[0])\n",
    "        # Reproject to UTM\n",
    "        im1_xr = im1_xr.rio.reproject('EPSG:'+epsg_UTM)\n",
    "        im2_xr = im2_xr.rio.reproject('EPSG:'+epsg_UTM)\n",
    "        # Replace no data values with NaN, account for image scalar\n",
    "        im1_xr = xr.where(im1_xr!=ds_dict['no_data_value'], \n",
    "                          im1_xr / ds_dict['SR_scalar'], \n",
    "                          np.nan)\n",
    "        im2_xr = xr.where(im2_xr!=ds_dict['no_data_value'], \n",
    "                          im2_xr / ds_dict['SR_scalar'], \n",
    "                          np.nan)\n",
    "        # Create list of images\n",
    "        im_list = [im1_xr, im2_xr]\n",
    "        \n",
    "        # Loop through image dates\n",
    "        for j, im_date in enumerate(im_dates):\n",
    "\n",
    "            im = im_list[j]\n",
    "            \n",
    "            # select df columns for study site and image date\n",
    "            data_pts = data_pts_full_S2_TOA.loc[(data_pts_full_S2_TOA['site_name']==site_name) \n",
    "                                            & (data_pts_full_S2_TOA['PS_im_date']==im_date[0:4]+'-'+im_date[4:6]+'-'+im_date[6:8])]\n",
    "            # reproject to UTM\n",
    "            data_pts = data_pts.to_crs(epsg_UTM)\n",
    "            \n",
    "            # grab x and y coordinates for data points at the site\n",
    "            data_pts_x = [data_pts['geometry'].reset_index(drop=True)[i].geoms[0].x\n",
    "                          for i in np.arange(0,len(data_pts))]\n",
    "            data_pts_y = [data_pts['geometry'].reset_index(drop=True)[i].geoms[0].y\n",
    "                          for i in np.arange(0,len(data_pts))]\n",
    "            \n",
    "            # extract band values at data points \n",
    "            for band_name in band_names:\n",
    "                data_pts[band_name] = [im.sel(x=x, y=y, method=\"nearest\")[band_name].data[0] \n",
    "                                       for x, y in list(zip(data_pts_x, data_pts_y))]\n",
    "\n",
    "            # plot images and data points\n",
    "            fig1, ax1 = plt.subplots(1, 1, figsize=(6,6))\n",
    "            ax1.imshow(np.dstack([im['B4'].data[0], im['B3'].data[0], im['B2'].data[0]]),\n",
    "                      extent=(np.min(im.x.data)/1e3, np.max(im.x.data)/1e3, \n",
    "                              np.min(im.y.data)/1e3, np.max(im.y.data)/1e3))\n",
    "            # ax1.scatter([x.geoms[0].x/1e3 for x in data_pts['geometry'].loc[data_pts['class']==1]], \n",
    "            #             [x.geoms[0].y/1e3 for x in data_pts['geometry'].loc[data_pts['class']==1]], c='cyan', s=2)\n",
    "            ax1.plot([x/1e3 for x in data_pts_x], [y/1e3 for y in data_pts_y], '.m', markersize=5)\n",
    "            ax1.set_xlabel('Easting [km]')\n",
    "            ax1.set_ylabel('Northing [km]')\n",
    "            plt.show()\n",
    "\n",
    "            # add data_pts back to full df\n",
    "            data_pts_full_S2_TOA.loc[(data_pts_full_S2_TOA['site_name']==site_name) \n",
    "                                 & (data_pts_full_S2_TOA['PS_im_date']==im_date[0:4]+'-'+im_date[4:6]+'-'+im_date[6:8])] = data_pts\n",
    "\n",
    "    # Add NDSI column\n",
    "    data_pts_full_S2_TOA['NDSI'] = ((data_pts_full_S2_TOA[ds_dict['NDSI'][0]] - data_pts_full_S2_TOA[ds_dict['NDSI'][1]]) / \n",
    "                                (data_pts_full_S2_TOA[ds_dict['NDSI'][0]] + data_pts_full_S2_TOA[ds_dict['NDSI'][1]]))\n",
    "    \n",
    "    # Remove no data points\n",
    "    data_pts_full_S2_TOA = data_pts_full_S2_TOA.dropna().reset_index(drop=True)\n",
    "    \n",
    "    # don't use shadowed snow (where class==shadowed snow, make it snow)\n",
    "    data_pts_full_S2_TOA.loc[data_pts_full_S2_TOA['class']==2, 'class'] = 1\n",
    "    \n",
    "    # Reduce memory usage in df\n",
    "    data_pts_full_S2_TOA = f.reduce_memory_usage(data_pts_full_S2_TOA)\n",
    "    \n",
    "    # Save training data to file\n",
    "    data_pts_full_S2_TOA.to_pickle(out_path + S2_TOA_training_data_fn)\n",
    "    print('S2-TOA training data saved to file:' + out_path + S2_TOA_training_data_fn)\n",
    "    \n",
    "    # Save feature columns\n",
    "    feature_cols_fn = out_path + 'S2_TOA_feature_cols.pkl'\n",
    "    pickle.dump(feature_cols_S2_TOA, open(feature_cols_fn, 'wb'))\n",
    "    print('Feature columns saved to file: ', feature_cols_fn)\n",
    "    \n",
    "    # Plot spectral pairplot for training data\n",
    "    df = data_pts_full_S2_TOA\n",
    "    df = df.sort_values(by='class')\n",
    "    df['Class'] = df['class'].astype(object)\n",
    "    # Assign labels to each class\n",
    "    df.loc[df['Class']==1, 'Class'] = 'Snow'\n",
    "    df.loc[df['Class']==2, 'Class'] = 'Snow'\n",
    "    df.loc[df['Class']==3, 'Class'] = 'Ice'\n",
    "    df.loc[df['Class']==4, 'Class'] = 'Bare rock'\n",
    "    df.loc[df['Class']==5, 'Class'] = 'Water'\n",
    "    # Create colormap\n",
    "    color_snow = '#4eb3d3'\n",
    "    color_ice = '#084081'\n",
    "    color_rock = '#fdbb84'\n",
    "    color_water = '#bdbdbd'\n",
    "    color_contour = '#f768a1'\n",
    "    colors = [color_snow, color_ice, color_rock, color_water]\n",
    "    # plot\n",
    "    fig = sns.pairplot(df[['Class'] + feature_cols_S2_TOA], corner=True, diag_kind='kde', hue='Class', palette=colors)\n",
    "    plt.show()\n",
    "    # save figure\n",
    "    if save_figures:\n",
    "        fig_fn = base_path + 'figures/spectral_pairplot_S2_TOA_training_data.png'\n",
    "        fig.savefig(fig_fn, facecolor='w', dpi=300)\n",
    "        print('figure saved to file:' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6cce4b-1ecb-4f72-9219-6344f94f7618",
   "metadata": {},
   "source": [
    "### d. MODIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a086fd7b-3692-48d2-a3e0-8f0c84dec557",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset='MODIS'\n",
    "ds_dict = dataset_dict[dataset]\n",
    "\n",
    "# -----Define bands and feature columns (predictors used in classification)\n",
    "# MODIS bands: \n",
    "# sur_refl_b01=Red, sur_refl_b04=Green, sur_refl_b03=Blue, sur_refl_b05=SWIR1, \n",
    "# sur_refl_b06=SWIR2, sur_refl_b07=SWIR3\n",
    "band_names = [band for band in ds_dict['bands'] if 'qc' not in band]\n",
    "feature_cols_M = [band for band in band_names] + ['NDSI']\n",
    "# buffer used for clipping images\n",
    "buffer = 3000 # [m]\n",
    "\n",
    "# -----Check if training data exist in file\n",
    "M_training_data_fn = 'M_training_data.pkl'\n",
    "if os.path.exists(out_path + M_training_data_fn):\n",
    "    \n",
    "    data_pts_full_M = pd.read_pickle(out_path + M_training_data_fn)\n",
    "    print('MODIS training data already exist... loaded from file.')\n",
    "    \n",
    "else: \n",
    "    \n",
    "    # Initialize full data points dataframe (for use in next step)\n",
    "    data_pts_full_M = data_pts_full_PS.copy(deep=True)\n",
    "    data_pts_full_M = data_pts_full_M.drop(columns=['blue', 'red', 'green', 'NIR', 'NDSI'])\n",
    "    data_pts_full_M[band_names] = \" \" # initialize band columns\n",
    "\n",
    "    # Loop through sites\n",
    "    for i, site_name in enumerate(site_names):\n",
    "    \n",
    "        print('----------')\n",
    "        print(site_name)\n",
    "        print('----------')\n",
    "\n",
    "        # Extract image dates from data point file names\n",
    "        im_dates = [s[len(site_name)+1:len(site_name)+9] for s in data_pts_fns if (site_name in s) and ('snow.shp' in s)]\n",
    "\n",
    "        # Load AOI\n",
    "        AOI_fn = AOI_path + site_name + '/glacier_outlines/' + site_name + '_USGS_*.shp'\n",
    "        AOI_fn = glob.glob(AOI_fn)[0]\n",
    "        AOI = gpd.read_file(AOI_fn)\n",
    "        # reproject AOI to WGS 84 for compatibility with images\n",
    "        AOI_WGS = AOI.to_crs(4326)\n",
    "        # reformat AOI_WGS bounding box as ee.Geometry for clipping images\n",
    "        AOI_WGS_bb_ee = ee.Geometry.Polygon(\n",
    "                                [[[AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.miny[0]],\n",
    "                                  [AOI_WGS.geometry.bounds.maxx[0], AOI_WGS.geometry.bounds.miny[0]],\n",
    "                                  [AOI_WGS.geometry.bounds.maxx[0], AOI_WGS.geometry.bounds.maxy[0]],\n",
    "                                  [AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.maxy[0]],\n",
    "                                  [AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.miny[0]]]\n",
    "                                ])\n",
    "\n",
    "        # Load images from Earth Engine\n",
    "        if site_name=='Gulkana':\n",
    "            im1_fn = '2021_06_15'\n",
    "            im2_fn = '2021_08_06'\n",
    "        elif site_name=='SouthCascade':\n",
    "            im1_fn= '2021_07_03'\n",
    "            im2_fn = '2021_08_28'\n",
    "        elif site_name=='Sperry':\n",
    "            im1_fn = '2021_06_27'\n",
    "            im2_fn = '2021_08_01'\n",
    "        elif site_name=='Wolverine':\n",
    "            im1_fn = '2021_06_15'\n",
    "            im2_fn = '2021_08_15'\n",
    "        im1, im2 = ee.Image('MODIS/061/MOD09GA/'+im1_fn), ee.Image('MODIS/061/MOD09GA/'+im2_fn)\n",
    "    \n",
    "        # Clip images and select bands\n",
    "        im1_clip = im1.clip(AOI_WGS_bb_ee.buffer(buffer)).select(band_names)\n",
    "        im2_clip = im2.clip(AOI_WGS_bb_ee.buffer(buffer)).select(band_names)\n",
    "        # Convert images to xarray Datasets\n",
    "        im1_xr = im1_clip.wx.to_xarray(scale=ds_dict['resolution_m'], crs='EPSG:4326')\n",
    "        im2_xr = im2_clip.wx.to_xarray(scale=ds_dict['resolution_m'], crs='EPSG:4326')\n",
    "        # Determine optimal UTM zone EPSG code\n",
    "        epsg_UTM = f.convert_wgs_to_utm((AOI_WGS.geometry.bounds.maxx[0] - AOI_WGS.geometry.bounds.minx[0]) + AOI_WGS.geometry.bounds.minx[0],\n",
    "                                         (AOI_WGS.geometry.bounds.maxy[0] - AOI_WGS.geometry.bounds.miny[0]) + AOI_WGS.geometry.bounds.miny[0])\n",
    "        # Reproject to UTM\n",
    "        im1_xr = im1_xr.rio.reproject('EPSG:'+epsg_UTM)\n",
    "        im2_xr = im2_xr.rio.reproject('EPSG:'+epsg_UTM)\n",
    "        # Replace no data values with NaN, account for image scalar\n",
    "        im1_xr = xr.where((im1_xr!=ds_dict['no_data_value']) &  (im1_xr > 0), \n",
    "                          im1_xr / ds_dict['SR_scalar'], \n",
    "                          np.nan)\n",
    "        im2_xr = xr.where((im2_xr!=ds_dict['no_data_value']) & (im2_xr > 0), \n",
    "                          im2_xr / ds_dict['SR_scalar'], \n",
    "                          np.nan)\n",
    "        # Create list of images\n",
    "        im_list = [im1_xr, im2_xr]\n",
    "        \n",
    "        # Loop through image dates\n",
    "        for j, im_date in enumerate(im_dates):\n",
    "\n",
    "            im = im_list[j]\n",
    "            \n",
    "            # select df columns for study site and image date\n",
    "            data_pts = data_pts_full_M.loc[(data_pts_full_M['site_name']==site_name) \n",
    "                                            & (data_pts_full_M['PS_im_date']==im_date[0:4]+'-'+im_date[4:6]+'-'+im_date[6:8])]\n",
    "            # reproject to UTM\n",
    "            data_pts = data_pts.to_crs(epsg_UTM)\n",
    "            \n",
    "            # grab x and y coordinates for data points at the site\n",
    "            data_pts_x = [data_pts['geometry'].reset_index(drop=True)[i].geoms[0].x\n",
    "                          for i in np.arange(0,len(data_pts))]\n",
    "            data_pts_y = [data_pts['geometry'].reset_index(drop=True)[i].geoms[0].y\n",
    "                          for i in np.arange(0,len(data_pts))]\n",
    "            \n",
    "            # extract band values at data points \n",
    "            for band_name in band_names:\n",
    "                data_pts[band_name] = [im.sel(x=x, y=y, method=\"nearest\")[band_name].data[0] \n",
    "                                       for x, y in list(zip(data_pts_x, data_pts_y))]\n",
    "\n",
    "            # plot images and data points\n",
    "            fig1, ax1 = plt.subplots(1, 1, figsize=(6,6))\n",
    "            ax1.imshow(np.dstack([im[ds_dict['RGB_bands'][0]].data[0],\n",
    "                                  im[ds_dict['RGB_bands'][1]].data[0],\n",
    "                                  im[ds_dict['RGB_bands'][2]].data[0]]),\n",
    "                      extent=(np.min(im.x.data)/1e3, np.max(im.x.data)/1e3, \n",
    "                              np.min(im.y.data)/1e3, np.max(im.y.data)/1e3))\n",
    "            # ax1.scatter([x.geoms[0].x/1e3 for x in data_pts['geometry'].loc[data_pts['class']==1]], \n",
    "            #             [x.geoms[0].y/1e3 for x in data_pts['geometry'].loc[data_pts['class']==1]], c='cyan', s=2)\n",
    "            ax1.plot([x/1e3 for x in data_pts_x], [y/1e3 for y in data_pts_y], '.m', markersize=5)\n",
    "            ax1.set_xlabel('Easting [km]')\n",
    "            ax1.set_ylabel('Northing [km]')\n",
    "            plt.show()\n",
    "\n",
    "            # add data_pts back to full df\n",
    "            data_pts_full_M.loc[(data_pts_full_M['site_name']==site_name) \n",
    "                                 & (data_pts_full_M['PS_im_date']==im_date[0:4]+'-'+im_date[4:6]+'-'+im_date[6:8])] = data_pts\n",
    "\n",
    "    # Add NDSI column\n",
    "    data_pts_full_M['NDSI'] = ((data_pts_full_M[ds_dict['NDSI'][0]] - data_pts_full_M[ds_dict['NDSI'][1]]) / \n",
    "                                (data_pts_full_M[ds_dict['NDSI'][0]] + data_pts_full_M[ds_dict['NDSI'][1]]))\n",
    "    \n",
    "    # Remove no data points\n",
    "    data_pts_full_M = data_pts_full_M.dropna().reset_index(drop=True)\n",
    "    \n",
    "    # don't use shadowed snow (where class==shadowed snow, make it snow)\n",
    "    data_pts_full_M.loc[data_pts_full_M['class']==2, 'class'] = 1\n",
    "    \n",
    "    # Reduce memory usage in df\n",
    "    data_pts_full_M = f.reduce_memory_usage(data_pts_full_M)\n",
    "    \n",
    "    # Save training data to file\n",
    "    data_pts_full_M.to_pickle(out_path + M_training_data_fn)\n",
    "    print('MODIS training data saved to file:' + out_path + M_training_data_fn)\n",
    "    \n",
    "    # Save feature columns\n",
    "    feature_cols_fn = out_path + 'M_feature_cols.pkl'\n",
    "    pickle.dump(feature_cols_M, open(feature_cols_fn, 'wb'))\n",
    "    print('Feature columns saved to file: ', feature_cols_fn)\n",
    "    \n",
    "    # Plot spectral pairplot for training data\n",
    "    df = data_pts_full_M\n",
    "    df = df.sort_values(by='class')\n",
    "    df['Class'] = df['class'].astype(object)\n",
    "    # Assign labels to each class\n",
    "    df.loc[df['Class']==1, 'Class'] = 'Snow'\n",
    "    df.loc[df['Class']==2, 'Class'] = 'Snow'\n",
    "    df.loc[df['Class']==3, 'Class'] = 'Ice'\n",
    "    df.loc[df['Class']==4, 'Class'] = 'Bare rock'\n",
    "    df.loc[df['Class']==5, 'Class'] = 'Water'\n",
    "    # Create colormap\n",
    "    color_snow = '#4eb3d3'\n",
    "    color_ice = '#084081'\n",
    "    color_rock = '#fdbb84'\n",
    "    color_water = '#bdbdbd'\n",
    "    color_contour = '#f768a1'\n",
    "    colors = [color_snow, color_ice, color_rock, color_water]\n",
    "    # plot\n",
    "    fig = sns.pairplot(df[['Class'] + feature_cols_M], corner=True, diag_kind='kde', hue='Class', palette=colors)\n",
    "    plt.show()\n",
    "    # save figure\n",
    "    if save_figures:\n",
    "        fig_fn = base_path + 'figures/spectral_pairplot_M_training_data.png'\n",
    "        fig.savefig(fig_fn, facecolor='w', dpi=300)\n",
    "        print('figure saved to file:' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bb2db6",
   "metadata": {},
   "source": [
    "## 2. Develop supervised classifiers for each dataset\n",
    "\n",
    "Use classified points at each site to determine the best classifiers for EACH site and for ALL sites using K-folds cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca691ea6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Define datasets to test\n",
    "datasets = ['PlanetScope']#, 'Landsat', 'Sentinel2_SR', 'Sentinel2_TOA']#, 'MODIS']\n",
    "\n",
    "# -----Loop through datasets\n",
    "for dataset in datasets:\n",
    "    \n",
    "    print('----------')\n",
    "    print(dataset)\n",
    "    print('----------')\n",
    "    \n",
    "    # Define variables and dataset prefix to use in file names based on dataset\n",
    "    if dataset=='PlanetScope':\n",
    "        dataset_prefix = 'PS'\n",
    "        data_pts_full = data_pts_full_PS\n",
    "        feature_cols = feature_cols_PS\n",
    "    elif dataset=='Landsat':\n",
    "        dataset_prefix = 'L'\n",
    "        data_pts_full = data_pts_full_L\n",
    "        feature_cols = feature_cols_L\n",
    "    elif dataset=='Sentinel2_SR':\n",
    "        dataset_prefix = 'S2_SR'\n",
    "        data_pts_full = data_pts_full_S2_SR\n",
    "        feature_cols = feature_cols_S2\n",
    "    elif dataset=='Sentinel2_TOA':\n",
    "        dataset_prefix = 'S2_TOA'\n",
    "        data_pts_full = data_pts_full_S2_TOA\n",
    "        feature_cols = feature_cols_S2\n",
    "    elif dataset=='MODIS':\n",
    "        dataset_prefix = 'M'\n",
    "        data_pts_full = data_pts_full_M\n",
    "        feature_cols = feature_cols_M\n",
    "        \n",
    "    # -----Test one classifier for each site\n",
    "    # print('Testing one classifier for EACH site...')\n",
    "    # for i in range(len(site_names)):\n",
    "    #     print(site_names[i])\n",
    "    #     # Select all columns in data_pts_full_PS for site\n",
    "    #     data_pts = data_pts_full.loc[data_pts_full['site_name']==site_names[i]]\n",
    "    #     data_pts = data_pts.dropna().reset_index(drop=True) # reset indixes\n",
    "    #     # Split data points into features (band values / terrain parameters) and target variable (class)\n",
    "    #     X = data_pts[feature_cols] # features\n",
    "    #     y = data_pts['class'].astype(int) # label\n",
    "    #     # Iterate over classifiers\n",
    "    #     accuracy = np.zeros(len(classifiers)) # mean accuracy\n",
    "    #     K = np.zeros(len(classifiers)) # mean Kappa score\n",
    "    #     j=0\n",
    "    #     for name, clf in zip(names, classifiers):\n",
    "    #         # Conduct K-Fold cross-validation\n",
    "    #         kfold = KFold(n_splits=num_folds, shuffle=True, random_state=1)\n",
    "#             accuracy_folds = np.zeros(num_folds) # accuracy for all simulations\n",
    "#             K_folds = np.zeros(num_folds) # kappa score for all MC simulations\n",
    "#             k=0 # iteration counter\n",
    "#             # enumerate the splits and summarize the distributions\n",
    "#             for train_ix, test_ix in kfold.split(X):\n",
    "#                 # select rows\n",
    "#                 X_train, X_test = X.loc[train_ix], X.loc[test_ix]\n",
    "#                 y_train, y_test = y[train_ix], y[test_ix]\n",
    "#                 # Train classifier\n",
    "#                 clf.fit(X_train, y_train)\n",
    "#                 # Predict class values using trained classifier\n",
    "#                 y_pred = clf.predict(X_test)\n",
    "#                 # Calculate overall accuracy\n",
    "#                 accuracy_folds[k] = metrics.accuracy_score(y_test, y_pred)\n",
    "#                 # Calculate Kappa score\n",
    "#                 K_folds[k] = metrics.cohen_kappa_score(y_test, y_pred)\n",
    "#                 k+=1\n",
    "\n",
    "#             # Calculate mean accuracy and Kappa score\n",
    "#             accuracy[j] = np.nanmean(accuracy_folds)\n",
    "#             K[j] = np.nanmean(K_folds)\n",
    "#             j+=1\n",
    "\n",
    "#         # Determine best classifier based on accuracy\n",
    "#         results = pd.DataFrame()\n",
    "#         results['Classifier'], results['Accuracy'], results['Kappa_score'] = names, accuracy, K\n",
    "#         clf_best_name = names[np.where(accuracy==np.max(accuracy))[0][0]]\n",
    "#         clf_best = classifiers[np.where(accuracy==np.max(accuracy))[0][0]]\n",
    "#         print(results)\n",
    "#         print('')\n",
    "#         print('Best accuracy classifier: ' + clf_best_name)\n",
    "\n",
    "#         # Save most accurate classifier\n",
    "#         if save_outputs==True:\n",
    "#             clf_fn = out_path + dataset_prefix + '_classifier_'+site_names[i]+'.sav'\n",
    "#             pickle.dump(clf_best, open(clf_fn, 'wb'))\n",
    "#             print('Most accurate classifier saved to file: ',clf_fn)\n",
    "            \n",
    "#         print(' ')\n",
    "        \n",
    "    # -----Test one classifier for all sites\n",
    "    print('Testing one classifier for ALL sites...')\n",
    "    # data_pts_full = data_pts_full.dropna()\n",
    "    X = data_pts_full[feature_cols].astype(float) # features\n",
    "    y = data_pts_full['class'].astype(int) # labels\n",
    "    \n",
    "    # Iterate over classifiers\n",
    "    num_folds = 10\n",
    "    accuracy = np.zeros(len(classifiers)) # mean accuracy\n",
    "    K = np.zeros(len(classifiers)) # mean Kappa score\n",
    "    CM = np.zeros((4, 4, len(classifiers))) # confusion matrix\n",
    "    j=0\n",
    "    for name, clf in zip(names, classifiers):\n",
    "\n",
    "        print(name)\n",
    "\n",
    "        # Conduct K-Fold cross-validation\n",
    "        kfold = KFold(n_splits=num_folds, shuffle=True, random_state=1)\n",
    "        accuracy_folds = np.zeros(num_folds) # accuracy for all simulations\n",
    "        K_folds = np.zeros(num_folds) # kappa score for all MC simulations\n",
    "        # CM_folds = np.zeros((4, 4, num_folds)) # confusion matrix for all folds\n",
    "        # enumerate the splits and summarize the distributions\n",
    "        k=0\n",
    "        for train_ix, test_ix in kfold.split(X):\n",
    "\n",
    "            # select rows\n",
    "            X_train, X_test = X.loc[train_ix], X.loc[test_ix]\n",
    "            y_train, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "            # Train classifier\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            # Predict class values using trained classifier\n",
    "            y_pred = clf.predict(X_test)\n",
    "\n",
    "            # Calculate overall accuracy\n",
    "            accuracy_folds[k] = metrics.accuracy_score(y_test, y_pred)\n",
    "            # Calculate Kappa score\n",
    "            K_folds[k] = metrics.cohen_kappa_score(y_test, y_pred)\n",
    "            # Calculate confusion matrix\n",
    "            # CM_folds[:, :, k] = metrics.confusion_matrix(y_test, y_pred)\n",
    "            \n",
    "            k+=1\n",
    "\n",
    "        # Calculate mean accuracy and Kappa score\n",
    "        accuracy[j] = np.nanmean(accuracy_folds)\n",
    "        K[j] = np.nanmean(K_folds)\n",
    "        # CM[:,:,j] = np.nansum(CM_folds, axis=2)\n",
    "        \n",
    "        # Determine feature importance using Random Forest model\n",
    "        if name==\"Random Forest\":\n",
    "            importances = clf.feature_importances_\n",
    "            # print('    Feature importances:')\n",
    "            # [print('   ', x, y) for x, y in zip(feature_cols, importances)]\n",
    "            fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
    "            bars = ax.bar(np.arange(len(feature_cols)), importances)\n",
    "            ax.bar_label(bars)\n",
    "            ax.set_xticks(np.arange(len(feature_cols)))\n",
    "            ax.set_xticklabels(feature_cols)\n",
    "            ax.set_xlabel('Features')\n",
    "            ax.set_ylabel('Importance')\n",
    "            ax.set_title(dataset+'Random Forest model feature importances')\n",
    "            plt.show()\n",
    "            fig.savefig(figures_out_path + 'RandomForest_feature_importances_'+dataset_prefix+'.png', \n",
    "                     dpi=300, facecolor='w')\n",
    "            print('    figure saved to file')\n",
    "        j+=1\n",
    "\n",
    "    # -----Determine best classifier based on accuracy\n",
    "    results = pd.DataFrame()\n",
    "    results['Classifier'], results['Accuracy'], results['Kappa_score'] = names, accuracy, K\n",
    "    clf_best_name = names[np.where(K==np.max(K))[0][0]]\n",
    "    clf_best = classifiers[np.where(K==np.max(K))[0][0]]\n",
    "    print(results)\n",
    "    print('')\n",
    "    print('Best accuracy classifier: ' + clf_best_name)\n",
    "\n",
    "    # -----Save most accurate classifier\n",
    "    if save_outputs==True:\n",
    "        clf_fn = out_path + dataset_prefix + '_classifier_all_sites.sav'\n",
    "        pickle.dump(clf_best, open(clf_fn, 'wb'))\n",
    "        print('Most accurate classifier saved to file: ',clf_fn)\n",
    "    \n",
    "    # -----Print confusion matrix\n",
    "    # CM_copy = pd.DataFrame(columns=['Snow', 'Shadowed_snow', 'Ice', 'Bare_ground', 'Water', 'All_snow'])\n",
    "    # CM_copy['Snow'] = CM[:,0,0]\n",
    "    # CM_copy['Shadowed_snow'] = CM[:,1,0]\n",
    "    # CM_copy['Ice'] = CM[:,2,0]\n",
    "    # CM_copy['Bare_ground'] = CM[:,3,0]\n",
    "    # CM_copy['Water'] = CM[:,4,0]\n",
    "    # CM_copy['All_snow'] = CM_copy['Snow'] + CM_copy['Shadowed_snow']\n",
    "    # CM_copy = CM_copy.drop(columns=['Snow', 'Shadowed_snow'])\n",
    "    # print(CM_copy)\n",
    "    \n",
    "    # -----Plot spectral pairplot for model predictions\n",
    "    # df = data_pts_full\n",
    "    # df = df.sort_values(by='class')\n",
    "    # # predict class for each training point\n",
    "    # df['Predicted Class'] = clf_best.predict(X).astype(object)\n",
    "    # # Assign labels to each class\n",
    "    # df.loc[df['Predicted Class']==1, 'Predicted Class'] = 'Snow'\n",
    "    # df.loc[df['Predicted Class']==2, 'Predicted Class'] = 'Snow'\n",
    "    # df.loc[df['Predicted Class']==3, 'Predicted Class'] = 'Ice'\n",
    "    # df.loc[df['Predicted Class']==4, 'Predicted Class'] = 'Bare rock'\n",
    "    # df.loc[df['Predicted Class']==5, 'Predicted Class'] = 'Water'\n",
    "    # # Create colormap\n",
    "    # color_snow = '#4eb3d3'\n",
    "    # color_ice = '#084081'\n",
    "    # color_rock = '#fdbb84'\n",
    "    # color_water = '#bdbdbd'\n",
    "    # color_contour = '#f768a1'\n",
    "    # colors = [color_snow, color_ice, color_rock, color_water]\n",
    "    # # plot\n",
    "    # fig = sns.pairplot(df[['Class'] + feature_cols], corner=True, diag_kind='kde', hue='Class', palette=colors)\n",
    "    # plt.show()\n",
    "    # # save figure\n",
    "    # if save_figures:\n",
    "    #     fig_fn = base_path + 'figures/spectral_pairplot_' + dataset_prefix + '_model_predictions.png'\n",
    "    #     fig.savefig(fig_fn, facecolor='w', dpi=300)\n",
    "    #     print('figure saved to file:' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e90386-11a5-466f-9f4e-64fc9d341d89",
   "metadata": {},
   "source": [
    "## 3. *Optional*\n",
    "\n",
    "### a. Test how the number of points used to train impacts model accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e317f484-f1bd-4b57-8b5b-b8df8adef102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Test supervised classification algorithms\n",
    "X = data_pts_full_PS[feature_cols_PS] # features\n",
    "y = data_pts_full_PS['class'].astype(int) # labels\n",
    "\n",
    "# Iterate over classifiers\n",
    "accuracy = np.zeros(len(classifiers)) # mean accuracy\n",
    "K = np.zeros(len(classifiers)) # mean Kappa score\n",
    "num_points_list = np.array([np.round(len(X)*x) for x in np.linspace(0.1, 1.0, num=20)], dtype=int)\n",
    "j=0\n",
    "for name, clf in zip(names, classifiers):\n",
    "    \n",
    "    print(name)\n",
    "\n",
    "    # initialize vectors\n",
    "    accuracies = np.zeros(len(num_points_list)) # accuracy for all simulations\n",
    "    Ks = np.zeros(len(num_points_list)) # kappa score for all MC simulations\n",
    "    # enumerate the splits and summarize the distributions\n",
    "    for k, num_points in enumerate(num_points_list):\n",
    "\n",
    "        # select random rows \n",
    "        I = np.random.randint(low=1, high=len(X), size=num_points, dtype=int)\n",
    "        Isplit = int(np.round(len(I)*0.8)) # index where to split training and testing\n",
    "        I_train, I_test = I[0:Isplit], I[Isplit:] # indices for pulling training and testing data\n",
    "        X_train, X_test = X.loc[I_train], X.loc[I_test] # features\n",
    "        y_train, y_test = y.loc[I_train], y.loc[I_test] # labels\n",
    "\n",
    "        # Train classifier\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Predict class values using trained classifier\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Calculate overall accuracy\n",
    "        accuracies[k] = metrics.accuracy_score(y_test, y_pred)\n",
    "        # Calculate Kappa score\n",
    "        Ks[k] = metrics.cohen_kappa_score(y_test, y_pred)\n",
    "        # Calculate confusion matrix\n",
    "        # CM_folds[:, :, k] = metrics.confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "    # Calculate mean accuracy and Kappa score\n",
    "    accuracy[j] = np.nanmean(accuracies)\n",
    "    K[j] = np.nanmean(Ks)\n",
    "\n",
    "    # Plot results\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "    ax.set_xlabel('Number of points used to train classifier')\n",
    "    ax.grid()\n",
    "    ax.plot(num_points_list, accuracies, '.-b', label='Accuracy')\n",
    "    ax.plot(num_points_list, Ks, '*-m', label='Kappa score')\n",
    "    ax.legend()\n",
    "    ax.set_title(name)\n",
    "    plt.show()\n",
    "    \n",
    "    j+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15270ccc",
   "metadata": {},
   "source": [
    "### b. Sentinel-1 (not-recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fea9139",
   "metadata": {},
   "source": [
    "#### Set-up training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef41fb8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = 'Sentinel-1'\n",
    "\n",
    "# -----Add path to preprocessing functions\n",
    "# path to gee_s1_ard/python-api/\n",
    "ard_path = base_path + '../gee_s1_ard/python-api/'\n",
    "# add functions to path\n",
    "sys.path.insert(1, ard_path)\n",
    "import wrapper as w\n",
    "\n",
    "# -----Define bands and feature columns (predictors used in classification)\n",
    "band_names = ['VV', 'VH', 'angle']\n",
    "feature_cols_S1 = band_names + ['VHVV', 'VV-VH']\n",
    "# buffer used for clipping images\n",
    "buffer = 2000 # [m]\n",
    "\n",
    "# -----Load classified points\n",
    "os.chdir(data_pts_path)\n",
    "data_pts_fns = glob.glob('*.shp')\n",
    "data_pts_fns.sort()\n",
    "\n",
    "# -----Check if training data exist in file\n",
    "S1_training_data_fn = 'S1_training_data.pkl'\n",
    "if os.path.exists(out_path + S1_training_data_fn):\n",
    "    \n",
    "    data_pts_full_L = pd.read_pickle(out_path + S1_training_data_fn)\n",
    "    print('Sentinel-1 training data already exist... loaded from file.')\n",
    "    \n",
    "else: \n",
    "    \n",
    "    # Initialize full data points dataframe (for use in next step)\n",
    "    data_pts_full_S1 = data_pts_full_PS.copy(deep=True)\n",
    "    # remove PS bands\n",
    "    data_pts_full_S1 = data_pts_full_S1.drop(columns=['blue', 'green', 'red', 'NIR', 'NDSI'])\n",
    "    # initialize band columns\n",
    "    data_pts_full_S1['S1_im_date'] = ' '\n",
    "    data_pts_full_S1[feature_cols_S1] = 0\n",
    "\n",
    "    # Loop through sites\n",
    "    # for i, site_name in enumerate(site_names):\n",
    "    site_name = 'SouthCascade'\n",
    "    \n",
    "    print('----------')\n",
    "    print(site_name)\n",
    "    print('----------')\n",
    "\n",
    "    # Extract image dates from data point file names\n",
    "    im_dates = [s[len(site_name)+1:len(site_name)+9] for s in data_pts_fns if (site_name in s) and ('snow.shp' in s)]\n",
    "\n",
    "    # Load AOI\n",
    "    AOI_fn = AOI_path + site_name + '/glacier_outlines/' + site_name + '_USGS_*.shp'\n",
    "    AOI_fn = glob.glob(AOI_fn)[0]\n",
    "    AOI = gpd.read_file(AOI_fn)\n",
    "    # reproject AOI to WGS 84 for compatibility with images\n",
    "    AOI_WGS = AOI.to_crs(4326)\n",
    "    # reformat AOI_WGS bounding box as ee.Geometry for clipping images\n",
    "    AOI_WGS_bb_ee = ee.Geometry.Polygon(\n",
    "                            [[[AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.miny[0]],\n",
    "                              [AOI_WGS.geometry.bounds.maxx[0], AOI_WGS.geometry.bounds.miny[0]],\n",
    "                              [AOI_WGS.geometry.bounds.maxx[0], AOI_WGS.geometry.bounds.maxy[0]],\n",
    "                              [AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.maxy[0]],\n",
    "                              [AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.miny[0]]]\n",
    "                            ]).buffer(3000)\n",
    "    # solve for optimal UTM zone for reprojection\n",
    "    AOI_WGS_centroid = [AOI_WGS.geometry[0].centroid.xy[0][0],\n",
    "                        AOI_WGS.geometry[0].centroid.xy[1][0]]\n",
    "    epsg_UTM = f.convert_wgs_to_utm(AOI_WGS_centroid[0], AOI_WGS_centroid[1])\n",
    "        \n",
    "        # # Load images from Earth Engine\n",
    "        # if site_name=='Gulkana':\n",
    "        #     im1_fn = 'LANDSAT/LC08/C02/T1_L2/LC08_067016_20210610'\n",
    "        #     im2_fn = 'LANDSAT/LC08/C02/T1_L2/LC08_068016_20210804'\n",
    "    if site_name=='SouthCascade':\n",
    "        im1_date_range = ['2021-07-01', '2021-07-03']\n",
    "        im2_date_range = ['2021-08-27', '2021-08-29']\n",
    "        # elif site_name=='Sperry':\n",
    "            # im1_fn = 'LANDSAT/LC08/C02/T1_L2/LC08_041026_20210706'\n",
    "            # im2_fn = 'LANDSAT/LC08/C02/T1_L2/LC08_041026_20210722'\n",
    "        # elif site_name=='Wolverine':\n",
    "            # im1_fn = 'LANDSAT/LC08/C02/T1_L2/LC08_067018_20220629'\n",
    "            # im2_fn = 'LANDSAT/LC08/C02/T1_L2/LC08_067018_20210829'        \n",
    "    date_ranges = [im1_date_range, im2_date_range]\n",
    "\n",
    "    # loop through images\n",
    "    for j in [0, 1]:\n",
    "\n",
    "        # Define dictionary of parameters\n",
    "        params = {'APPLY_BORDER_NOISE_CORRECTION': True,\n",
    "                  'APPLY_TERRAIN_FLATTENING': True,\n",
    "                  'APPLY_SPECKLE_FILTERING': True, \n",
    "                  'POLARIZATION': 'VVVH',\n",
    "                  'PLATFORM_NUMBER': None,\n",
    "                  'ORBIT': None, \n",
    "                  'ORBIT_NUM': None, \n",
    "                  'SPECKLE_FILTER_FRAMEWORK': 'MULTI',\n",
    "                  'SPECKLE_FILTER': 'LEE',\n",
    "                  'SPECKLE_FILTER_KERNEL_SIZE': 9,\n",
    "                  'SPECKLE_FILTER_NR_OF_IMAGES': 10,\n",
    "                  'APPLY_TERRAIN_FLATTENING': True,\n",
    "                  'DEM': ee.Image(\"NASA/ASTER_GED/AG100_003\"),\n",
    "                  'TERRAIN_FLATTENING_MODEL': 'VOLUME',\n",
    "                  'TERRAIN_FLATTENING_ADDITIONAL_LAYOVER_SHADOW_BUFFER': 0,\n",
    "                  'FORMAT' : 'DB',\n",
    "                  'CLIP_TO_ROI': True,\n",
    "                  'SAVE_ASSET': False,\n",
    "                  'ASSET_ID': None,\n",
    "                  'START_DATE': date_ranges[j][0],\n",
    "                  'STOP_DATE': date_ranges[j][1],\n",
    "                  'ROI': AOI_WGS_bb_ee\n",
    "                 }\n",
    "\n",
    "        # run the gee_s1_ard wrapper\n",
    "        im = w.s1_preproc(params)\n",
    "        \n",
    "        im_date = im_dates[j]\n",
    "            \n",
    "        # Convert ee.ImageCollection to xarray.Dataset\n",
    "        im_ds = im.wx.to_xarray(scale=10, crs='EPSG:4326')\n",
    "        # reproject to UTM\n",
    "        im_ds = im_ds.rio.reproject('EPSG:'+epsg_UTM)\n",
    "        # replace no data values with NaN\n",
    "        im_ds = im_ds.where(im_ds!=-32768)\n",
    "        \n",
    "        # select df columns for study site and image date\n",
    "        data_pts = data_pts_full_S1.loc[(data_pts_full_S1['site_name']==site_name) \n",
    "                                        & (data_pts_full_S1['PS_im_date']==im_date[0:4]+'-'+im_date[4:6]+'-'+im_date[6:8])]\n",
    "        # add image date\n",
    "        data_pts['S1_im_date'] = str(im_ds.time.data[0])[0:10]\n",
    "        # reproject to UTM\n",
    "        data_pts = data_pts.to_crs(epsg_UTM)\n",
    "            \n",
    "        # grab x and y coordinates for data points at the site\n",
    "        data_pts_x = [data_pts['geometry'].reset_index(drop=True)[i].geoms[0].x\n",
    "                      for i in np.arange(0,len(data_pts))]\n",
    "        data_pts_y = [data_pts['geometry'].reset_index(drop=True)[i].geoms[0].y\n",
    "                      for i in np.arange(0,len(data_pts))]\n",
    "        # extract band values at data points \n",
    "        for band_name in band_names:\n",
    "            data_pts[band_name] = [im_ds.sel(x=x, y=y, method=\"nearest\")[band_name].data[0] \n",
    "                                   for x, y in list(zip(data_pts_x, data_pts_y))]\n",
    "\n",
    "        # add data_pts back to full df\n",
    "        data_pts_full_S1.loc[(data_pts_full_S1['site_name']==site_name) \n",
    "                             & (data_pts_full_S1['PS_im_date']==im_date[0:4]+'-'+im_date[4:6]+'-'+im_date[6:8])] = data_pts\n",
    "\n",
    "        # plot images and data points\n",
    "        fig1, ax1 = plt.subplots(1, 1, figsize=(10,10))\n",
    "        ax1.imshow(im_ds['VV'].data[0], cmap='Greys',\n",
    "                   extent=(np.min(im_ds.x.data)/1e3, np.max(im_ds.x.data)/1e3, \n",
    "                           np.min(im_ds.y.data)/1e3, np.max(im_ds.y.data)/1e3))\n",
    "        ax1.scatter([x.geoms[0].x/1e3 for x in data_pts['geometry'].loc[data_pts['class']==1]], \n",
    "                    [x.geoms[0].y/1e3 for x in data_pts['geometry'].loc[data_pts['class']==1]], c='cyan', s=1)\n",
    "        ax1.scatter([x.geoms[0].x/1e3 for x in data_pts['geometry'].loc[data_pts['class']==3]], \n",
    "                    [x.geoms[0].y/1e3 for x in data_pts['geometry'].loc[data_pts['class']==3]], c='blue', s=1)\n",
    "        ax1.scatter([x.geoms[0].x/1e3 for x in data_pts['geometry'].loc[data_pts['class']==4]], \n",
    "                    [x.geoms[0].y/1e3 for x in data_pts['geometry'].loc[data_pts['class']==4]], c='orange', s=1)\n",
    "        ax1.scatter([x.geoms[0].x/1e3 for x in data_pts['geometry'].loc[data_pts['class']==5]], \n",
    "                    [x.geoms[0].y/1e3 for x in data_pts['geometry'].loc[data_pts['class']==5]], c='grey', s=1)\n",
    "        ax1.set_xlabel('Easting [km]')\n",
    "        ax1.set_ylabel('Northing [km]')\n",
    "        plt.show()\n",
    "\n",
    "    # Remove no data points\n",
    "    data_pts_full_S1 = data_pts_full_S1.dropna().reset_index(drop=True)\n",
    "    \n",
    "    # Add VH/VV and VV-VH columns\n",
    "    data_pts_full_S1['VHVV'] = data_pts_full_S1['VH'] / data_pts_full_S1['VV']\n",
    "    data_pts_full_S1['VV-VH'] = data_pts_full_S1['VV'] - data_pts_full_S1['VH']\n",
    "\n",
    "    # Reduce memory usage in df\n",
    "    data_pts_full_S1 = f.reduce_memory_usage(data_pts_full_S1)\n",
    "\n",
    "    # Save training data to file\n",
    "    data_pts_full_S1.to_pickle(out_path + S1_training_data_fn)\n",
    "    print('Landsat training data saved to file:' + out_path + S1_training_data_fn)\n",
    "\n",
    "    # Save feature columns\n",
    "    feature_cols_fn = out_path + 'S1_feature_cols.pkl'\n",
    "    pickle.dump(feature_cols_S1, open(feature_cols_fn, 'wb'))\n",
    "    print('Feature columns saved to file: '+ feature_cols_fn)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205a72b1",
   "metadata": {},
   "source": [
    "#### Plot pairplot of training data spectral characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a70138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_pts_full_S1['class'] = data_pts_full_S1['class'].astype(object)\n",
    "fig = sns.pairplot(data_pts_full_S1[['class'] + feature_cols_S1], markers='.',  \n",
    "             corner=True, diag_kind='kde', hue='class', palette=\"colorblind\");\n",
    "# save figure\n",
    "if save_figures:\n",
    "    fig_fn = base_path + 'figures/spectral_pairplot_Sentinel1.png'\n",
    "    fig.savefig(fig_fn, facecolor='w', dpi=300)\n",
    "    print('figure saved to file:' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c9c580",
   "metadata": {},
   "source": [
    "#### Test one classifier for _each_ site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5364648b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Test supervised classification algorithms\n",
    "# Loop through sites\n",
    "# for i, site_name in enumerate(site_names):\n",
    "site_name = 'SouthCascade'\n",
    "\n",
    "print('----------')\n",
    "print(site_name)\n",
    "print('----------')\n",
    "\n",
    "# Select all columns in data_pts_full_PS for site\n",
    "data_pts = data_pts_full_S1.loc[data_pts_full_S1['site_name']==site_name]\n",
    "data_pts = data_pts.reset_index(drop=True)\n",
    "\n",
    "# Split data points into features (band values / terrain parameters) and target variable (class)\n",
    "X = data_pts[feature_cols_S1] # features\n",
    "y = data_pts['class'] # target variable\n",
    "y = y.astype(int)\n",
    "\n",
    "# Iterate over classifiers\n",
    "accuracy = np.zeros(len(classifiers)) # mean accuracy\n",
    "K = np.zeros(len(classifiers)) # mean Kappa score\n",
    "j=0\n",
    "for name, clf in zip(names, classifiers):\n",
    "\n",
    "    print(name)\n",
    "\n",
    "    # Conduct K-Fold cross-validation\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True, random_state=1)\n",
    "    accuracy_folds = np.zeros(num_folds) # accuracy for all simulations\n",
    "    K_folds = np.zeros(num_folds) # kappa score for all MC simulations\n",
    "    k=0 # iteration counter\n",
    "    # enumerate the splits and summarize the distributions\n",
    "    for train_ix, test_ix in kfold.split(X):\n",
    "\n",
    "        # select rows\n",
    "        X_train, X_test = X.loc[train_ix], X.loc[test_ix]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "        # Train classifier\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Predict class values using trained classifier\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Calculate overall accuracy\n",
    "        accuracy_folds[k] = metrics.accuracy_score(y_test, y_pred)\n",
    "        # Calculate Kappa score\n",
    "        K_folds[k] = metrics.cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "        k+=1\n",
    "\n",
    "    # Calculate mean accuracy and Kappa score\n",
    "    accuracy[j] = np.nanmean(accuracy_folds)\n",
    "    K[j] = np.nanmean(K_folds)\n",
    "\n",
    "    j+=1\n",
    "\n",
    "# Determine best classifier based on accuracy\n",
    "results = pd.DataFrame()\n",
    "results['Classifier'], results['Accuracy'], results['Kappa_score'] = names, accuracy, K\n",
    "clf_best_name = names[np.where(accuracy==np.max(accuracy))[0][0]]\n",
    "clf_best = classifiers[np.where(accuracy==np.max(accuracy))[0][0]]\n",
    "print(results)\n",
    "print('')\n",
    "print('Best accuracy classifier: ' + clf_best_name)\n",
    "\n",
    "# -----Save most accurate classifier\n",
    "if save_outputs==True:\n",
    "    clf_fn = out_path + 'S1_classifier_'+site_names[i]+'.sav'\n",
    "    pickle.dump(clf_best, open(clf_fn, 'wb'))\n",
    "    print('Most accurate classifier saved to file: ',clf_fn)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bdcfbf",
   "metadata": {},
   "source": [
    "#### Test one classifier for _all_ sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3206a546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Define image bands\n",
    "bands = [x for x in im_ds.data_vars]\n",
    "# bands = [band for band in bands if (band != 'QA_PIXEL') and ('B' in band)]\n",
    "        \n",
    "im_date = str(im_ds.time.data[0])[0:10]\n",
    "print(im_date)\n",
    "        \n",
    "im_AOI = im_ds\n",
    "\n",
    "# add VHVV and VV-VH columns\n",
    "im_ds['VHVV'] = im_ds['VH'] / im_ds['VV']\n",
    "im_ds['VV-VH'] = im_ds['VV'] - im_ds['VH']\n",
    "\n",
    "# find indices of real numbers (no NaNs allowed in classification)\n",
    "ix = [np.where(np.isnan(im_AOI[band].data), False, True) for band in bands]\n",
    "I_real = np.full(np.shape(im_AOI[bands[0]].data), True)\n",
    "for ixx in ix:\n",
    "    I_real = I_real & ixx\n",
    "            \n",
    "# create df of image band values\n",
    "df = pd.DataFrame(columns=feature_cols_S1)\n",
    "for col in feature_cols_S1:\n",
    "    df[col] = np.ravel(im_AOI[col].data[I_real])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# -----Classify image\n",
    "if len(df)>1:\n",
    "    array_classified = clf.predict(df[feature_cols_S1])\n",
    "else:\n",
    "    print(\"No real values found to classify, skipping...\")\n",
    "    # continue\n",
    "\n",
    "# reshape from flat array to original shape\n",
    "im_classified = np.zeros(im_AOI.to_array().data[0].shape)\n",
    "im_classified[:] = np.nan\n",
    "im_classified[I_real] = array_classified\n",
    "            \n",
    "# -----Plot results\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,6))\n",
    "ax = ax.flatten()\n",
    "# define x and y limits\n",
    "xmin, xmax = np.min(im_ds.x.data)/1e3, np.max(im_ds.x.data)/1e3\n",
    "ymin, ymax = np.min(im_ds.y.data)/1e3, np.max(im_ds.y.data)/1e3\n",
    "# define colors for plotting\n",
    "color_snow = '#4eb3d3'\n",
    "color_ice = '#084081'\n",
    "color_rock = '#fdbb84'\n",
    "color_water = '#bdbdbd'\n",
    "color_contour = '#f768a1'\n",
    "# create colormap\n",
    "colors = [color_snow, color_snow, color_ice, color_rock, color_water]\n",
    "cmp = matplotlib.colors.ListedColormap(colors)\n",
    "# RGB image\n",
    "ax[0].imshow(im_ds['VV'].data[0], cmap='Greys',\n",
    "             extent=(xmin, xmax, ymin, ymax))\n",
    "ax[0].set_xlabel(\"Easting [km]\")\n",
    "ax[0].set_ylabel(\"Northing [km]\")\n",
    "ax[0].set_title('RGB image')\n",
    "# classified image\n",
    "ax[1].imshow(im_classified[0], cmap=cmp, vmin=1, vmax=5,\n",
    "             extent=(np.min(im_AOI.x.data)/1e3, np.max(im_AOI.x.data)/1e3,\n",
    "                     np.min(im_AOI.y.data)/1e3, np.max(im_AOI.y.data)/1e3))\n",
    "# plot dummy points for legend\n",
    "ax[1].scatter(0, 0, color=color_snow, s=50, label='snow')\n",
    "ax[1].scatter(0, 0, color=color_ice, s=50, label='ice')\n",
    "ax[1].scatter(0, 0, color=color_rock, s=50, label='rock')\n",
    "ax[1].scatter(0, 0, color=color_water, s=50, label='water')\n",
    "ax[1].set_title('Classified image')\n",
    "ax[1].set_xlabel('Easting [km]')\n",
    "ax[1].legend(loc='best')\n",
    "# AOI\n",
    "if AOI.geometry[0].geom_type=='MultiPolygon': # loop through geoms if AOI = MultiPolygon\n",
    "    for j, poly in enumerate(AOI.geometry[0].geoms):\n",
    "        # only include legend label for first geom\n",
    "        if j==0:\n",
    "            ax[0].plot([x/1e3 for x in poly.exterior.coords.xy[0]], [y/1e3 for y in poly.exterior.coords.xy[1]], '-k', linewidth=1, label='AOI')\n",
    "        else:\n",
    "            ax[0].plot([x/1e3 for x in poly.exterior.coords.xy[0]], [y/1e3 for y in poly.exterior.coords.xy[1]], '-k', linewidth=1, label='_nolegend_')\n",
    "        ax[1].plot([x/1e3 for x in poly.exterior.coords.xy[0]], [y/1e3 for y in poly.exterior.coords.xy[1]], '-k', linewidth=1, label='_nolegend_')\n",
    "else:\n",
    "    ax[0].plot([x/1e3 for x in AOI.geometry[0].exterior.coords.xy[0]], [y/1e3 for y in AOI.geometry[0].exterior.coords.xy[1]], '-k', linewidth=1, label='AOI')\n",
    "    ax[1].plot([x/1e3 for x in AOI.geometry[0].exterior.coords.xy[0]], [y/1e3 for y in AOI.geometry[0].exterior.coords.xy[1]], '-k', linewidth=1, label='_nolegend_')\n",
    "# reset x and y limits\n",
    "ax[0].set_xlim(xmin, xmax)\n",
    "ax[0].set_ylim(ymin, ymax)\n",
    "ax[1].set_xlim(xmin, xmax)\n",
    "ax[1].set_ylim(ymin, ymax)\n",
    "fig.suptitle(im_date)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planet-snow",
   "language": "python",
   "name": "planet-snow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
