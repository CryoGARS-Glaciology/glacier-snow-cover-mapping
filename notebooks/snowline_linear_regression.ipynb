{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16794d14-1223-48e3-8dd8-a9c749a7b2d8",
   "metadata": {},
   "source": [
    "# Fit a linear trendline to snowline time series\n",
    "\n",
    "Rainey Aberle\n",
    "\n",
    "2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e7d881-cb28-43f2-bb00-85bc6465a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import iqr\n",
    "from time import mktime\n",
    "\n",
    "# path to snow-cover-mapping\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1311957-512b-418b-b854-3f4b9e6cd0c4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "site_names = ['Wolverine', 'Gulkana', 'LemonCreek', 'SouthCascade', 'Sperry']\n",
    "\n",
    "# -----loop through sites\n",
    "nmc = 100 # number of monte carlo simulations\n",
    "pTrain = 0.8 # percentage of points to use for training\n",
    "for site_name in site_names:\n",
    "    \n",
    "    print(site_name)\n",
    "    print('----------')\n",
    "    \n",
    "    # -----Load snowlines             \n",
    "    # PlanetScope\n",
    "    PS_sl_est_path = glob.glob(base_path + '../study-sites/' + site_name + \n",
    "                               '/imagery/PlanetScope/snowlines/*snowlines.pkl')[0]\n",
    "    PS_sl_est = pd.read_pickle(PS_sl_est_path)\n",
    "    PS_sl_est['dataset'] = 'PlanetScope'\n",
    "    # Sentinel-2\n",
    "    S2_sl_est_path = glob.glob(base_path + '../study-sites/' + site_name + \n",
    "                               '/imagery/Sentinel-2/snowlines/*snowlines.pkl')[0]\n",
    "    S2_sl_est = pd.read_pickle(S2_sl_est_path)\n",
    "    S2_sl_est['dataset'] = 'Sentinel2'\n",
    "    # Landsat\n",
    "    L_sl_est_path = glob.glob(base_path + '../study-sites/' + site_name + \n",
    "                              '/imagery/Landsat/snowlines/*snowlines.pkl')[0]\n",
    "    L_sl_est = pd.read_pickle(L_sl_est_path)\n",
    "    L_sl_est['dataset'] = 'Landsat'\n",
    "    \n",
    "    # -----Concatenate snowlines dataframes\n",
    "    sl_est_full = pd.concat([PS_sl_est, S2_sl_est, L_sl_est])\n",
    "    # unify datetime datatypes\n",
    "    sl_est_full['datetime'] = sl_est_full['datetime'].astype(np.datetime64)\n",
    "    # add year column\n",
    "    sl_est_full['year'] = [x.year for x in sl_est_full['datetime']]\n",
    "    # sort df by datetime\n",
    "    sl_est_full = sl_est_full.sort_values(by=['datetime'])\n",
    "    # grab unique years\n",
    "    years = np.unique(sl_est_full['year'])\n",
    "    \n",
    "    # set up figure\n",
    "    # fig, ax = plt.subplots(1, len(years), figsize=(6*len(years), 6))\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6*len(years), 6))\n",
    "    ax.plot(sl_est_full['datetime'], sl_est_full['snowlines_elevs_median'], 'ok', markersize=10, label='data points')\n",
    "    ax.set_ylabel('Snowline elevation [m]')\n",
    "    ax.grid()\n",
    "    plt.rcParams.update({'font.size':14})\n",
    "\n",
    "    # -----Loop through years\n",
    "    results_linear_model = pd.DataFrame()\n",
    "    for i, year in enumerate(years):\n",
    "        \n",
    "        # subset df\n",
    "        sl_est_full_year = sl_est_full.loc[(sl_est_full['year']==year) \n",
    "                                           & (sl_est_full['datetime'] < pd.to_datetime(str(year)+'-10-01'))]\n",
    "                                           # & (sl_est_full['datetime'] > pd.to_datetime(str(year)+'-06-01'))]\n",
    "        \n",
    "        # conduct Monte Carlo simulations to generate 100 linear fit models\n",
    "        # grab X and Y data from snowline dates and median elevations\n",
    "        datetimes = np.ravel(sl_est_full_year['datetime'])\n",
    "        snowlines_elevs_median = np.array(np.ravel(sl_est_full_year['snowlines_elevs_median']), dtype=float)\n",
    "        # remove NaNs\n",
    "        X = datetimes[~np.isnan(snowlines_elevs_median)]\n",
    "        if len(X) < 2:\n",
    "            print('Not enough data points in ' + str(year) + ' for linear fit, skipping...')\n",
    "            continue\n",
    "        y = snowlines_elevs_median[~np.isnan(snowlines_elevs_median)]\n",
    "        # convert dates to days after the first image date capture\n",
    "        day1 = X[0] - np.timedelta64(1, 'D')\n",
    "        X = np.array([np.timedelta64(day - day1, 'D') for day in X], dtype=float)\n",
    "        # initialize coefficients data frame\n",
    "        X_mod = np.linspace(X[0], X[-1], num=100) # points at which to evaluate the model\n",
    "        y_mod = np.zeros((nmc, len(X_mod))) # array to hold modeled Y values\n",
    "        m_mod = np.zeros(nmc) # linear fit coefficients\n",
    "        b_mod = np.zeros(nmc) # linear fit intercepts\n",
    "        y_mod_err = np.zeros(nmc) # array to hold error associated with each model\n",
    "        # loop through Monte Carlo simulations\n",
    "        for j in np.arange(0,nmc):\n",
    "            # split into training and testing data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=pTrain, shuffle=True)\n",
    "            # fit linear to training data\n",
    "            fit = LinearRegression(positive=True).fit(X_train.reshape(-1, 1), y_train)\n",
    "            # apply fourier model to testing data\n",
    "            y_pred = fit.predict(X_test.reshape(-1, 1))\n",
    "            # calculate mean error\n",
    "            y_mod_err[j] = np.sum(np.abs(y_test - y_pred)) / len(y_test)\n",
    "            # apply the model to the full X data\n",
    "            y_mod[j,:] = fit.predict(X_mod.reshape(-1, 1))\n",
    "            # store the coefficient and intercept\n",
    "            m_mod[j] = fit.coef_[0]\n",
    "            b_mod[j] = fit.intercept_\n",
    "\n",
    "        # identify model with lowest error\n",
    "        Ibest = np.argmin(y_mod_err)\n",
    "        m_mod_best = m_mod[Ibest]\n",
    "        b_mod_best = b_mod[Ibest]\n",
    "        y_mod_best = [x*m_mod_best+b_mod_best for x in X_mod]\n",
    "        # convert X back to dates\n",
    "        X = [np.timedelta64(int(x), 'D')+day1 for x in X]\n",
    "        X_mod = [np.timedelta64(int(x), 'D')+day1 for x in X_mod]\n",
    "        \n",
    "        # save results in DataFrame\n",
    "        result_linear_model = pd.DataFrame({'year': year,\n",
    "                               'X': [X],\n",
    "                               'y': [y],\n",
    "                               'X_mod': [X_mod],\n",
    "                               'y_mod_best': [y_mod_best]\n",
    "                              })\n",
    "        results_linear_model = pd.concat([results_linear_model, result_linear_model])\n",
    "        \n",
    "        # calculate the IQR and median models\n",
    "        # m_mod_iqr, m_mod_median = iqr(m_mod), np.nanmedian(b_mod)\n",
    "        # m_mod_P25, m_mod_P75 = m_mod_median - m_mod_iqr/2, m_mod_median + m_mod_iqr/2\n",
    "        # b_mod_iqr, b_mod_median = iqr(b_mod), np.nanmedian(b_mod)\n",
    "        # b_mod_P25, b_mod_P75 = b_mod_median - b_mod_iqr/2, b_mod_median + b_mod_iqr/2   \n",
    "        # y_mod_median = [m_mod_median*x + b_mod_median for x in X_mod]\n",
    "        # y_mod_P25 = [m_mod_P25*x + b_mod_P25 for x in X_mod]\n",
    "        # y_mod_P75 = [m_mod_P75*x + b_mod_P75 for x in X_mod]\n",
    "        # y_mod_iqr = iqr(y_mod, axis=0)\n",
    "        # y_mod_median = np.nanmedian(y_mod, axis=0)\n",
    "        # y_mod_P25 = y_mod_median - y_mod_iqr/2\n",
    "        # y_mod_P75 = y_mod_median + y_mod_iqr/2\n",
    "        \n",
    "        # plot results\n",
    "        # ax[i].fill_between(X_mod, y_mod_P25, y_mod_P75, facecolor='blue', alpha=0.5, label='model$_{IQR}$')\n",
    "        # ax[i].plot(X_mod, y_mod_median, '.-b', linewidth=1, label='model$_{median}$')\n",
    "        if i==1:\n",
    "            ax.plot(X_mod, y_mod_best, '-b', linewidth=3, label='best linear model')\n",
    "        else:\n",
    "            ax.plot(X_mod, y_mod_best, '-b', linewidth=3, label='_nolegend_')\n",
    "        ax.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    # save results in df\n",
    "    results_snowlines = sl_est_full\n",
    "    out_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/../study-sites/' + site_name + '/imagery/'\n",
    "    out_fn = (site_name + '_' + str(np.max(sl_est_full['datetime'])).replace('-','').replace(':','')[0:8]\n",
    "              + '_' + str(np.max(sl_est_full['datetime'])).replace('-','')[0:8] + '_snowlines.pkl')\n",
    "    results_snowlines.to_pickle(out_path + out_fn)\n",
    "    print('compiled snowlines saved to file: ' + out_path + out_fn)\n",
    "    print(' ')\n",
    "    results_linear_model.to_pickle(out_path + out_fn[0:-4]+ '_linear_model.pkl')\n",
    "    print('snowlines linear model saved to file: ' + out_path + out_fn)\n",
    "    print(' ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f88aa60-f5cb-4775-8f61-9db70dc0b4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planet-snow",
   "language": "python",
   "name": "planet-snow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
