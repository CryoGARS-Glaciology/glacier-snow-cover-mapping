{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff1eb620",
   "metadata": {},
   "source": [
    "# Notebook to make figures for presentations, manuscripts, etc.\n",
    "\n",
    "Rainey Aberle\n",
    "\n",
    "2022/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e9e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import contextily as cx\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from skimage.measure import find_contours\n",
    "import ee\n",
    "import sys\n",
    "from shapely.geometry import Point, LineString, Polygon, MultiPolygon\n",
    "import rasterio as rio\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap, LightSource\n",
    "import glob\n",
    "import wxee as wx\n",
    "import matplotlib\n",
    "import pickle\n",
    "from scipy.signal import medfilt\n",
    "from scipy.stats import iqr\n",
    "import os\n",
    "import glob\n",
    "import operator\n",
    "import json\n",
    "from ast import literal_eval\n",
    "import seaborn as sns\n",
    "import wxee as wx\n",
    "import geedim as gd\n",
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "from shapely import wkt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# path to snow-cover-mapping/\n",
    "base_path = '/Users/raineyaberle/Research/glacier_snow_cover_mapping/glacier-snow-cover-mapping/'\n",
    "\n",
    "# path to study sites\n",
    "study_sites_path = '/Users/raineyaberle/Research/glacier_snow_cover_mapping/study-sites/'\n",
    "# determine whether to save output figures\n",
    "save_figures = True\n",
    "\n",
    "# path for saving output figures\n",
    "figures_out_path = os.path.join(base_path, 'figures')\n",
    "\n",
    "# add path to functions\n",
    "sys.path.insert(1, os.path.join(base_path, 'functions'))\n",
    "import pipeline_utils as f\n",
    "import PlanetScope_preprocessing as psp\n",
    "\n",
    "# load dataset dictionary\n",
    "dataset_dict = json.load(open(os.path.join(base_path,'inputs-outputs', 'datasets_characteristics.json')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4579e3a-9da2-4e1a-a7b4-da00e6a7cb0a",
   "metadata": {},
   "source": [
    "### Define some colormaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc8b362-d077-481f-b2eb-edb6bca4a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Imagery Datasets\n",
    "color_Landsat = '#ff7f00'\n",
    "color_Sentinel2 = '#984ea3'\n",
    "color_PlanetScope = '#4daf4a'\n",
    "\n",
    "ListedColormap([color_Landsat, color_Sentinel2, color_PlanetScope])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775c7849-b09b-4c83-9477-2af67f9ac92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Classified images\n",
    "# Indicies: 0 = snow, 1 = shadowed snow, 2 = ice, 3 = bare ground, 4 = water\n",
    "colors_classified = list(dataset_dict['classified_image']['class_colors'].values())\n",
    "ListedColormap(colors_classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609c2024-e1a9-405a-a1df-7fb5ff9f6dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Elevation\n",
    "cmap_elev = matplotlib.colors.LinearSegmentedColormap.from_list('custom_colormap', \n",
    "                                                                ['k', '#01665e', '#f5f5f5', '#8c510a'], \n",
    "                                                                N=256, gamma=1.0)\n",
    "cmap_elev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013784a0-a641-4ae4-b9e7-24d20d8afe1f",
   "metadata": {},
   "source": [
    "## Figure 1. Spectral signatures for earth materials and satellite band ranges and example NDSI thresholding applied to an image at Wolverine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9d1ed4-cb10-4ba7-9620-8ebc4a38b268",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# -----Set up figure\n",
    "# define colors for different materials\n",
    "color_snow = colors_classified[0]\n",
    "color_firn = '#12617a'\n",
    "color_ice = colors_classified[2]\n",
    "color_dirty_ice = '#1a2c40'\n",
    "color_veg = '#006d2c'\n",
    "color_rock = colors_classified[3]\n",
    "color_water = colors_classified[4]\n",
    "# plot\n",
    "fontsize = 16\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig = plt.figure(figsize=(12,14))\n",
    "linewidth=2\n",
    "# define axes layout using gridspec\n",
    "gs = gridspec.GridSpec(2, 2, height_ratios=[1, 1])\n",
    "ax = [fig.add_subplot(gs[0, :]), fig.add_subplot(gs[1, 0]), fig.add_subplot(gs[1, 1])]\n",
    "\n",
    "# -----Plot satellite band ranges\n",
    "def draw_boxes(axis, band_ranges, NDSI_indices, y0=0.2, box_height=0.04, \n",
    "               facecolor='#bdbdbd', edgecolor='k', alpha=1.0, NDSI_label=False):\n",
    "    labeled = False\n",
    "    # loop over band ranges\n",
    "    for i, band_range in enumerate(band_ranges):\n",
    "        # convert from nanometers to micrometers\n",
    "        x0, x1 = band_range[0], band_range[1]\n",
    "        # calculate width\n",
    "        box_width = x1-x0\n",
    "        # create rectangle and add to axes\n",
    "        axis.add_patch(Rectangle((x0, y0), width=box_width, height=box_height, \n",
    "                       facecolor=facecolor, edgecolor=edgecolor, alpha=alpha))\n",
    "        # plot star on NDSI bands\n",
    "        if i in NDSI_indices:\n",
    "            if (not labeled) and NDSI_label:\n",
    "                label = 'NDSI bands'\n",
    "                labeled = True\n",
    "            else:\n",
    "                label='_nolegend_'\n",
    "            axis.plot(x0 + box_width/2, y0 + box_height/2, '*k', markersize=10, label=label)\n",
    "\n",
    "    # add one rectangle to contain all bands\n",
    "    x0, x1 = band_ranges[0][0], band_ranges[-1][-1]\n",
    "    box_width = x1-x0\n",
    "    axis.add_patch(Rectangle((x0, y0), width=box_width, height=box_height, facecolor='none', edgecolor='k', alpha=1.0))\n",
    "    return\n",
    "\n",
    "# Landsat 8/9 OLI\n",
    "L_band_ranges = [[0.45, 0.51], [0.53, 0.59], [0.64, 0.67], [0.85, 0.88], # 2, 3, 4, 5\n",
    "                 [1.57, 1.65], [2.11, 2.29]] # 6, 7\n",
    "L_band_names = ['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2']#, 'TIRS1', 'TIRS2']\n",
    "L_NDSI_band_indices = [1, 4]\n",
    "draw_boxes(ax[0], L_band_ranges, L_NDSI_band_indices, y0=1.001, NDSI_label=True, facecolor=color_Landsat)\n",
    "ax[0].text(2.32, 1.005, 'Landsat 8/9 (30 m)')\n",
    "# Sentinel-2 MSI\n",
    "S2_20_band_ranges = [[0.69, 0.718], [0.727, 0.755], [0.764, 0.802], # B5, B6, B7\n",
    "                     [0.845, 0.85], [1.52, 1.70], [2.010, 2.37]] # B8A, B11 (SWIR1), B12 (SWIR2)\n",
    "S2_20_NDSI_band_indices = [4]\n",
    "draw_boxes(ax[0], S2_20_band_ranges, S2_20_NDSI_band_indices, y0=1.101, facecolor=color_Sentinel2)\n",
    "ax[0].text(2.4, 1.105, 'Sentinel-2 (20 m)')\n",
    "\n",
    "S2_10_band_ranges = [[0.425, 0.555], [0.525, 0.595], [0.635, 0.695], # B2 B3 B4 \n",
    "                     [0.728, 1.038]] # B8 (NIR)\n",
    "S2_10_NDSI_band_indices = [1]\n",
    "draw_boxes(ax[0], S2_10_band_ranges, S2_10_NDSI_band_indices, y0=1.201, facecolor=color_Sentinel2)\n",
    "ax[0].text(1.068, 1.205, 'Sentinel-2 (10 m)')\n",
    "# PlanetScope 4-band\n",
    "PS_band_ranges = [[0.455, 0.515], [0.51, 0.59], [0.590, 0.670], [0.780, 0.860]]\n",
    "PS_NDSI_indices = [1, 3]\n",
    "draw_boxes(ax[0], PS_band_ranges, PS_NDSI_indices, y0=1.301, facecolor=color_PlanetScope)\n",
    "ax[0].text(0.90, 1.305, 'PlanetScope 4-band (3-5 m)')\n",
    "\n",
    "# -----Load spectral signatures data and plot\n",
    "# Painter et al. (2009): snow, coarse- to fine-grained\n",
    "spec_path_painter = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/write-ups/CH1_snow_cover_mapping_methods_manuscript/figures/spectral_signatures_Painter_et_al_2009'\n",
    "os.chdir(spec_path_painter)\n",
    "coarse_snow = pd.read_csv('coarse_snow_Painter_et_al_2009.csv', header=None)\n",
    "fine_snow = pd.read_csv('fine_snow_Painter_et_al_2009.csv', header=None)\n",
    "# interpolate to same x values\n",
    "x_snow = np.linspace(0, 2.5, num=100)\n",
    "y_coarse = np.interp(x_snow, coarse_snow[0].values, coarse_snow[1].values)\n",
    "y_fine = np.interp(x_snow, fine_snow[0].values, fine_snow[1].values)\n",
    "y_med = np.array([np.nanmean([y1, y2]) for y1, y2 in list(zip(y_coarse, y_fine))])\n",
    "# plot\n",
    "ax[0].fill_between(x_snow, y_fine, y_coarse, color=color_snow, alpha=0.4)\n",
    "ax[0].plot(x_snow, y_med, '-', color=color_snow, linewidth=linewidth, label='snow')\n",
    "# Salvatori et al. (2022): ice \n",
    "spec_path_salv = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/write-ups/CH1_snow_cover_mapping_methods_manuscript/figures/spectral_signatures_Salvatori_et_al_2022'\n",
    "os.chdir(spec_path_salv)\n",
    "ice = pd.read_csv('ice.csv', header=None)\n",
    "# interpolate min, max, and mean values\n",
    "x_ice = np.linspace(0.4, 2.4, num=50)\n",
    "dx = x_ice[1] - x_ice[0]\n",
    "y_ice_min, y_ice_max, y_ice_mean = np.zeros(len(x_ice)), np.zeros(len(x_ice)), np.zeros(len(x_ice))\n",
    "for i in range(0,len(x_ice)-1):\n",
    "    x_window = [x_ice[i] - dx/2, x_ice[i] + dx/2]\n",
    "    if np.any((ice[0].values > x_window[0]) & (ice[0].values < x_window[1])):\n",
    "        y_ice_min[i] = np.nanmin(ice[1].values[(ice[0].values > x_window[0]) & (ice[0].values < x_window[1])])\n",
    "        y_ice_max[i] = np.nanmax(ice[1].values[(ice[0].values > x_window[0]) & (ice[0].values < x_window[1])])\n",
    "        y_ice_mean[i] = np.nanmean(ice[1].values[(ice[0].values > x_window[0]) & (ice[0].values < x_window[1])])\n",
    "    else:\n",
    "        y_ice_min[i], y_ice_max[i], y_ice_mean[i] = np.nan, np.nan, np.nan\n",
    "# plot\n",
    "ax[0].fill_between(x_ice, y_ice_min, y_ice_max, facecolor=color_ice, edgecolor=None, alpha=0.4)\n",
    "ax[0].plot(x_ice, y_ice_mean, '-', color=color_ice, linewidth=linewidth, label='ice and firn')\n",
    "# Zeng et al. (1984) / Hendriks et al. (2003): firn\n",
    "# spec_path_zeng = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/write-ups/CH1_snow_cover_mapping_methods_manuscript/figures/spectral_signatures_firn_Hendriks_et_al_2003'\n",
    "# firn = pd.read_csv(os.path.join(spec_path_zeng, 'Firn.csv'), header=None)\n",
    "# firn.rename(columns={0:'Wavelength', 1:'Reflectance'}, inplace=True)\n",
    "# firn.sort_values(by='Wavelength', inplace=True)\n",
    "# firn['Wavelength'] = firn['Wavelength'] / 1000\n",
    "# firn.loc[firn['Reflectance'] <= 0] = np.nan\n",
    "# firn.dropna(inplace=True)\n",
    "# firn.reset_index(drop=True, inplace=True)\n",
    "# dirty_ice = pd.read_csv(os.path.join(spec_path_zeng, 'Dirty glacier ice.csv'), header=None)\n",
    "# dirty_ice.rename(columns={0:'Wavelength', 1:'Reflectance'}, inplace=True)\n",
    "# dirty_ice.sort_values(by='Wavelength', inplace=True)\n",
    "# dirty_ice['Wavelength'] = dirty_ice['Wavelength'] / 1000\n",
    "# dirty_ice.reset_index(drop=True, inplace=True)\n",
    "# # plot\n",
    "# ax[0].plot(firn['Wavelength'], firn['Reflectance'], '-', color=color_firn, label='firn')\n",
    "# ax[0].plot(dirty_ice['Wavelength'], dirty_ice['Reflectance'], '-', color=color_dirty_ice, label='dirty glacier ice')\n",
    "\n",
    "# USGS: vegetation, soil, seawater\n",
    "colors = [color_veg, color_rock, color_water]\n",
    "spec_path_usgs = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/write-ups/CH1_snow_cover_mapping_methods_manuscript/figures/spectral_signatures_USGS/'\n",
    "os.chdir(spec_path_usgs)\n",
    "# define prefixes used in file names for each material\n",
    "prefixes = ['Aspen', 'Basalt', 'Seawater']\n",
    "# define labels for plot\n",
    "labels = ['vegetation', 'soil', 'seawater']\n",
    "# loop through prefixes\n",
    "for i, prefix in enumerate(prefixes):\n",
    "    # grab folder name\n",
    "    folder = glob.glob('*'+prefix+'*')[0]\n",
    "    # load wavelengths\n",
    "    wave_fn = glob.glob(folder + '/*Wavelengths*.txt')[0]\n",
    "    wave = pd.read_csv(wave_fn)\n",
    "    wave = wave[wave.keys()[0]].values\n",
    "    if prefix=='Basalt':\n",
    "        refl_fn = glob.glob(folder + '/*'+prefix+'*.txt')[1]\n",
    "    else:\n",
    "        refl_fn = glob.glob(folder + '/*'+prefix+'*.txt')[0]\n",
    "    refl = pd.read_csv(refl_fn)\n",
    "    refl = refl[refl.keys()[0]].values\n",
    "    refl[refl<0] = np.nan\n",
    "    # plot\n",
    "    ax[0].plot(wave, refl, '-', color=colors[i], linewidth=linewidth, label=labels[i])\n",
    "    \n",
    "ax[0].grid(True)\n",
    "ax[0].set_xlim(0.4, 3.3)\n",
    "ax[0].set_ylim(0, 1.4)\n",
    "ax[0].set_yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax[0].legend(loc='center right', bbox_to_anchor=[0.8, 0.3, 0.2, 0.2])\n",
    "ax[0].set_xlabel('Wavelength [$\\mu$m]')\n",
    "ax[0].set_ylabel('Reflectance')\n",
    "ax[0].text((ax[0].get_xlim()[1] - ax[0].get_xlim()[0])*0.94 + ax[0].get_xlim()[0],\n",
    "           (ax[0].get_ylim()[1] - ax[0].get_ylim()[0])*0.07 + ax[0].get_ylim()[0], '(a)', fontweight='bold', fontsize=fontsize+2, \n",
    "           bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "\n",
    "# -----Load Landsat image at Wolverine\n",
    "# image datetime\n",
    "dt = np.datetime64('2020-08-17')\n",
    "# load AOI\n",
    "AOI_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/write-ups/CH1_snow_cover_mapping_methods_manuscript/Aberle_et_al_dataset_submission/Wolverine/AOIs'\n",
    "AOI_fn = glob.glob(os.path.join(AOI_path, 'Wolverine_Glacier_Boundaries_20201019.shp'))[0]\n",
    "AOI_UTM = gpd.read_file(AOI_fn)\n",
    "# initialize GEE\n",
    "ee.Initialize()\n",
    "\n",
    "def query_gee_for_image(dt, aoi_utm):\n",
    "    # -----Grab datetime from snowline df\n",
    "    date_start = str(dt - np.timedelta64(1, 'D'))\n",
    "    date_end = str(dt + np.timedelta64(1, 'D'))\n",
    "    # -----Buffer AOI by 1km\n",
    "    aoi_utm_buffer = aoi_utm.buffer(1e3)\n",
    "    # determine bounds for image plotting\n",
    "    bounds = aoi_utm_buffer.geometry[0].bounds\n",
    "    # -----Reformat AOI for image filtering\n",
    "    # reproject CRS from AOI to WGS\n",
    "    aoi_wgs = aoi_utm.to_crs('EPSG:4326')\n",
    "    aoi_buffer_wgs = aoi_utm_buffer.to_crs('EPSG:4326')\n",
    "    # prepare AOI for querying geedim (AOI bounding box)\n",
    "    region = {'type': 'Polygon',\n",
    "              'coordinates': [[[aoi_buffer_wgs.geometry.bounds.minx[0], aoi_buffer_wgs.geometry.bounds.miny[0]],\n",
    "                               [aoi_buffer_wgs.geometry.bounds.maxx[0], aoi_buffer_wgs.geometry.bounds.miny[0]],\n",
    "                               [aoi_buffer_wgs.geometry.bounds.maxx[0], aoi_buffer_wgs.geometry.bounds.maxy[0]],\n",
    "                               [aoi_buffer_wgs.geometry.bounds.minx[0], aoi_buffer_wgs.geometry.bounds.maxy[0]],\n",
    "                               [aoi_buffer_wgs.geometry.bounds.minx[0], aoi_buffer_wgs.geometry.bounds.miny[0]]\n",
    "                               ]]}\n",
    "    region_buffer_ee = ee.Geometry.Polygon([[[aoi_buffer_wgs.geometry.bounds.minx[0], aoi_buffer_wgs.geometry.bounds.miny[0]],\n",
    "                                              [aoi_buffer_wgs.geometry.bounds.maxx[0], aoi_buffer_wgs.geometry.bounds.miny[0]],\n",
    "                                              [aoi_buffer_wgs.geometry.bounds.maxx[0], aoi_buffer_wgs.geometry.bounds.maxy[0]],\n",
    "                                              [aoi_buffer_wgs.geometry.bounds.minx[0], aoi_buffer_wgs.geometry.bounds.maxy[0]],\n",
    "                                              [aoi_buffer_wgs.geometry.bounds.minx[0], aoi_buffer_wgs.geometry.bounds.miny[0]]\n",
    "                                            ]])\n",
    "\n",
    "    # -----Query GEE for Landsat 8 imagery\n",
    "    im_col_gd = gd.MaskedCollection.from_name('LANDSAT/LC08/C02/T1_L2').search(start_date=date_start,\n",
    "                                                                               end_date=date_end,\n",
    "                                                                               mask=True,\n",
    "                                                                               region=region,\n",
    "                                                                               fill_portion=50)\n",
    "    im_col_ee = im_col_gd.ee_collection\n",
    "    \n",
    "    # -----Return first image as xarray.Dataset\n",
    "     # Grab first image\n",
    "    im_ee = im_col_ee.first()\n",
    "    # create MaskedImage from ID\n",
    "    im_gd = gd.MaskedImage.from_id(im_ee.getInfo()['id'], mask=False, region=region)\n",
    "    # convert to ee.Image\n",
    "    im_ee = ee.Image(im_gd.ee_image).select(im_gd.refl_bands)\n",
    "    # convert to xarray.Datasets\n",
    "    crs = str(AOI_UTM.crs.to_epsg())\n",
    "    im_xr = im_ee.wx.to_xarray(scale=30, region=region, crs='EPSG: '+ crs)\n",
    "    # account for image scalar\n",
    "    im_xr = xr.where(im_xr != dataset_dict['Landsat']['no_data_value'],\n",
    "                     im_xr / dataset_dict['Landsat']['image_scalar'], np.nan)\n",
    "    # set CRS\n",
    "    im_xr.rio.write_crs('EPSG:' + crs, inplace=True)\n",
    "    \n",
    "    return im_xr\n",
    "\n",
    "im_xr = query_gee_for_image(dt, AOI_UTM)\n",
    "\n",
    "# -----Calcualte NDSI\n",
    "NDSI = ((im_xr[dataset_dict['Landsat']['NDSI_bands'][0]].data[0] - im_xr[dataset_dict['Landsat']['NDSI_bands'][1]].data[0])\n",
    "        / (im_xr[dataset_dict['Landsat']['NDSI_bands'][0]].data[0] + im_xr[dataset_dict['Landsat']['NDSI_bands'][1]].data[0]))\n",
    "NDSI_thresh = np.where(NDSI > 0.4, 1, 0)\n",
    "NDSI_cmap = ListedColormap(['white', dataset_dict['classified_image']['class_colors']['Snow']])\n",
    "\n",
    "# ----- Plot\n",
    "xticks = np.arange(int(np.min(im_xr.x.data)/1e3)+1, np.max(im_xr.x.data)/1e3, step=2)\n",
    "yticks = np.arange(int(np.min(im_xr.y.data)/1e3)+1, np.max(im_xr.y.data)/1e3, step=2)\n",
    "ax[1].imshow(np.dstack([im_xr['SR_B4'].data[0], im_xr['SR_B3'].data[0], im_xr['SR_B2'].data[0]]),\n",
    "             extent=(np.min(im_xr.x.data)/1e3, np.max(im_xr.x.data)/1e3, np.min(im_xr.y.data)/1e3, np.max(im_xr.y.data)/1e3))\n",
    "ax[1].set_xticks(xticks)\n",
    "ax[1].set_yticks(yticks)\n",
    "ax[1].grid(False)\n",
    "ax[1].set_xlabel('Easting [km]')\n",
    "ax[1].set_ylabel('Northing [km]')\n",
    "ax[1].text((ax[1].get_xlim()[1] - ax[1].get_xlim()[0])*0.9 + ax[1].get_xlim()[0],\n",
    "           (ax[1].get_ylim()[1] - ax[1].get_ylim()[0])*0.07 + ax[1].get_ylim()[0], '(b)', fontweight='bold', fontsize=fontsize+2, \n",
    "           bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "ax[2].imshow(NDSI_thresh, cmap=NDSI_cmap,\n",
    "             extent=(np.min(im_xr.x.data)/1e3, np.max(im_xr.x.data)/1e3, np.min(im_xr.y.data)/1e3, np.max(im_xr.y.data)/1e3))\n",
    "ax[2].set_xticks(xticks)\n",
    "ax[2].set_yticks(yticks)\n",
    "ax[2].grid(False)\n",
    "ax[2].set_xlabel('Easting [km]')\n",
    "ax[2].text((ax[2].get_xlim()[1] - ax[2].get_xlim()[0])*0.9 + ax[2].get_xlim()[0],\n",
    "           (ax[2].get_ylim()[1] - ax[2].get_ylim()[0])*0.07 + ax[2].get_ylim()[0], '(c)', fontweight='bold', fontsize=fontsize+2, \n",
    "           bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "# plot dummy points for legend\n",
    "xlim, ylim = ax[2].get_xlim(), ax[2].get_ylim()\n",
    "ax[2].plot(0, 0, 's', markersize=15, markerfacecolor=NDSI_cmap(1), markeredgecolor='k', linewidth=1, label='classified as snow')\n",
    "ax[2].plot(0, 0, 's', markersize=15, markerfacecolor='w', markeredgecolor='k', linewidth=1, label='classified as no snow')\n",
    "ax[2].legend(loc='lower left')\n",
    "# reset axis limits\n",
    "ax[2].set_xlim(xlim)\n",
    "ax[2].set_ylim(ylim)\n",
    "\n",
    "# annotations\n",
    "# snow\n",
    "ax[1].arrow(397, 6699, -2, 1, color=colors_classified[0], width=0.05)\n",
    "ax[1].text(397, 6698.9, 'snow', color=colors_classified[0], fontsize='large',\n",
    "           bbox=dict(facecolor='white', edgecolor=colors_classified[0], linewidth=3, pad=5))\n",
    "# firn\n",
    "ax[1].arrow(397, 6697.5, -1.8, 1.6, color=color_firn, width=0.05)\n",
    "ax[1].text(397, 6697.4, 'firn', color=color_firn, fontsize='large',\n",
    "           bbox=dict(facecolor='white', edgecolor=color_firn, linewidth=3, pad=5))\n",
    "# ice\n",
    "ax[1].arrow(397, 6696, -2, 1, color=colors_classified[2], width=0.05)\n",
    "ax[1].text(397, 6695.9, 'ice', color=colors_classified[2], fontsize='large',\n",
    "           bbox=dict(facecolor='white', edgecolor=colors_classified[2], linewidth=3, pad=5))\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----Save figure to file\n",
    "if save_figures:\n",
    "    fig_fn = os.path.join(figures_out_path, 'fig01_spectral_signatures_satellite_bands.png')\n",
    "    fig.savefig(fig_fn, facecolor='w', dpi=300, bbox_inches='tight')\n",
    "    print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593f951a",
   "metadata": {},
   "source": [
    "## Figure 2. Study sites - USGS Benchmark Glaciers & Emmons Glacier, WA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc8c5b-b16b-4abf-8cf4-307a8b589a30",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Load RGI data\n",
    "# path to RGI data\n",
    "rgi_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/GIS_data/RGI/'\n",
    "# RGI shapefile names\n",
    "rgi_fns = ['01_rgi60_Alaska/01_rgi60_Alaska.shp', \n",
    "           '02_rgi60_WesternCanadaUS/02_rgi60_WesternCanadaUS.shp']\n",
    "# load and combine rgis\n",
    "rgis = gpd.GeoDataFrame()\n",
    "for rgi_fn in rgi_fns:\n",
    "    rgi = gpd.read_file(rgi_path + rgi_fn)\n",
    "    rgis = pd.concat([rgis, rgi])\n",
    "rgis.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302aee73-a407-4cc7-9665-4f24cb931f4a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Count number of glaciers with areas < 1 km^2 (for manuscript)\n",
    "print('Total # of glaciers in RGI regions 1 and 2 = ', len(rgis))\n",
    "print('Number of glaciers with areas < 1 km^2 = ', len(rgis.loc[rgis['Area'] < 1]))\n",
    "print('Percentage of glaciers with areas < 1 km^2 = ', len(rgis.loc[rgis['Area'] < 1]) / len(rgis))\n",
    "print(' ')\n",
    "\n",
    "# -----Count total area of glaciers with areas < 1 km^2 with respect to total glacier area (for manuscript)\n",
    "lt1_area = np.sum(rgis.loc[rgis['Area'] < 1, 'Area'].values)\n",
    "gte1_area = np.sum(rgis.loc[rgis['Area'] >= 1, 'Area'].values)\n",
    "print('Total area of glaciers with area < 1 km^2 = ', lt1_area, ' km^2')\n",
    "print('Total area of glaciers with area >= 1 km^2 = ', gte1_area, ' km^2')\n",
    "print('Areal percentage of glaciers with area < 1 km^2 = ', lt1_area / gte1_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a209a3d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Define site specifics\n",
    "site_names = ['Gulkana', 'Wolverine', 'LemonCreek', 'Sperry', 'SouthCascade', 'Emmons']\n",
    "site_names_display = [x.replace('C', ' C') for x in site_names]\n",
    "text_labels = ['(a)', '(b)', '(c)', '(d)', '(e)', '(f)']\n",
    "\n",
    "# -----Set up figure\n",
    "fontsize = 14\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "ax = [fig.add_subplot(2, 3, 1), fig.add_subplot(2, 3, 2), fig.add_subplot(2, 3, 3),\n",
    "      fig.add_subplot(2, 4, 5), fig.add_subplot(2, 4, 6), fig.add_subplot(2, 4, 7), fig.add_subplot(2, 4, 8)]\n",
    "epsg_A = 32610    \n",
    "    \n",
    "# -----Loop through sites\n",
    "for i, (site_name, site_name_display, text_label) in enumerate(zip(site_names, site_names_display, text_labels)):\n",
    "    ### AOI\n",
    "    # load file\n",
    "    if site_name=='Emmons':\n",
    "        AOI_fn = glob.glob(os.path.join(study_sites_path, site_name, 'AOIs', site_name + '_RGI_outline.shp'))[0]\n",
    "    else:\n",
    "        AOI_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/write-ups/CH1_snow_cover_mapping_methods_manuscript/Aberle_et_al_dataset_submission'\n",
    "        AOI_fn = sorted(glob.glob(os.path.join(AOI_path, site_name, 'AOIs', site_name + '_Glacier_Boundaries_*.shp')))[-1]\n",
    "    AOI = gpd.read_file(AOI_fn)\n",
    "    AOI_WGS = AOI.to_crs(4326)\n",
    "    # solve for optimal UTM zone\n",
    "    AOI_centroid = [AOI_WGS.geometry[0].centroid.xy[0][0],\n",
    "                    AOI_WGS.geometry[0].centroid.xy[1][0]]\n",
    "    epsg_UTM = f.convert_wgs_to_utm(AOI_centroid[0], AOI_centroid[1])\n",
    "    # reproject\n",
    "    AOI_UTM = AOI_WGS.to_crs(epsg_UTM)\n",
    "    AOI_A = AOI.to_crs(epsg_A)\n",
    "    ### DEM\n",
    "    if site_name=='Emmons':\n",
    "        DEM_fn =  glob.glob(os.path.join(study_sites_path, site_name, 'DEMs', site_name + '*NASADEM*.tif'))[0]\n",
    "    else:\n",
    "        DEM_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/GIS_data/USGS/DEMs/' + site_name\n",
    "        DEM_fn =  sorted(glob.glob(os.path.join(DEM_path, site_name + '*_DEM.tif')))[-1]\n",
    "    DEM = xr.open_dataset(DEM_fn)\n",
    "    DEM = DEM.rename({'band_data': 'elevation'})\n",
    "    # reproject \n",
    "    DEM = DEM.rio.reproject(str('EPSG:'+epsg_UTM))\n",
    "    ### Plot\n",
    "    # A) Study sites map\n",
    "    ax[0].plot(AOI_A.geometry[0].centroid.xy[0][0], AOI_A.geometry[0].centroid.xy[1][0], '.', markersize=5)\n",
    "    ax[0].text(AOI_A.geometry[0].centroid.xy[0][0], AOI_A.geometry[0].centroid.xy[1][0],\n",
    "               text_labels[i].replace('(','').replace(')',''), bbox=dict(facecolor='white', edgecolor='black', pad=5))\n",
    "    # Individual glacier plot\n",
    "    DEM_im = ax[i+1].imshow(DEM.elevation.data[0], extent=(np.min(DEM.x.data), np.max(DEM.x.data), np.min(DEM.y.data), np.max(DEM.y.data)),\n",
    "                            cmap=cmap_elev, clim=(0,4000))\n",
    "    AOI_UTM.plot(ax=ax[i+1], edgecolor='k', facecolor='none', linewidth=2)\n",
    "    if AOI_UTM.geometry[0].geom_type=='MultiPolygon':\n",
    "        xmin_AOI = np.min([np.min(geom.exterior.coords.xy[0]) for geom in AOI_UTM.geometry[0].geoms])\n",
    "        xmax_AOI = np.max([np.max(geom.exterior.coords.xy[0]) for geom in AOI_UTM.geometry[0].geoms])\n",
    "        ymin_AOI = np.min([np.min(geom.exterior.coords.xy[1]) for geom in AOI_UTM.geometry[0].geoms])\n",
    "        ymax_AOI = np.max([np.max(geom.exterior.coords.xy[1]) for geom in AOI_UTM.geometry[0].geoms])      \n",
    "    else:\n",
    "        xmin_AOI = np.min(AOI_UTM.geometry[0].exterior.coords.xy[0])\n",
    "        xmax_AOI = np.max(AOI_UTM.geometry[0].exterior.coords.xy[0])\n",
    "        ymin_AOI = np.min(AOI_UTM.geometry[0].exterior.coords.xy[1])\n",
    "        ymax_AOI = np.max(AOI_UTM.geometry[0].exterior.coords.xy[1])  \n",
    "    xmin = xmin_AOI - 0.05*(xmax_AOI - xmin_AOI)\n",
    "    xmax = xmax_AOI + 0.05*(xmax_AOI - xmin_AOI)\n",
    "    ymin = ymin_AOI - 0.05*(ymax_AOI - ymin_AOI)\n",
    "    ymax = ymax_AOI + 0.05*(ymax_AOI - ymin_AOI) \n",
    "    # change x and y tick labels to km\n",
    "    if (i < 3) or (i==5):\n",
    "        ax[i+1].set_xticks(np.arange(np.round(xmin,-3), np.round(xmax,-3), 2e3))\n",
    "        ax[i+1].set_yticks(np.arange(np.round(ymin,-3), np.round(ymax,-3), 2e3)) \n",
    "    elif i==3:\n",
    "        ax[i+1].set_xticks([296500, 297000])\n",
    "        ax[i+1].set_yticks([5389000, 5389500])\n",
    "    else:\n",
    "        ax[i+1].set_xticks(np.arange(np.round(xmin,-3), np.round(xmax,-3), 1e3))\n",
    "        ax[i+1].set_yticks(np.arange(np.round(ymin,-3), np.round(ymax,-3), 1e3)) \n",
    "    if i==3:\n",
    "        ax[i+1].set_xticklabels([str(x/1e3) for x in ax[i+1].get_xticks()])\n",
    "        ax[i+1].set_yticklabels([str(y/1e3) for y in ax[i+1].get_yticks()])\n",
    "    else:\n",
    "        ax[i+1].set_xticklabels([str(int(x/1e3)) for x in ax[i+1].get_xticks()])\n",
    "        ax[i+1].set_yticklabels([str(int(y/1e3)) for y in ax[i+1].get_yticks()])        \n",
    "    ax[i+1].set_xlim(xmin, xmax)\n",
    "    ax[i+1].set_ylim(ymin, ymax)\n",
    "    ax[i+1].set_title(text_label + ' ' + site_name_display + ' Glacier')\n",
    "    ax[i+1].grid()\n",
    "    # add axes labels\n",
    "    if (i==1) or (i==3):\n",
    "        ax[i].set_ylabel('Northing [km]')\n",
    "    if i > 1:\n",
    "        ax[i+1].set_xlabel('Easting [km]')\n",
    "\n",
    "# A: study sites map\n",
    "ax[0].set_xlim(-2000000, 1500000)\n",
    "ax[0].set_ylim(5000000, 7800000)\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "cx.add_basemap(ax[0], crs='EPSG:'+str(epsg_A), source=cx.providers.Esri.WorldGrayCanvas, attribution=False)\n",
    "rgis_reproj = rgis.to_crs('EPSG:'+str(epsg_A))\n",
    "rgis_reproj.plot(ax=ax[0], facecolor=colors_classified[2], edgecolor=colors_classified[2])\n",
    "# add colorbar for elevations at bottom of figure\n",
    "fig.subplots_adjust(bottom=0.1)\n",
    "cbar_ax = fig.add_axes([0.35, 0.03, 0.3, 0.02])\n",
    "fig.colorbar(DEM_im, cax=cbar_ax, orientation='horizontal', label='Elevation [m]')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if save_figures:\n",
    "    fig_fn = os.path.join(figures_out_path, 'fig02_study_sites_elevations.png')\n",
    "    fig.savefig(fig_fn, dpi=300, facecolor='white', edgecolor='none', bbox_inches='tight')\n",
    "    print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b81f98-4a3d-4cf6-9253-98f9877d1fae",
   "metadata": {},
   "source": [
    "## Figure 3. Example images and training points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d2762-3292-461b-ac21-ce6a6e1578a3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "\n",
    "data_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/snow_cover_mapping/manually-classified-points/Sentinel-2/'\n",
    "im_dates = ['20210615', '20210802']\n",
    "site_name = 'Gulkana'\n",
    "\n",
    "# load glacier outlines from file\n",
    "study_sites_path = '/Users/raineyaberle/Google Drive/My Drive/Research/CryoGARS-Glaciology/Advising/student-research/Alexandra-Friel/snow_cover_mapping_application/study-sites/'\n",
    "AOI_fn = study_sites_path + 'Gulkana/AOIs/Gulkana_USGS_glacier_outline_2021.shp'\n",
    "AOI = gpd.read_file(AOI_fn)\n",
    "AOI_color = 'w'\n",
    "\n",
    "# grab Sentinel-2_SR image file name\n",
    "os.chdir(data_path)\n",
    "im_fns = ['Gulkana_20210615T211519_20210615T211514_T06VWR.tif', 'Gulkana_20210802T212531_20210802T212655_T06VWR.tif']\n",
    "\n",
    "# set up figure\n",
    "fontsize = 16\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16,8))\n",
    "text_labels = ['(a)', '(b)']\n",
    "\n",
    "# iterate over images\n",
    "for i, (im_date, im_fn) in enumerate(zip(im_dates, im_fns)):\n",
    "    im = rxr.open_rasterio(im_fn)\n",
    "    # grab CRS\n",
    "    crs = im.rio.crs.to_epsg()\n",
    "    im = im / 1e4\n",
    "    AOI = AOI.to_crs(crs)\n",
    "\n",
    "    # grab validation point names\n",
    "    data_pts_fns = sorted(glob.glob(site_name + '*' + im_date + '*.shp'))\n",
    "\n",
    "    # Plot\n",
    "    # RGB image\n",
    "    ax[i].imshow(np.dstack([im.data[3], im.data[2], im.data[1]]), \n",
    "                           extent=(np.min(im.x.data)/1e3, np.max(im.x.data)/1e3,\n",
    "                                   np.min(im.y.data)/1e3, np.max(im.y.data)/1e3))\n",
    "    # Glacier boundary\n",
    "    if type(AOI.geometry[0])==MultiPolygon:\n",
    "        for j, geom in enumerate(AOI.geometry[0].geoms):\n",
    "            if j==0:\n",
    "                ax[i].plot(np.divide(geom.exterior.coords.xy[0], 1e3), np.divide(geom.exterior.coords.xy[1], 1e3), \n",
    "                           '-', linewidth=2, color=AOI_color, label='Glacier boundaries')\n",
    "            else:\n",
    "                ax[i].plot(np.divide(geom.exterior.coords.xy[0], 1e3), np.divide(geom.exterior.coords.xy[1], 1e3), \n",
    "                           '-', linewidth=2, color=AOI_color, label='_nolegend')\n",
    "    else:\n",
    "        ax[i].plot(np.divide(AOI.geometry[0].exterior.coords.xy[0], 1e3), np.divide(AOI.geometry[0].exterior.coords.xy[1], 1e3),\n",
    "                   '-', color=AOI_color, label='Glacier boundaries')\n",
    "    # Classified points\n",
    "    for data_pts_fn in data_pts_fns:\n",
    "        data_pts = gpd.read_file(data_pts_fn)\n",
    "        data_pts = data_pts.to_crs(crs)\n",
    "        if 'snow.' in data_pts_fn:\n",
    "            color = colors_classified[0]\n",
    "            label = 'Snow'\n",
    "        elif 'snow-shadowed' in data_pts_fn:\n",
    "            color = colors_classified[1]\n",
    "            label = 'Shadowed snow'\n",
    "        elif 'ice' in data_pts_fn:\n",
    "            color = colors_classified[2]\n",
    "            label = 'Ice'\n",
    "        elif 'rock' in data_pts_fn:\n",
    "            color = colors_classified[3]\n",
    "            label = 'Rock'\n",
    "        elif 'water' in data_pts_fn:\n",
    "            color = colors_classified[4]\n",
    "            label = 'Water'\n",
    "        for j, point in enumerate(data_pts['geometry'].values):\n",
    "            if j==0:\n",
    "                legend_label = label\n",
    "            else:\n",
    "                legend_label = '_nolegend'\n",
    "            ax[i].plot(np.divide(point.geoms[0].coords.xy[0], 1e3), np.divide(point.geoms[0].coords.xy[1], 1e3), \n",
    "                       '.', color=color, markersize=4, label=legend_label)\n",
    "\n",
    "        ax[i].set_xlim(575, 583)\n",
    "        ax[i].set_ylim(7014, 7020.5)\n",
    "    # text label\n",
    "    ax[i].text((ax[i].get_xlim()[1] - ax[i].get_xlim()[0]) * 0.05 + ax[i].get_xlim()[0], \n",
    "               (ax[i].get_ylim()[1] - ax[i].get_ylim()[0]) * 0.9 + ax[i].get_ylim()[0],\n",
    "               text_labels[i], fontsize=fontsize+2, fontweight='bold', bbox=dict(facecolor='white', edgecolor='none', pad=5))\n",
    "    # axes ticks\n",
    "    ax[i].set_xticks(np.arange(576, 583, 2))\n",
    "    ax[i].set_yticks(np.arange(7014, 7021, 2))\n",
    "        \n",
    "ax[0].set_xlabel('Easting [km]')\n",
    "ax[0].set_ylabel('Northing [km]')\n",
    "ax[1].set_xlabel('Easting [km]')\n",
    "legend = ax[1].legend(loc='lower right', bbox_to_anchor=[0.5, 1.05, 0.2, 0.2], markerscale=5, ncols=5, frameon=1)\n",
    "for line in legend.get_lines():\n",
    "    line.set_linewidth(3.0)\n",
    "frame = legend.get_frame()\n",
    "frame.set_color('#bdbdbd')\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "if save_figures:\n",
    "    fig_fn = os.path.join(figures_out_path, 'fig03_example_training_points.png')\n",
    "    fig.savefig(fig_fn, dpi=250, facecolor='white', edgecolor='none', bbox_inches='tight')\n",
    "    print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d29c3a2",
   "metadata": {},
   "source": [
    "## Figure 4. Methods workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00177f0c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "font_size = 32\n",
    "save_figures = 1\n",
    "\n",
    "# -----Load dataset dictionary\n",
    "with open(base_path + 'inputs-outputs/datasets_characteristics.json') as fn:\n",
    "    dataset_dict = json.load(fn)\n",
    "dataset = 'PlanetScope'\n",
    "\n",
    "# -----Image settings\n",
    "# site name\n",
    "site_name = 'SouthCascade'\n",
    "# create colormap for classified image\n",
    "cmp = ListedColormap(colors_classified)\n",
    "\n",
    "# -----Load AOI as gpd.GeoDataFrame\n",
    "AOI_fn = study_sites_path + site_name + '/AOIs/' + site_name + '_USGS_*.shp'\n",
    "AOI_fn = glob.glob(AOI_fn)[0]\n",
    "AOI = gpd.read_file(AOI_fn)\n",
    "# reproject the AOI to WGS to solve for the optimal UTM zone\n",
    "AOI_WGS = AOI.to_crs(4326)\n",
    "AOI_WGS_centroid = [AOI_WGS.geometry[0].centroid.xy[0][0],\n",
    "                    AOI_WGS.geometry[0].centroid.xy[1][0]]\n",
    "epsg_UTM = f.convert_wgs_to_utm(AOI_WGS_centroid[0], AOI_WGS_centroid[1])\n",
    "# reproject AOI to UTM\n",
    "AOI_UTM = AOI.to_crs(str(epsg_UTM))\n",
    "\n",
    "# -----Load DEM as Xarray DataSet\n",
    "DEM_fn = study_sites_path + site_name + '/DEMs/' + site_name + '*_DEM*.tif'\n",
    "# load DEM as xarray DataSet\n",
    "DEM_fn = glob.glob(DEM_fn)[0]\n",
    "DEM = xr.open_dataset(DEM_fn)\n",
    "DEM = DEM.rename({'band_data': 'elevation'})\n",
    "# reproject the DEM to the optimal UTM zone\n",
    "DEM = DEM.rio.reproject(str('EPSG:'+epsg_UTM))\n",
    "# remove unnecessary data (possible extra bands from ArcticDEM or other DEM)\n",
    "if len(np.shape(DEM.elevation.data))>2:\n",
    "    DEM['elevation'] = DEM.elevation[0]\n",
    "    \n",
    "# -----1. Raw image\n",
    "im_path = study_sites_path + site_name + '/imagery/PlanetScope/mosaics/'\n",
    "im_fn = '20210924_18.tif'\n",
    "im = xr.open_dataset(im_path + im_fn)\n",
    "# determine image date from image mosaic file name\n",
    "im_date = im_fn[0:4] + '-' + im_fn[4:6] + '-' + im_fn[6:8] + 'T' + im_fn[9:11] + ':00:00'\n",
    "im_dt = np.datetime64(im_date)\n",
    "xmin, xmax, ymin, ymax = np.min(im.x.data), np.max(im.x.data), np.min(im.y.data), np.max(im.y.data)\n",
    "# plot\n",
    "fig1, ax1 = plt.subplots(figsize=(8,8))\n",
    "ax1.imshow(np.dstack([im.band_data.data[2]/1e4, im.band_data.data[1]/1e4, im.band_data.data[0]/1e4]), \n",
    "           extent=(xmin, xmax, ymin, ymax))\n",
    "AOI.plot(ax=ax1, facecolor='none', edgecolor='k', linewidth=3)\n",
    "ax1.set_xlim(xmin, xmax)\n",
    "ax1.set_ylim(ymin, ymax)\n",
    "ax1.axis('off')\n",
    "\n",
    "# -----2. Adjusted image\n",
    "polygon_top, polygon_bottom = f.create_aoi_elev_polys(AOI_UTM, DEM)\n",
    "im_adj, im_adj_method = f.planetscope_adjust_image_radiometry(im, im_dt, polygon_top, polygon_bottom, dataset_dict, skip_clipped=False)\n",
    "# plot\n",
    "fig2, ax2 = plt.subplots(figsize=(8,8))\n",
    "ax2.imshow(np.dstack([im_adj.Red.data[0], im_adj.Green.data[0], im_adj.Blue.data[0]]), \n",
    "           extent=(xmin, xmax, ymin, ymax))\n",
    "AOI_UTM.plot(ax=ax2, facecolor='none', edgecolor='k', linewidth=3)\n",
    "ax2.set_xlim(xmin, xmax)\n",
    "ax2.set_ylim(ymin, ymax)\n",
    "ax2.axis('off')\n",
    "\n",
    "# -----3. Classified image\n",
    "im_classified_path = study_sites_path + site_name + '/imagery/classified/'\n",
    "im_classified_fn = '20210924T180000_SouthCascade_PlanetScope_classified.nc'\n",
    "im_classified = xr.open_dataset(im_classified_path + im_classified_fn)\n",
    "# remove no data values\n",
    "im_classified = xr.where(im_classified==-9999, np.nan, im_classified)\n",
    "# plot\n",
    "fig3, ax3 = plt.subplots(figsize=(8,8))\n",
    "plt.rcParams.update({'font.size':font_size, 'font.sans-serif':'Arial'})\n",
    "ax3.imshow(im_classified.classified.data[0], cmap=cmp, vmin=1, vmax=5,\n",
    "           extent=(xmin, xmax, ymin, ymax))\n",
    "AOI.plot(ax=ax3, facecolor='none', edgecolor='k', linewidth=3)\n",
    "# plot dummy points for legend\n",
    "ax3.scatter(0, 0, marker='s', color=colors_classified[0], s=300, label='snow')\n",
    "ax3.scatter(0, 0, marker='s', color=colors_classified[1], s=300, label='shadowed snow')\n",
    "ax3.scatter(0, 0, marker='s', color=colors_classified[2], s=300, label='ice')\n",
    "ax3.scatter(0, 0, marker='s', color=colors_classified[3], s=300, label='rock')\n",
    "ax3.scatter(0, 0, marker='s', color=colors_classified[4], s=300, label='water')\n",
    "ax3.set_xlim(xmin, xmax+300)\n",
    "ax3.set_ylim(ymin, ymax)\n",
    "ax3.legend(loc='center right', bbox_to_anchor=[1.3, 0.7, 0.2, 0.2])\n",
    "ax3.axis('off')\n",
    "\n",
    "# -----4. Snow edges\n",
    "# create binary snow matrix\n",
    "im_binary = xr.where(im_classified.classified.data[0] <=2, 1, 0)\n",
    "# Find contours at a constant value of 0.5 (between 0 and 1)\n",
    "contours = find_contours(im_binary, 0.5)\n",
    "# convert contour points to image coordinates\n",
    "contours_coords = []\n",
    "for contour in contours: \n",
    "    ix = np.round(contour[:,1]).astype(int)\n",
    "    iy = np.round(contour[:,0]).astype(int)\n",
    "    coords = (im_adj.isel(x=ix, y=iy).x.data, # image x coordinates\n",
    "              im_adj.isel(x=ix, y=iy).y.data) # image y coordinates\n",
    "    # zip points together\n",
    "    xy = list(zip([x for x in coords[0]], \n",
    "                  [y for y in coords[1]]))\n",
    "    contours_coords = contours_coords + [xy]\n",
    "# plot\n",
    "fig4, ax4 = plt.subplots(figsize=(8,8))\n",
    "plt.rcParams.update({'font.size':font_size, 'font.sans-serif':'Arial'})\n",
    "binary_plt = ax4.imshow(im_binary, cmap='Greys')\n",
    "for i, contour in list(zip(np.arange(0,len(contours)), contours)):\n",
    "    if i==0:\n",
    "        plt.plot(contour[:,1], contour[:,0], '-c', label='edges', linewidth=2)\n",
    "    else:\n",
    "        plt.plot(contour[:,1], contour[:,0], '-c', label='_nolegend', linewidth=2)\n",
    "# plot dummy points for legend\n",
    "ax4.scatter(np.array([-10, -9]),np.array([-10, -9]), edgecolor='k', facecolor='k', s=100, label='snow')\n",
    "ax4.scatter(np.array([-10, -9]),np.array([-10, -9]), edgecolor='k', facecolor='w', s=100, label='no snow')\n",
    "ax4.set_xlim(0,len(im.x.data)+300)\n",
    "ax4.set_ylim(len(im.y.data), 0)\n",
    "ax4.legend(loc='upper right', bbox_to_anchor=[0.9, 0.8, 0.2, 0.2])\n",
    "ax4.axis('off')\n",
    "\n",
    "# -----5. Snow line\n",
    "snowlines_fn = study_sites_path + site_name + '/imagery/snowlines/20210924T180000_SouthCascade_PlanetScope_snowline.csv'\n",
    "snowlines = pd.read_csv(snowlines_fn)\n",
    "snowlines_X = snowlines.snowlines_coords_X.apply(literal_eval)[0]\n",
    "snowlines_Y = snowlines.snowlines_coords_Y.apply(literal_eval)[0]\n",
    "\n",
    "# plot\n",
    "fig5, ax5 = plt.subplots(figsize=(8,8))\n",
    "plt.rcParams.update({'font.size':font_size, 'font.sans-serif':'Arial'})\n",
    "binary_plt = ax5.imshow(im_binary, \n",
    "                        extent=(xmin, xmax, ymin, ymax),\n",
    "                        cmap='Greys')\n",
    "ax5.plot(snowlines_X, snowlines_Y, '.m', label='_nolegend', markersize=10)\n",
    "ax5.plot(-20, -20, 'm', label='snowline', linewidth=5)\n",
    "# plot dummy points for legend\n",
    "ax5.scatter(np.array([-10, -9]),np.array([-10, -9]), edgecolor='k', facecolor='k', s=100, label='snow')\n",
    "ax5.scatter(np.array([-10, -9]),np.array([-10, -9]), edgecolor='k', facecolor='w', s=100, label='no snow')\n",
    "ax5.set_xlim(xmin, xmax+300)\n",
    "ax5.set_ylim(ymin, ymax)\n",
    "ax5.legend(loc='center right', bbox_to_anchor=[1.0, 0.6, 0.2, 0.2])\n",
    "ax5.axis('off')\n",
    "plt.show()\n",
    "\n",
    "if save_figures:\n",
    "    fig_fns = ['methods_workflow_1.png', 'methods_workflow_2.png', 'methods_workflow_3.png', \n",
    "               'methods_workflow_4.png', 'methods_workflow_5.png']\n",
    "    for fig_fn, fig in list(zip(fig_fns, [fig1, fig2, fig3, fig4, fig5])):\n",
    "        fig_fn = os.path.join(figures_out_path, fig_fn)\n",
    "        fig.savefig(fig_fn, dpi=300, facecolor='white', edgecolor='none', bbox_inches='tight')\n",
    "        print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d78494-d9a0-4fc9-b39c-f07441646475",
   "metadata": {},
   "source": [
    "## Figures 5 and S3. Timseries of SCA, weekly median trends for SCA, AAR, and median snowline elevations for the USGS Benchmark Glaciers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8578ec6e-b1c6-449c-bade-37cfb9d94685",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Define site names\n",
    "site_names = ['Gulkana', 'Wolverine', 'LemonCreek', 'Sperry', 'SouthCascade']\n",
    "\n",
    "# -----Load and compile snowlines\n",
    "sl_ests_full = pd.DataFrame()\n",
    "for site_name in site_names:\n",
    "\n",
    "    print(site_name)\n",
    "    \n",
    "    # load estimated snowlines  \n",
    "    sl_est_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/write-ups/CH1_snow_cover_mapping_methods_manuscript/Aberle_et_al_dataset_submission/' + site_name + '/imagery/snowlines/'\n",
    "    sl_est_fns = glob.glob(os.path.join(sl_est_path, '*snowline.csv'))\n",
    "    sl_ests = pd.DataFrame()\n",
    "    for sl_est_fn in sl_est_fns:\n",
    "        sl_est = pd.read_csv(sl_est_fn)\n",
    "        sl_ests = pd.concat([sl_ests, sl_est])\n",
    "    sl_ests.reset_index(drop=True, inplace=True)\n",
    "    sl_ests['datetime'] = pd.to_datetime(sl_ests['datetime'], format='mixed')\n",
    "\n",
    "    # concatenate to full df\n",
    "    sl_ests_full = pd.concat([sl_ests_full, sl_ests])\n",
    "\n",
    "# reset index, add year and week columns\n",
    "sl_ests_full.reset_index(drop=True, inplace=True)\n",
    "sl_ests_full['Year'] = sl_ests_full['datetime'].dt.year\n",
    "sl_ests_full['Week'] = sl_ests_full['datetime'].dt.isocalendar().week\n",
    "sl_ests_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c15fcf-cc5d-4bcb-b942-39d0c42b4a66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Settings and display parameters\n",
    "site_names = ['Gulkana', 'Wolverine', 'LemonCreek', 'Sperry', 'SouthCascade']\n",
    "site_names_display = [x.replace('C', ' C') for x in site_names]\n",
    "text_labels1 = ['(a)', '(b)', '(c)', '(d)', '(e)']\n",
    "text_labels2 = [['(a)', '(b)', '(c)'], ['(d)', '(e)', '(f)'], ['(g)', '(h)', '(i)'], ['(j)', '(k)', '(l)'], ['(m)', '(n)', '(o)']]\n",
    "\n",
    "# -----Set up figures\n",
    "fontsize=12\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "# time series: SCA\n",
    "fig1, ax1 = plt.subplots(5, 1, figsize=(10, 12))\n",
    "# weekly median trends: SCA, AAR, median snowline elevation\n",
    "fig2, ax2 = plt.subplots(5, 3, figsize=(10, 12))\n",
    "alpha = 0.9\n",
    "color_sca, color_aar, color_snowline = colors_classified[0], '#FFC107', '#FB65FB'\n",
    "\n",
    "# -----Loop through sites\n",
    "for site_name, site_name_display, text_label, i in list(zip(site_names, site_names_display, text_labels1, np.arange(0,len(site_names)))):\n",
    "    \n",
    "    print(site_name)\n",
    "    \n",
    "    # subset snowlines\n",
    "    sl_ests = sl_ests_full.loc[sl_ests_full['site_name']==site_name].reset_index(drop=True)\n",
    "    sl_ests = sl_ests.sort_values(by='datetime')\n",
    "        \n",
    "    # -----Define axis limits\n",
    "    xmin, xmax = np.datetime64('2013-05-01T00:00:00'), np.datetime64('2023-12-01T00:00:00')\n",
    "    sl_elev_median_min = np.nanmin(sl_ests['snowline_elevs_median_m'])\n",
    "    sl_elev_median_max = np.nanmax(sl_ests['snowline_elevs_median_m'])\n",
    "    yrange1 = [0, np.nanmax(sl_ests['SCA_m2']) * 1e-6 * 1.1]\n",
    "    yrange2_sca = yrange1\n",
    "    yrange2_aar = [-0.1, 1.1]\n",
    "    min_snowline_elev, max_snowline_elev = np.nanmin(sl_ests['snowline_elevs_median_m']), np.nanmax(sl_ests['snowline_elevs_median_m'])\n",
    "    yrange2_snowline = [min_snowline_elev * 0.97, max_snowline_elev * 1.02]\n",
    "\n",
    "    # -----Plot light grey boxes where no observations exist on SCA plots\n",
    "    years = np.arange(2012, 2023, step=1)\n",
    "    for year in years:\n",
    "        min_date, max_date = np.datetime64(str(year) + '-11-01'), np.datetime64(str(year+1) + '-05-01')\n",
    "        rect = matplotlib.patches.Rectangle((min_date, yrange1[0]), width=max_date-min_date, height=yrange1[1]-yrange1[0], color='#d9d9d9')\n",
    "        ax1[i].add_patch(rect)\n",
    "        \n",
    "    # -----Plot SCA time series\n",
    "    # PlanetScope\n",
    "    ax1[i].plot(sl_ests['datetime'].loc[sl_ests['dataset']=='PlanetScope'], \n",
    "                np.divide(sl_ests['SCA_m2'].loc[sl_ests['dataset']=='PlanetScope'].values, 1e6), \n",
    "                '.', markeredgecolor='w', markerfacecolor=color_PlanetScope, \n",
    "                alpha=alpha, markersize=8, markeredgewidth=1, label='PlanetScope')\n",
    "    # Sentinel-2 SR\n",
    "    ax1[i].plot(sl_ests['datetime'].loc[sl_ests['dataset']=='Sentinel-2_SR'], \n",
    "                np.divide(sl_ests['SCA_m2'].loc[sl_ests['dataset']=='Sentinel-2_SR'].values, 1e6), \n",
    "                'D', markeredgecolor='w', markerfacecolor=color_Sentinel2, \n",
    "                alpha=alpha, markersize=4, markeredgewidth=1, label='Sentinel-2 SR')\n",
    "    # Sentinel-2 TOA\n",
    "    ax1[i].plot(sl_ests['datetime'].loc[sl_ests['dataset']=='Sentinel-2_TOA'], \n",
    "                np.divide(sl_ests['SCA_m2'].loc[sl_ests['dataset']=='Sentinel-2_TOA'].values, 1e6), \n",
    "                'D', markeredgecolor=color_Sentinel2, markerfacecolor='None', \n",
    "                alpha=alpha, markersize=3, markeredgewidth=1.2, label='Sentinel-2 TOA')  \n",
    "    # Landsat\n",
    "    ax1[i].plot(sl_ests['datetime'].loc[sl_ests['dataset']=='Landsat'], \n",
    "                np.divide(sl_ests['SCA_m2'].loc[sl_ests['dataset']=='Landsat'].values, 1e6), \n",
    "                '^', markeredgecolor='w', markerfacecolor=color_Landsat, \n",
    "                alpha=alpha, markersize=6, markeredgewidth=1, label='Landsat')       \n",
    "\n",
    "    # -----Plot max glacier area on SCA plots\n",
    "    AOI_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/write-ups/CH1_snow_cover_mapping_methods_manuscript/Aberle_et_al_dataset_submission/' + site_name + '/AOIs/'\n",
    "    AOI_fn = sorted(glob.glob(os.path.join(AOI_path, site_name + '_Glacier_Boundaries_*.shp')))[0]\n",
    "    AOI = gpd.read_file(AOI_fn)\n",
    "    # ax1[i].plot([xmin, xmax], [AOI.geometry[0].area / 1e6, AOI.geometry[0].area / 1e6], '--', color='grey')\n",
    "    ax2[i,0].plot([15, 45], [AOI.geometry[0].area / 1e6, AOI.geometry[0].area / 1e6], '--', color='grey')\n",
    "\n",
    "    # -----Plot min and max elevations on snowline plots\n",
    "    ax2[i,2].plot([15, 45], [min_snowline_elev, min_snowline_elev], '--', color='grey')\n",
    "    ax2[i,2].plot([15, 45], [max_snowline_elev, max_snowline_elev], '--', color='grey')\n",
    "    \n",
    "    # -----Adjust axes display settings\n",
    "    ax1[i].set_xlim(xmin, xmax)\n",
    "    ax1[i].set_ylim(yrange1[0], yrange1[1])\n",
    "    ax1[i].set_xticks([np.datetime64(str(year)+'-01-01') for year in np.arange(2013, 2024)])\n",
    "    if site_name == site_names[-1]:\n",
    "        ax1[i].set_xticklabels([str(year) for year in np.arange(2013, 2024)])\n",
    "    else:\n",
    "        ax1[i].set_xticklabels([])\n",
    "    ax1[i].grid(True)\n",
    "    ax1[i].text(np.datetime64('2013-02-01'),\n",
    "                (ax1[i].get_ylim()[1] - ax1[i].get_ylim()[0]) * 0.06 + ax1[i].get_ylim()[0],\n",
    "                text_label + ' ' + site_name_display + ' (N=' + str(len(sl_ests)) + ')',\n",
    "                bbox=dict(facecolor='white', edgecolor='black', pad=5))\n",
    "    if site_name=='SouthCascade':\n",
    "        ax1[i].set_yticks(np.arange(0, 2.4, step=0.4))\n",
    "    if i==2:\n",
    "        ax1[i].set_ylabel('Snow-covered area [km$^2$]', fontsize=fontsize+2)\n",
    "    ax2[i,0].set_ylabel(site_name_display)\n",
    "        \n",
    "    # -----Calculate median and interquartile ranges for weekly trends\n",
    "    q1, q3 = 0.25, 0.75 # define quartiles\n",
    "    # calculate weekly trends using only Sentinel-2 snowlines\n",
    "    sl_ests_noPS = sl_ests.loc[sl_ests['dataset']!='PlanetScope']   \n",
    "    for ax, column, color, yrange, ylabel in list(zip([ax2[i,0], ax2[i,1], ax2[i,2]], \n",
    "                                                      ['SCA_m2', 'AAR', 'snowline_elevs_median_m'],\n",
    "                                                      [color_sca, color_aar, color_snowline],\n",
    "                                                      [yrange2_sca, yrange2_aar, yrange2_snowline],\n",
    "                                                      ['SCA [km$^2$]', 'Transient AAR', 'Median snowline altitude [m]'])):\n",
    "        weekly = sl_ests_noPS.groupby(by='Week')[column].agg(['median', lambda x: x.quantile(q1), lambda x: x.quantile(q3)])\n",
    "        weekly.columns = ['Median', 'Q1', 'Q3'] # Rename the columns for clarity\n",
    "        weekly.index = weekly.index.astype(float)\n",
    "        # plot\n",
    "        if column=='SCA_m2':\n",
    "            weekly = weekly / 1e6  # convert from m^2 to km^2\n",
    "        ax.fill_between(weekly.index, weekly['Q1'], weekly['Q3'].values, color=color, alpha=0.5)\n",
    "        ax.plot(weekly.index, weekly['Median'], color=color, linewidth=2)\n",
    "        ax.grid(True)\n",
    "        # adjust x axis\n",
    "        ax.set_xlim(15, 45)\n",
    "        ax.set_xticks([18, 31, 44])\n",
    "        ax.set_xticklabels([])\n",
    "        if i==len(site_names)-1:\n",
    "            ax.set_xticklabels(['May', 'Aug', 'Nov'])\n",
    "        # adjust y axis\n",
    "        ax.set_ylim(yrange[0], yrange[1])\n",
    "        # if column=='SCA_m2':\n",
    "        #     ax.set_xticks(\n",
    "        if column=='AAR':\n",
    "            ax.set_yticks([0, 0.25, 0.5, 0.75, 1])\n",
    "            ax.set_yticklabels(['0.0', '', '0.5', '', '1.0'])\n",
    "        if i==0:\n",
    "            ax.set_title(ylabel)\n",
    "\n",
    "# -----Add text labels to figure 2\n",
    "for i in np.arange(0,len(site_names)):\n",
    "    for j in np.arange(0, 3):\n",
    "        ax2[i,j].text((ax2[i,j].get_xlim()[1] - ax2[i,j].get_xlim()[0]) * 0.05 + ax2[i,j].get_xlim()[0],\n",
    "                      (ax2[i,j].get_ylim()[1] - ax2[i,j].get_ylim()[0]) * 0.15 + ax2[i,j].get_ylim()[0],\n",
    "                      text_labels2[i][j], fontweight='bold', fontsize=fontsize+2,\n",
    "                      bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "\n",
    "# -----Add legend to figure 1\n",
    "ax1[0].legend(loc='center', bbox_to_anchor=(0.5, 1.1), handletextpad=0.1, labelspacing=0.5, markerscale=2, ncol=4)\n",
    "\n",
    "fig1.tight_layout()\n",
    "fig2.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----Save figures\n",
    "if save_figures:\n",
    "    fig1_fn = os.path.join(figures_out_path, 'figS3_timeseries_SCA.png')\n",
    "    fig1.savefig(fig1_fn, dpi=300, facecolor='w', edgecolor='none', bbox_inches='tight')\n",
    "    print('figure 1 saved to file: ' + fig1_fn)\n",
    "    fig2_fn = os.path.join(figures_out_path, 'fig05_weekly_median_trends_no_PlanetScope.png')\n",
    "    fig2.savefig(fig2_fn, dpi=300, facecolor='w', edgecolor='none', bbox_inches='tight')\n",
    "    print('figure 2 saved to file: ' + fig2_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f220d2c-a1d4-4148-ad36-bb8dc82ff1b7",
   "metadata": {},
   "source": [
    "### Testing other plots... weekly medians excluding PlanetScope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21995485-c87e-4739-ab47-b14471a0de3d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(site_names), 1, figsize=(10, 18))\n",
    "cmap = matplotlib.cm.viridis\n",
    "\n",
    "for i, site_name in enumerate(site_names):\n",
    "    sl_ests = sl_ests_full.loc[sl_ests_full['site_name']==site_name].reset_index(drop=True)\n",
    "    sl_ests_noPS = sl_ests.loc[sl_ests['dataset']!='PlanetScope']\n",
    "    years = [2017, 2018, 2019, 2020, 2021, 2022]\n",
    "    for j, year in enumerate(years):\n",
    "        sl_ests_year = sl_ests_noPS.loc[sl_ests['Year']==year]\n",
    "        sl_ests_year_weekly_median = sl_ests_year.groupby('Week')['SCA_m2'].mean()\n",
    "        # ax[i].plot(sl_ests_year_weekly_median.index.values, np.divide(sl_ests_year_weekly_median.values, 1e6), \n",
    "        #         '.-', color=cmap(j/len(unique_years)), label=year)\n",
    "        # convert year and week to date for plotting\n",
    "        weekly_median = pd.DataFrame({'Week': sl_ests_year_weekly_median.index.values,\n",
    "                                      'SCA_m2': sl_ests_year_weekly_median.values,\n",
    "                                      'Formatted Date': year * 1000 + sl_ests_year_weekly_median.index * 10})\n",
    "        weekly_median['date'] = pd.to_datetime(weekly_median['Formatted Date'], format='%Y%W%w')\n",
    "        ax[i].plot(weekly_median['date'], np.divide(weekly_median['SCA_m2'], 1e6), '.-', color=cmap(j/len(years)))\n",
    "\n",
    "    ax[i].set_ylabel('Snow-covered area [km$^2$]')\n",
    "    ax[i].grid()\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3055b332-fbcd-442b-b0e1-4b0d8e420517",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot only USGS ELAs over time\n",
    "usgs_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/GIS_data/USGS/benchmarkGlacier_massBalance/'\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "col = plt.cm.viridis\n",
    "for i, site_name in enumerate(site_names):\n",
    "    usgs_fn = usgs_path + site_name+'/Output_'+site_name+'_Glacier_Wide_solutions_calibrated.csv'\n",
    "    usgs_file = pd.read_csv(usgs_fn)\n",
    "    ELA = usgs_file['ELA']\n",
    "    ELA_date = usgs_file['Ba_Date'].astype('datetime64[ns]')\n",
    "    plt.plot(ELA_date, ELA, '.-', color=col((i+1)/len(site_names)), label=site_name)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a62121d-8836-402f-8cce-576543d218c9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print stats for SCA\n",
    "\n",
    "# -----Loop through sites\n",
    "for site_name in site_names:\n",
    "    \n",
    "    print(site_name)\n",
    "    \n",
    "    # load estimated snow lines  \n",
    "    sl_est_fns = glob.glob(study_sites_path + site_name + '/imagery/snowlines/*snowline.csv')\n",
    "    sl_ests = gpd.GeoDataFrame()\n",
    "    for sl_est_fn in sl_est_fns:\n",
    "        sl_est = pd.read_csv(sl_est_fn)\n",
    "        sl_ests = pd.concat([sl_ests, sl_est])\n",
    "    sl_ests.reset_index(drop=True, inplace=True)\n",
    "    sl_ests['datetime'] = pd.to_datetime(sl_ests['datetime'], format='mixed')\n",
    "    \n",
    "    # identify min and max SCAs\n",
    "    imin = np.argwhere(sl_ests['SCA_m2'].values==np.nanmin(sl_ests['SCA_m2'].values))[0][0]\n",
    "    SCA_min, SCA_min_date = sl_ests.iloc[imin]['SCA_m2'], sl_ests.iloc[imin]['datetime']\n",
    "    print('Minimum SCA: ' + str(SCA_min) + ' m^2 on ' + str(SCA_min_date))\n",
    "    imax = np.argwhere(sl_ests['SCA_m2'].values==np.nanmax(sl_ests['SCA_m2'].values))[0][0]\n",
    "    SCA_max, SCA_max_date = sl_ests.iloc[imax]['SCA_m2'], sl_ests.iloc[imax]['datetime']\n",
    "    print('Maximum SCA: ' + str(SCA_max) + ' m^2 on ' + str(SCA_max_date)       )  \n",
    "    print(' ')\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce8d43f-fe2e-4b81-8c00-2918286386de",
   "metadata": {},
   "source": [
    "## Figure 6. Example shortcomings and successes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a77fd2-88f5-46fe-9c61-d50ae962bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Initialize GEE\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()\n",
    "\n",
    "# -----Query GEE for images\n",
    "im_dates = ['20190820T125044', '20190808T212744', '20190731T125046', '20190706T211748']\n",
    "im_site_names = ['Sperry', 'Gulkana', 'Sperry', 'Gulkana']\n",
    "im_datasets = ['Sentinel-2_TOA', 'Sentinel-2_SR', 'Sentinel-2_SR', 'Sentinel-2_SR']\n",
    "data_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/write-ups/CH1_snow_cover_mapping_methods_manuscript/Aberle_et_al_dataset_submission/'\n",
    "AOIs_UTM = []\n",
    "epsg_UTMs = []\n",
    "ims_ds = []\n",
    "for im_date, site_name, dataset in zip(im_dates, im_site_names, im_datasets):\n",
    "    # load AOI\n",
    "    AOI_path =  os.path.join(data_path, site_name, 'AOIs')\n",
    "    AOI_fn = sorted(glob.glob(os.path.join(AOI_path, '*Glacier_Boundaries_*.shp')))[-1]\n",
    "    AOI = gpd.read_file(AOI_fn)\n",
    "    AOI_WGS = AOI.to_crs('EPSG:4326')\n",
    "    # solve for optimal UTM zone\n",
    "    AOI_centroid = [AOI_WGS.geometry[0].centroid.xy[0][0],\n",
    "                    AOI_WGS.geometry[0].centroid.xy[1][0]]\n",
    "    epsg_UTM = f.convert_wgs_to_utm(AOI_centroid[0], AOI_centroid[1])\n",
    "    epsg_UTMs.append(epsg_UTM)\n",
    "    AOI_UTM = AOI_WGS.to_crs('EPSG:' + epsg_UTM)\n",
    "    AOIs_UTM.append(AOI_UTM)  # append to list of AOIs\n",
    "    \n",
    "    # query GEE for imagery\n",
    "    im_dt = np.datetime64(im_date[0:4] + '-' + im_date[4:6] + '-' + im_date[6:8])\n",
    "    date_start, date_end = str(im_dt), str(im_dt + np.timedelta64(1, 'D'))\n",
    "    month_start, month_end = 1, 12\n",
    "    cloud_cover_max = 100\n",
    "    mask_clouds=True\n",
    "    im_ds = f.query_gee_for_imagery(dataset_dict, dataset, AOI_UTM, date_start, date_end, month_start, month_end, \n",
    "                                    cloud_cover_max, mask_clouds)[0]\n",
    "    # reproject to UTM\n",
    "    im_ds = im_ds.rio.reproject('EPSG:' + epsg_UTM)\n",
    "    ims_ds.append(im_ds)  # append to list of images\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26980931-eba9-4714-8b5e-3c4e7df04d0c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Set up figure\n",
    "fontsize = 16\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig, ax = plt.subplots(3, 4, figsize=(14,8), gridspec_kw={'width_ratios':[1, 1, 2, 2], 'height_ratios':[1, 1, 0.5]})\n",
    "ax = ax.flatten()\n",
    "text_labels = [['(a)', '(b)'], ['(c)', '(d)'], ['(e)', '(f)'], ['(g)', '(h)']]\n",
    "\n",
    "data_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/write-ups/CH1_snow_cover_mapping_methods_manuscript/Aberle_et_al_dataset_submission/'\n",
    "\n",
    "# -----Iterate over images\n",
    "for i, (im_date, site_name, dataset, text_label, AOI, epsg_UTM, im_ds) in enumerate(zip(im_dates, im_site_names, \n",
    "                                                                              im_datasets, text_labels, AOIs_UTM, epsg_UTMs, ims_ds)):\n",
    "        \n",
    "    # classified image\n",
    "    im_classified_path = os.path.join(data_path, site_name, 'imagery', 'classified')\n",
    "    im_classified_fn = glob.glob(os.path.join(im_classified_path, im_date + '_' + site_name + '_' + dataset + '*.nc'))[0]\n",
    "    im_classified = xr.open_dataset(im_classified_fn)\n",
    "    im_classified = xr.where((im_classified < 1e3) & (im_classified != -9999) & (im_classified!=0), im_classified, np.nan)\n",
    "    im_classified = im_classified.rio.write_crs('EPSG:4326')\n",
    "    # snowline\n",
    "    sl_path = os.path.join(data_path, site_name, 'imagery', 'snowlines')\n",
    "    sl_fn = glob.glob(os.path.join(sl_path, im_date + '_' + site_name + '_' + dataset + '*.csv'))[0]\n",
    "    sl_df = pd.read_csv(sl_fn)\n",
    "    # plot RGB image\n",
    "    ax[2*i].imshow(np.dstack([im_ds[dataset_dict[dataset]['RGB_bands'][0]].data[0], \n",
    "                              im_ds[dataset_dict[dataset]['RGB_bands'][1]].data[0], \n",
    "                              im_ds[dataset_dict[dataset]['RGB_bands'][2]].data[0]]),\n",
    "                   extent=(np.min(im_ds.x.data), np.max(im_ds.x.data), \n",
    "                           np.min(im_ds.y.data), np.max(im_ds.y.data)))    \n",
    "    # plot classified image\n",
    "    im_classified = im_classified.rio.reproject('EPSG:' + epsg_UTM)\n",
    "    im_classified = xr.where(im_classified < 1e3, im_classified, np.nan)\n",
    "    ax[1+2*i].imshow(im_classified.classified.data[0], cmap=ListedColormap(colors_classified), clim=(1,5),\n",
    "                     extent=(np.min(im_classified.x.data), np.max(im_classified.x.data), \n",
    "                             np.min(im_classified.y.data), np.max(im_classified.y.data)))\n",
    "    # plot snowline\n",
    "    if sl_df['geometry'][0] != '[]':\n",
    "        sl_df['geometry'] = sl_df['geometry'].apply(wkt.loads)\n",
    "        ax[2*i].plot(*sl_df['geometry'][0].coords.xy, '.m', markersize=1)\n",
    "        ax[1+2*i].plot(*sl_df['geometry'][0].coords.xy, '.m', markersize=1)\n",
    "    # set same limits on axes\n",
    "    ax[2*i].set_xlim(ax[1+2*i].get_xlim())\n",
    "    ax[2*i].set_ylim(ax[1+2*i].get_ylim())\n",
    "    # add text labels\n",
    "    ax[2*i].text((ax[2*i].get_xlim()[1] - ax[2*i].get_xlim()[0]) * 0.8 + ax[2*i].get_xlim()[0],\n",
    "                 (ax[2*i].get_ylim()[1] - ax[2*i].get_ylim()[0]) * 0.1 + ax[2*i].get_ylim()[0],\n",
    "                 text_label[0], fontweight='bold', bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "    ax[1+2*i].text((ax[1+2*i].get_xlim()[1] - ax[1+2*i].get_xlim()[0]) * 0.8 + ax[1+2*i].get_xlim()[0],\n",
    "                   (ax[1+2*i].get_ylim()[1] - ax[1+2*i].get_ylim()[0]) * 0.1 + ax[1+2*i].get_ylim()[0],\n",
    "                   text_label[1], fontweight='bold', fontsize=fontsize+2,\n",
    "                   bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "\n",
    "# add dummy points for legend\n",
    "xmin, xmax = ax[2].get_xlim()\n",
    "ymin, ymax = ax[2].get_ylim()\n",
    "ax[2].plot([-10, -10], [-10, -20], '-m', linewidth=3, label='snowline')\n",
    "ax[2].plot(-10, -10, 's', markersize=20, markerfacecolor=colors_classified[0], \n",
    "             markeredgecolor='k', markeredgewidth=1, label='snow')\n",
    "ax[2].plot(-10, -10, 's', markersize=20, markerfacecolor=colors_classified[1], \n",
    "             markeredgecolor='k', markeredgewidth=1, label='shadowed snow')\n",
    "ax[2].plot(-10, -10, 's', markersize=20, markerfacecolor=colors_classified[2], \n",
    "             markeredgecolor='k', markeredgewidth=1, label='ice')\n",
    "ax[2].plot(-10, -10, 's', markersize=20, markerfacecolor=colors_classified[3], \n",
    "             markeredgecolor='k', markeredgewidth=1, label='rock')\n",
    "ax[2].plot(-10, -10, 's', markersize=20, markerfacecolor=colors_classified[4], \n",
    "             markeredgecolor='k', markeredgewidth=1, label='water')\n",
    "ax[2].set_xlim(xmin, xmax)\n",
    "ax[2].set_ylim(ymin, ymax)\n",
    "handles, labels = ax[2].get_legend_handles_labels()\n",
    "leg = fig.legend(handles, labels, loc = (0.015, 0.0))\n",
    "\n",
    "# add arrows to areas of interest\n",
    "for axis in [ax[0], ax[1], ax[4], ax[5]]:\n",
    "    axis.arrow(297.3e3, 5388.8e3, -0.15e3, 0.15e3, width=30, fill=True, edgecolor='k', facecolor='w')\n",
    "for axis in [ax[2], ax[3], ax[6], ax[7]]:\n",
    "    axis.arrow(581.8e3, 7020e3, -0.8e3, -0.8e3, width=110, fill=True, edgecolor='k', facecolor='w')\n",
    "\n",
    "# remove empty axes\n",
    "for axis in ax[8:]:\n",
    "    axis.remove()\n",
    "\n",
    "# remove axis ticks\n",
    "for axis in ax:\n",
    "    axis.set_xticks([])\n",
    "    axis.set_yticks([])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "if save_figures:\n",
    "    fig_fn = os.path.join(figures_out_path, \n",
    "                          'fig06_example_successes+shortcomings.png')\n",
    "    fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "    print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c99bedf-4300-4adf-983f-0ebe0ecf97c8",
   "metadata": {},
   "source": [
    "## Figure 7. Firn detection at Wolverine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a7b94c-e573-44b0-ba5b-d9596147cced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Initialize GEE\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()\n",
    "\n",
    "# -----Load AOI\n",
    "site_name = 'Wolverine'\n",
    "AOI_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/write-ups/CH1_snow_cover_mapping_methods_manuscript/Aberle_et_al_dataset_submission/Wolverine/AOIs/'\n",
    "AOI_fn = glob.glob(os.path.join(AOI_path, 'Wolverine_Glacier_Boundaries_20201019.shp'))[0]\n",
    "AOI_UTM = gpd.read_file(AOI_fn)\n",
    "\n",
    "# -----Query GEE for images\n",
    "im_date = '20190828T212833'\n",
    "datasets = ['Sentinel-2_TOA', 'Sentinel-2_SR']\n",
    "im_ds_list = []\n",
    "for dataset in datasets:\n",
    "    im_dt = np.datetime64(im_date[0:4] + '-' + im_date[4:6] + '-' + im_date[6:8])\n",
    "    date_start, date_end = str(im_dt), str(im_dt + np.timedelta64(1, 'D'))\n",
    "    month_start, month_end = 1, 12\n",
    "    cloud_cover_max = 100\n",
    "    mask_clouds=True\n",
    "    im_ds = f.query_gee_for_imagery(dataset_dict, dataset, AOI_UTM, date_start, date_end, month_start, month_end, cloud_cover_max, mask_clouds)[0]\n",
    "    im_ds_list.append(im_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5774a18e-5089-4481-9f77-e648a58bf1f5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Set up figure\n",
    "fontsize = 12\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = [fig.add_axes([0.05, 0.4, 0.35, 0.3]), \n",
    "      fig.add_axes([0.43, 0.4, 0.35, 0.3]),\n",
    "      fig.add_axes([0.05, 0.05, 0.35, 0.3]),\n",
    "      fig.add_axes([0.43, 0.05, 0.35, 0.3])\n",
    "     ]\n",
    "\n",
    "# -----Difficult conditions (unsuccessful)\n",
    "im_ds = im_ds_list[0]\n",
    "dataset = 'Sentinel-2_TOA'\n",
    "# classified image\n",
    "im_classified_path = os.path.join(AOI_path, '..', 'imagery', 'classified')\n",
    "im_classified_fn = glob.glob(os.path.join(im_classified_path, im_date + '_' + site_name + '_' + dataset + '*.nc'))[0]\n",
    "im_classified = xr.open_dataset(im_classified_fn)\n",
    "im_classified = xr.where((im_classified!=-9999) & (im_classified!=0), im_classified, np.nan)\n",
    "im_classified = im_classified.rio.write_crs('EPSG:4326')\n",
    "im_classified = im_classified.rio.reproject(im_ds.rio.crs)\n",
    "# snowline\n",
    "sl_path = os.path.join(AOI_path, '..', 'imagery', 'snowlines')\n",
    "sl_fn = glob.glob(os.path.join(sl_path, im_date + '_' + site_name + '_' + dataset + '*.csv'))[0]\n",
    "sl_df = pd.read_csv(sl_fn)\n",
    "sl_df['geometry'] = sl_df['geometry'].apply(wkt.loads)\n",
    "sl_gdf = gpd.GeoDataFrame(sl_df, crs=sl_df['HorizontalCRS'][0])\n",
    "sl_gdf = sl_gdf.to_crs('EPSG:' + str(im_ds.rio.crs.to_epsg()))\n",
    "# plot\n",
    "ax[0].imshow(np.dstack([im_ds[dataset_dict[dataset]['RGB_bands'][0]].data[0], \n",
    "                          im_ds[dataset_dict[dataset]['RGB_bands'][1]].data[0], \n",
    "                          im_ds[dataset_dict[dataset]['RGB_bands'][2]].data[0]]),\n",
    "              extent=(np.min(im_ds.x.data), np.max(im_ds.x.data), \n",
    "                      np.min(im_ds.y.data), np.max(im_ds.y.data)))\n",
    "im_classified = xr.where(im_classified < 1e3, im_classified, np.nan)\n",
    "ax[1].imshow(im_classified.classified.data[0], cmap=ListedColormap(colors_classified), clim=(1,5),\n",
    "               extent=(np.min(im_classified.x.data), np.max(im_classified.x.data), \n",
    "                       np.min(im_classified.y.data), np.max(im_classified.y.data)))\n",
    "# zoom in on firn area\n",
    "ax[0].set_ylim(6698*1e3, 6701.5*1e3)\n",
    "ax[1].set_ylim(6698*1e3, 6701.5*1e3) \n",
    "ax[0].plot(*sl_gdf['geometry'][0].coords.xy, '.', color='orange', markersize=1)\n",
    "ax[1].plot(*sl_gdf['geometry'][0].coords.xy, '.', color='orange', markersize=1)\n",
    "                                        \n",
    "# -----Ideal conditions (successes)\n",
    "im_ds = im_ds_list[1]\n",
    "dataset = 'Sentinel-2_SR'\n",
    "# classified image\n",
    "im_classified_fn = glob.glob(os.path.join(im_classified_path, im_date + '_' + site_name + '_' + dataset + '*.nc'))[0]\n",
    "im_classified = xr.open_dataset(im_classified_fn)\n",
    "im_classified = xr.where((im_classified!=-9999) & (im_classified!=0), im_classified, np.nan)\n",
    "im_classified = im_classified.rio.write_crs('EPSG:4326')\n",
    "im_classified = im_classified.rio.reproject(im_ds.rio.crs)\n",
    "# snowline\n",
    "sl_fn = glob.glob(os.path.join(sl_path, im_date + '_' + site_name + '_' + dataset + '*.csv'))[0]\n",
    "sl_df = pd.read_csv(sl_fn)\n",
    "sl_df['geometry'] = sl_df['geometry'].apply(wkt.loads)\n",
    "sl_gdf = gpd.GeoDataFrame(sl_df, crs=sl_df['HorizontalCRS'][0])\n",
    "im_ds = im_ds_list[1]\n",
    "sl_gdf = sl_gdf.to_crs('EPSG:' + str(im_ds.rio.crs.to_epsg()))\n",
    "# plot\n",
    "ax[2].imshow(np.dstack([im_ds[dataset_dict[dataset]['RGB_bands'][0]].data[0], \n",
    "                          im_ds[dataset_dict[dataset]['RGB_bands'][1]].data[0], \n",
    "                          im_ds[dataset_dict[dataset]['RGB_bands'][2]].data[0]]),\n",
    "              extent=(np.min(im_ds.x.data), np.max(im_ds.x.data), \n",
    "                      np.min(im_ds.y.data), np.max(im_ds.y.data)))\n",
    "im_classified = xr.where(im_classified < 1e3, im_classified, np.nan)\n",
    "ax[3].imshow(im_classified.classified.data[0], cmap=ListedColormap(colors_classified), clim=(1,5),\n",
    "               extent=(np.min(im_classified.x.data), np.max(im_classified.x.data), \n",
    "                       np.min(im_classified.y.data), np.max(im_classified.y.data)))\n",
    "ax[2].plot(*sl_gdf['geometry'][0].coords.xy, '.m', markersize=1)\n",
    "ax[3].plot(*sl_gdf['geometry'][0].coords.xy, '.m', markersize=1)\n",
    "# zoom in on firn area\n",
    "ax[2].set_ylim(6698*1e3, 6701.5*1e3)\n",
    "ax[3].set_ylim(6698*1e3, 6701.5*1e3)\n",
    " \n",
    "# remove axis ticks and labels\n",
    "for axis in ax:\n",
    "    axis.set_xticks([])\n",
    "    axis.set_yticks([])\n",
    "    \n",
    "# add dummy points for legend\n",
    "xmin, xmax = ax[0].get_xlim()\n",
    "ymin, ymax = ax[0].get_ylim()\n",
    "ax[3].plot([-10, -10], [-10, -20], '-', color='orange', linewidth=3, label='Incorrect snowline')\n",
    "ax[3].plot([-10, -10], [-10, -20], '-m', linewidth=3, label='Correct snowline')\n",
    "ax[3].plot(-10, -10, 's', markersize=20, markerfacecolor=colors_classified[0], \n",
    "             markeredgecolor='k', markeredgewidth=1, label='snow')\n",
    "ax[3].plot(-10, -10, 's', markersize=20, markerfacecolor=colors_classified[1], \n",
    "             markeredgecolor='k', markeredgewidth=1, label='shadowed snow')\n",
    "ax[3].plot(-10, -10, 's', markersize=20, markerfacecolor=colors_classified[2], \n",
    "             markeredgecolor='k', markeredgewidth=1, label='ice')\n",
    "ax[3].plot(-10, -10, 's', markersize=20, markerfacecolor=colors_classified[3], \n",
    "             markeredgecolor='k', markeredgewidth=1, label='rock')\n",
    "ax[3].plot(-10, -10, 's', markersize=20, markerfacecolor=colors_classified[4], \n",
    "             markeredgecolor='k', markeredgewidth=1, label='water')\n",
    "ax[3].set_xlim(xmin, xmax)\n",
    "ax[3].set_ylim(ymin, ymax)\n",
    "\n",
    "# add text labels and arrows indicating firn\n",
    "text_labels = ['(a)', '(b)', '(c)', '(d)']\n",
    "for i, axis in enumerate(ax):\n",
    "    axis.text(ax[0].get_xlim()[0] + 0.89*(ax[0].get_xlim()[1] - ax[0].get_xlim()[0]),\n",
    "              ax[0].get_ylim()[0] + 0.1*(ax[0].get_ylim()[1] - ax[0].get_ylim()[0]),\n",
    "              text_labels[i], fontweight='bold', fontsize=fontsize+2, bbox=dict(facecolor='white', edgecolor='w', pad=5))\n",
    "    axis.arrow(393.8*1e3, 6698.55*1e3, 0, 0.5e3, color='white', width=0.5, head_width=150, head_length=150, length_includes_head=True, zorder=10)    \n",
    "    axis.arrow(395*1e3, 6698.63*1e3, 0, 0.5e3, color='white', width=0.5, head_width=150, head_length=150, length_includes_head=True, zorder=10)\n",
    "    axis.arrow(395.52*1e3, 6698.4*1e3, 0.4e3, 0.4e3, color='white', width=0.5, head_width=150, head_length=150, length_includes_head=True, zorder=10)\n",
    "\n",
    "# add legend\n",
    "handles, labels = ax[3].get_legend_handles_labels()\n",
    "leg = fig.legend(handles, labels, loc = (0.79, 0.35))\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----Save figure\n",
    "if save_figures:\n",
    "    fig_fn = os.path.join(figures_out_path, 'fig07_example_firn_detection.png')\n",
    "    fig.savefig(fig_fn, dpi=300, facecolor='w', edgecolor='none', bbox_inches='tight')\n",
    "    print('figure saved to file: '+ fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2549bdb5-a00c-4810-94c8-e1909c14532b",
   "metadata": {},
   "source": [
    "## Figure 9. South Cascade snowline cover elevations distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f9c438-716d-4b3d-b711-0f3dd11bd879",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize()\n",
    "date_start, date_end = '2022-10-02', '2022-10-03'\n",
    "dataset = 'Sentinel-2_SR'\n",
    "site_name = 'SouthCascade'\n",
    "\n",
    "# load AOI\n",
    "AOI_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/write-ups/CH1_snow_cover_mapping_methods_manuscript/Aberle_et_al_dataset_submission/SouthCascade/AOIs'\n",
    "AOI_fn = glob.glob(os.path.join(AOI_path, 'SouthCascade_Glacier_Boundaries_20210813.shp'))[0]\n",
    "AOI = gpd.read_file(AOI_fn)\n",
    "\n",
    "# query GEE for image\n",
    "im_xr = f.query_gee_for_imagery(dataset_dict, dataset, AOI, date_start, date_end, \n",
    "                                month_start=1, month_end=12, cloud_cover_max=70, mask_clouds=True)[0]\n",
    "im_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d4f6b4-e98e-4f82-84e6-bc7859a76f5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load EGM96 geoid heights\n",
    "egm96_fn = os.path.join(base_path, 'inputs-outputs', 'us_nga_egm96_15.tif')\n",
    "egm96 = xr.open_dataset(egm96_fn)\n",
    "egm96 = egm96.rename({'band_data': 'geoid_height'})\n",
    "\n",
    "# load classified image\n",
    "im_classified_fn = os.path.join(AOI_path, '..', site_name, 'imagery', 'classified',\n",
    "                                '20221002T132103_SouthCascade_Sentinel-2_SR_classified.nc')\n",
    "im_classified = xr.open_dataset(im_classified_fn)\n",
    "im_classified = im_classified.rio.reproject('EPSG:'+str(AOI.crs.to_epsg()))\n",
    "\n",
    "# load snowline\n",
    "snowline_fn = os.path.join(AOI_path, '..', site_name, 'imagery', 'snowlines',\n",
    "                           '20221002T132103_SouthCascade_Sentinel-2_SR_snowline.csv')\n",
    "snowline = pd.read_csv(snowline_fn)\n",
    "snowline['geometry'] = snowline['geometry'].apply(wkt.loads)\n",
    "snowline_gdf = gpd.GeoDataFrame(snowline, geometry=snowline['geometry'], crs='EPSG:4326')\n",
    "snowline_gdf['snowline_elevs_m'] = [pd.eval(snowline_gdf['snowline_elevs_m'][0].replace('\\n','').replace(' ',', '))]\n",
    "# reference elevations to the ellipsoid instead for direct comparison\n",
    "geoid_heights = [egm96.sel(x=x, y=y, method='nearest').geoid_height.data[0] for x,y in \n",
    "                 list(zip(snowline_gdf.geometry[0].coords.xy[0], snowline_gdf.geometry[0].coords.xy[1]))]\n",
    "snowline_elevs_m_ellipsoid = [(x+y) for x,y in list(zip(snowline_gdf['snowline_elevs_m'][0], geoid_heights))]\n",
    "\n",
    "# load DEM\n",
    "DEM_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/GIS_data/USGS/DEMs/SouthCascade'\n",
    "DEM_fn = os.path.join(DEM_path, 'SouthCascade_2021.08.13_DEM.tif')\n",
    "DEM = xr.open_dataset(DEM_fn)\n",
    "DEM = DEM.rio.reproject('EPSG:'+str(AOI.crs.to_epsg()))\n",
    "DEM = xr.where(DEM >= 1e38, np.nan, DEM)\n",
    "DEM = DEM.rio.write_crs('EPSG:'+str(AOI.crs.to_epsg()))\n",
    "\n",
    "# clip DEM to AOI and interpolate to classified image coordinates\n",
    "dem_aoi = DEM.rio.clip(AOI.geometry, AOI.crs)\n",
    "dem_aoi_interp = dem_aoi.interp(x=im_classified.x.data, y=im_classified.y.data, method='linear')\n",
    "# add elevation as a band to classified image for convenience\n",
    "im_classified['elevation'] = (('time', 'y', 'x'), dem_aoi_interp.band_data.data)\n",
    "\n",
    "# determine snow covered elevations\n",
    "all_elev = np.ravel(dem_aoi_interp.band_data.data)\n",
    "all_elev = all_elev[~np.isnan(all_elev)]  # remove NaNs\n",
    "snow_est_elev = np.ravel(im_classified.where((im_classified.classified <= 2))\n",
    "                         .where(im_classified.classified != -9999).elevation.data)\n",
    "snow_est_elev = snow_est_elev[~np.isnan(snow_est_elev)]  # remove NaNs\n",
    "\n",
    "# -----Create elevation histograms\n",
    "# determine bins to use in histograms\n",
    "elev_min = np.fix(np.nanmin(np.ravel(im_classified.elevation.data)) / 10) * 10\n",
    "elev_max = np.round(np.nanmax(np.ravel(im_classified.elevation.data)) / 10) * 10\n",
    "bin_edges = np.linspace(elev_min, elev_max, num=int((elev_max - elev_min) / 10 + 1))\n",
    "bin_centers = (bin_edges[1:] + bin_edges[0:-1]) / 2\n",
    "# calculate elevation histograms\n",
    "hist_elev = np.histogram(all_elev, bins=bin_edges)[0]\n",
    "hist_snow_est_elev = np.histogram(snow_est_elev, bins=bin_edges)[0]\n",
    "hist_snow_est_elev_norm = hist_snow_est_elev / hist_elev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b37fb-30ce-4184-a8f8-07d5d29fad95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# PLOT\n",
    "fontsize = 18\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16,8))\n",
    "# RGB image\n",
    "xmin, xmax = np.min(im_xr.x.data)/1e3, np.max(im_xr.x.data)/1e3\n",
    "ymin, ymax = np.min(im_xr.y.data)/1e3, np.max(im_xr.y.data)/1e3\n",
    "ax[0].imshow(np.dstack([im_xr['B4'].data[0], im_xr['B3'].data[0], im_xr['B2'].data[0]]),\n",
    "             extent=(xmin, xmax, ymin, ymax))\n",
    "snowline_gdf = snowline_gdf.to_crs(snowline_gdf['HorizontalCRS'][0])\n",
    "ax[0].plot(np.divide(snowline_gdf.geometry[0].coords.xy[0], 1e3), \n",
    "           np.divide(snowline_gdf.geometry[0].coords.xy[1], 1e3), '.m', markersize=2)\n",
    "x_mesh, y_mesh = np.meshgrid(im_classified.x.data, im_classified.y.data)\n",
    "ax[0].contour(np.divide(x_mesh, 1e3), np.divide(y_mesh, 1e3), im_classified.elevation.data[0], \n",
    "              levels=[np.nanmedian(snowline_elevs_m_ellipsoid)], colors='#ff7f00', linewidths=3)\n",
    "# dummy lines for legend\n",
    "ax[0].plot([0,1], [0,1], '-m', linewidth=2, label='Snowline')\n",
    "ax[0].plot([0,1], [0,1], '-', color='#ff7f00', linewidth=2, label='Median snowline altitude')\n",
    "ax[0].set_xlim(xmin, xmax)\n",
    "ax[0].set_ylim(ymin, ymax)\n",
    "ax[0].legend(loc='lower center', bbox_to_anchor=[0.4, 1.02, 0.2, 0.2], framealpha=1)\n",
    "ax[0].set_xlabel('Easting [km]')\n",
    "ax[0].set_ylabel('Northing [km]')\n",
    "ax[0].set_yticks([5356.5, 5357, 5357.5, 5358, 5358.5])\n",
    "ax[0].text((xmax-xmin)*0.08 + xmin, (ymax - ymin)*0.08 + ymin, \n",
    "           '(a)', fontweight='bold', fontsize=fontsize+2, bbox=dict(facecolor='white', pad=5))\n",
    "# histograms\n",
    "sl_elev_min = np.nanmin(snowline_elevs_m_ellipsoid)\n",
    "sl_elev_max = np.nanmax(snowline_elevs_m_ellipsoid)\n",
    "rect = Rectangle((sl_elev_min, 0), width=sl_elev_max-sl_elev_min, height=950, facecolor='m', alpha=0.2)\n",
    "ax[1].add_patch(rect)\n",
    "ax[1].bar(bin_centers, hist_elev, width=(bin_centers[1] - bin_centers[0]), color='#238443', \n",
    "          align='center', label='All elevations')\n",
    "ax[1].bar(bin_centers, hist_snow_est_elev, width=(bin_centers[1] - bin_centers[0]), \n",
    "          color=colors_classified[0], align='center', label='Snow-covered elevations')\n",
    "ax[1].plot([np.nanmedian(snowline_elevs_m_ellipsoid), np.nanmedian(snowline_elevs_m_ellipsoid)], [0, 1000], '-',\n",
    "           color='#ff7f00', linewidth=3, label='Median snowline altitude')\n",
    "ax[1].grid()\n",
    "ax[1].set_xlabel('Elevation [m]')\n",
    "ax[1].set_ylabel('Count')\n",
    "ax[1].set_ylim(0, 950)\n",
    "xmin, xmax = ax[1].get_xlim()\n",
    "ymin, ymax = ax[1].get_ylim()\n",
    "ax[1].text((xmax-xmin)*0.05 + xmin, (ymax - ymin)*0.08 + ymin, \n",
    "           '(b)', fontweight='bold', fontsize=fontsize+2, bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "ax[1].text(1812, 900, 'Snowline elevation range', color='m', fontweight='bold', \n",
    "           bbox=dict(facecolor='w', edgecolor='None', pad=3))\n",
    "ax[1].arrow(1825, 910, -50, 0, color='m', width=5, length_includes_head=True, head_length=7, head_width=25)\n",
    "ax[1].arrow(2035, 910, 50, 0, color='m', width=5, length_includes_head=True, head_length=7, head_width=25)\n",
    "\n",
    "ax[1].legend(loc='lower center', bbox_to_anchor=[0.4, 1.02, 0.2, 0.2], framealpha=1)\n",
    "ax[1].set_position([0.55, 0.17, 0.5, 0.65])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'f09_SouthCascade_snow_covered_elevations.png')\n",
    "fig.savefig(fig_fn, dpi=300, facecolor='w', edgecolor='none', bbox_inches='tight')\n",
    "print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06ea65e-4d61-45a1-82ba-e97bea948dd2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# load EGM96 geoid heights\n",
    "egm96_fn = os.path.join(base_path, 'inputs-outputs', 'us_nga_egm96_15.tif')\n",
    "egm96 = xr.open_dataset(egm96_fn)\n",
    "egm96 = egm96.rename({'band_data': 'geoid_height'})\n",
    "\n",
    "# load classified image\n",
    "im_classified_fn = os.path.join(study_sites_path, site_name, 'imagery', 'classified',\n",
    "                                '20221002T132103_SouthCascade_Sentinel-2_SR_classified.nc')\n",
    "im_classified = xr.open_dataset(im_classified_fn)\n",
    "im_classified = im_classified.rio.reproject('EPSG:'+str(AOI.crs.to_epsg()))\n",
    "\n",
    "# load snowline\n",
    "snowline_fn = os.path.join(study_sites_path, site_name, 'imagery', 'snowlines',\n",
    "                           '20221002T132103_SouthCascade_Sentinel-2_SR_snowline.csv')\n",
    "snowline = pd.read_csv(snowline_fn)\n",
    "snowline['geometry'] = snowline['geometry'].apply(wkt.loads)\n",
    "snowline_gdf = gpd.GeoDataFrame(snowline, geometry=snowline['geometry'], crs='EPSG:4326')\n",
    "snowline_gdf['snowline_elevs_m'] = [pd.eval(snowline_gdf['snowline_elevs_m'][0].replace('\\n','').replace(' ',', '))]\n",
    "# reference elevations to the ellipsoid instead for direct comparison\n",
    "geoid_heights = [egm96.sel(x=x, y=y, method='nearest').geoid_height.data[0] for x,y in \n",
    "                 list(zip(snowline_gdf.geometry[0].coords.xy[0], snowline_gdf.geometry[0].coords.xy[1]))]\n",
    "snowline_elevs_m_ellipsoid = [(x+y) for x,y in list(zip(snowline_gdf['snowline_elevs_m'][0], geoid_heights))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5125424-b058-4fc9-b22a-99c84ef7711b",
   "metadata": {},
   "source": [
    "## Figure S1. PlanetScope image adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856b2b11-76b0-4dce-97ff-c0541c91846e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "site_name = 'LemonCreek'\n",
    "\n",
    "# load AOI and DEM\n",
    "aoi_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/write-ups/CH1_snow_cover_mapping_methods_manuscript/Aberle_et_al_dataset_submission/LemonCreek/AOIs'\n",
    "aoi_fn = glob.glob(os.path.join(aoi_path, site_name + '_Glacier_Boundaries_20211005.shp'))[0]\n",
    "aoi = gpd.read_file(aoi_fn)\n",
    "dem_path = os.path.join(study_sites_path, site_name, 'DEMs')\n",
    "dem_fn = glob.glob(os.path.join(dem_path, site_name + '*USGS*.tif'))[0]\n",
    "dem = xr.open_dataset(dem_fn)\n",
    "dem = dem.rio.reproject('EPSG:'+str(aoi.crs.to_epsg()))\n",
    "dem = dem.rename({'band_data': 'elevation'})\n",
    "dem = xr.where(dem > 1e38, np.nan, dem)\n",
    "dem = dem.isel(band=0)\n",
    "dem = dem.rio.write_crs('EPSG:'+str(aoi.crs.to_epsg()))\n",
    "\n",
    "# load dynamics ranges of all raw image mosaics\n",
    "ps_im_path = os.path.join(aoi_path, '..', 'imagery', 'PlanetScope', 'mosaics')\n",
    "im_fns = sorted(os.listdir(ps_im_path))\n",
    "im_fns = [x for x in im_fns if not x.startswith('.')]\n",
    "dynamic_ranges = pd.DataFrame()\n",
    "for im_fn in tqdm(im_fns):\n",
    "    im_date = im_fn[0:4] + '-' + im_fn[4:6] + '-' + im_fn[6:8]\n",
    "    im = xr.open_dataset(os.path.join(ps_im_path, im_fn))\n",
    "    im = xr.where(im == -9999, np.nan, im / 1e4)\n",
    "    b_min, b_max = np.nanmin(im.band_data.data[0]), np.nanmax(im.band_data.data[0])\n",
    "    dynamic_range = pd.DataFrame({'date': [im_date],\n",
    "                                  'b_min': [b_min],\n",
    "                                  'b_max': [b_max]})\n",
    "    dynamic_ranges = pd.concat([dynamic_ranges, dynamic_range])\n",
    "dynamic_ranges.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# load raw image\n",
    "im_date = '2020-07-29'\n",
    "im_fn = glob.glob(os.path.join(ps_im_path, im_date.replace('-','') + '*.tif'))[0]\n",
    "im = xr.open_dataset(im_fn)\n",
    "im = im / 1e4\n",
    "\n",
    "# construct elevation polygons\n",
    "polygon_top, polygon_bottom = psp.create_aoi_elev_polys(aoi, dem)\n",
    "\n",
    "# adjust image radiometry\n",
    "im_dt = np.datetime64(im_date)\n",
    "im_adj = psp.planetscope_adjust_image_radiometry(im, im_dt, polygon_top, polygon_bottom, dataset_dict, skip_clipped=False)[0]\n",
    "\n",
    "# PLOT\n",
    "fontsize=12\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig = plt.figure(figsize=(8,12))\n",
    "gs = matplotlib.gridspec.GridSpec(5, 2, figure=fig)\n",
    "# dynamic ranges\n",
    "ax0 = fig.add_subplot(gs[0,:])\n",
    "ax0.boxplot(dynamic_ranges[['b_min', 'b_max']], vert=False, bootstrap=100)\n",
    "ax0.set_xlim(0,1.5)\n",
    "ax0.set_yticks([1,2])\n",
    "ax0.set_yticklabels(['B$_{min}$', 'B$_{max}$'])\n",
    "ax0.set_xlabel('Reflectance')\n",
    "aoi_color = '#084594'\n",
    "poly_color = '#41ab5d'\n",
    "# original RGB image\n",
    "ax1 = fig.add_subplot(gs[1:4, 0])\n",
    "ax1.imshow(np.dstack([im.band_data.data[2], im.band_data.data[1], im.band_data.data[0]]),\n",
    "             extent=(np.min(im.x.data)/1e3, np.max(im.x.data)/1e3, \n",
    "                     np.min(im.y.data)/1e3, np.max(im.y.data)/1e3))\n",
    "for geom in aoi.geometry[0].geoms:\n",
    "    aoi_line = ax1.plot(np.divide(geom.exterior.coords.xy[0], 1e3), np.divide(geom.exterior.coords.xy[1], 1e3), '-', color=aoi_color)\n",
    "for geom in polygon_top.geoms:\n",
    "    poly_line = ax1.plot(np.divide(geom.exterior.coords.xy[0], 1e3), np.divide(geom.exterior.coords.xy[1], 1e3), '--', color=poly_color)\n",
    "ax1.set_ylabel('Northing [km]')\n",
    "ax1.set_xlabel('Easting [km]')\n",
    "# original band histograms\n",
    "ax3 = fig.add_subplot(gs[4,0])\n",
    "ax3.hist(np.ravel(im.band_data.data[3]), color='purple', bins=100, histtype='step', linewidth=2, label='NIR')\n",
    "ax3.hist(np.ravel(im.band_data.data[2]), color='blue', bins=100, histtype='step', linewidth=2, label='B')\n",
    "ax3.hist(np.ravel(im.band_data.data[1]), color='green', bins=100, histtype='step', linewidth=2, label='G')\n",
    "ax3.hist(np.ravel(im.band_data.data[0]), color='red', bins=100, histtype='step', linewidth=2, label='R')\n",
    "ax3.grid()\n",
    "ax3.set_xlim(0,1.5)\n",
    "ax3.set_ylim(0,1e5)\n",
    "ax3.set_yticklabels([])\n",
    "ax3.set_ylabel('Counts')\n",
    "ax3.set_xlabel('Reflectance')\n",
    "# adjusted RGB image\n",
    "ax2 = fig.add_subplot(gs[1:4,1])\n",
    "ax2.imshow(np.dstack([im_adj.Red.data[0], im_adj.Green.data[0], im_adj.Blue.data[0]]),\n",
    "             extent=(np.min(im_adj.x.data)/1e3, np.max(im_adj.x.data)/1e3, \n",
    "                     np.min(im_adj.y.data)/1e3, np.max(im_adj.y.data)/1e3))\n",
    "ax2.set_xlabel('Easting [km]')\n",
    "for geom in aoi.geometry[0].geoms:\n",
    "    aoi_line = ax2.plot(np.divide(geom.exterior.coords.xy[0], 1e3), np.divide(geom.exterior.coords.xy[1], 1e3), '-', color=aoi_color)\n",
    "for geom in polygon_top.geoms:\n",
    "    poly_line = ax2.plot(np.divide(geom.exterior.coords.xy[0], 1e3), np.divide(geom.exterior.coords.xy[1], 1e3), '--', color=poly_color)\n",
    "ax2.legend([aoi_line[0], poly_line[0]], ['Glacier boundary', 'Z$_{P80}$ Polygon'], \n",
    "           loc='center right', bbox_to_anchor=[1.7, 0.5, 0.2, 0.2])\n",
    "# adjusted band histograms\n",
    "ax4 = fig.add_subplot(gs[4,1])\n",
    "ax4.hist(np.ravel(im_adj.NIR.data[0]), color='purple', bins=100, histtype='step', linewidth=2, label='NIR')\n",
    "ax4.hist(np.ravel(im_adj.Blue.data[0]), color='blue', bins=100, histtype='step', linewidth=2, label='B')\n",
    "ax4.hist(np.ravel(im_adj.Green.data[0]), color='green', bins=100, histtype='step', linewidth=2, label='G')\n",
    "ax4.hist(np.ravel(im_adj.Red.data[0]), color='red', bins=100, histtype='step', linewidth=2, label='R')\n",
    "ax4.legend(loc='center right', bbox_to_anchor=[1.2, 0.4, 0.2, 0.2])\n",
    "ax4.grid()\n",
    "ax4.set_xlim(0,1.5)\n",
    "# ax4.set_ylim(0,1e5)\n",
    "ax4.set_yticklabels([])\n",
    "ax4.set_xlabel('Reflectance')\n",
    "# add text labels\n",
    "text_labels = ['(a)', '(b)', '(c)', '(d)', '(e)']\n",
    "for ax, text_label, i in list(zip([ax0, ax1, ax2, ax3, ax4], text_labels, np.arange(0,len(text_labels)))):\n",
    "    if i==0:\n",
    "        xscale, yscale = 0.03, 0.8\n",
    "    elif (i==1) or (i==2):\n",
    "        xscale, yscale = 0.1, 0.93\n",
    "    else:\n",
    "        xscale, yscale = 0.05, 0.8\n",
    "    ax.text((ax.get_xlim()[1] - ax.get_xlim()[0])*xscale + ax.get_xlim()[0],\n",
    "            (ax.get_ylim()[1] - ax.get_ylim()[0])*yscale + ax.get_ylim()[0],\n",
    "            text_label, fontweight='bold', fontsize=fontsize+2, bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'figS1_PlanetScope_image_adjustment.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('saved figure to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e6518b-94b2-44a6-aa97-5d2c07d77edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize=12\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig = plt.figure(figsize=(8,12))\n",
    "gs = matplotlib.gridspec.GridSpec(5, 2, figure=fig)\n",
    "# dynamic ranges\n",
    "ax0 = fig.add_subplot(gs[0,:])\n",
    "ax0.boxplot(dynamic_ranges[['b_min', 'b_max']], vert=False, bootstrap=100)\n",
    "ax0.set_xlim(0,1.5)\n",
    "ax0.set_yticks([1,2])\n",
    "ax0.set_yticklabels(['B$_{min}$', 'B$_{max}$'])\n",
    "ax0.set_xlabel('Reflectance')\n",
    "aoi_color = '#084594'\n",
    "poly_color = '#41ab5d'\n",
    "# original RGB image\n",
    "ax1 = fig.add_subplot(gs[1:4, 0])\n",
    "ax1.imshow(np.dstack([im.band_data.data[2], im.band_data.data[1], im.band_data.data[0]]),\n",
    "             extent=(np.min(im.x.data)/1e3, np.max(im.x.data)/1e3, \n",
    "                     np.min(im.y.data)/1e3, np.max(im.y.data)/1e3))\n",
    "for geom in aoi.geometry[0].geoms:\n",
    "    aoi_line = ax1.plot(np.divide(geom.exterior.coords.xy[0], 1e3), np.divide(geom.exterior.coords.xy[1], 1e3), '-', color=aoi_color)\n",
    "for geom in polygon_top.geoms:\n",
    "    poly_line = ax1.plot(np.divide(geom.exterior.coords.xy[0], 1e3), np.divide(geom.exterior.coords.xy[1], 1e3), '--', color=poly_color)\n",
    "ax1.set_yticks(np.arange(6468, 6476, step=2))\n",
    "ax1.set_xticks(np.arange(536, 541, step=2))\n",
    "ax1.set_xlim((536, 540))\n",
    "ax1.set_ylim((6468.3, 6475))\n",
    "ax1.set_ylabel('Northing [km]')\n",
    "ax1.set_xlabel('Easting [km]')\n",
    "# original band histograms\n",
    "ax3 = fig.add_subplot(gs[4,0])\n",
    "ax3.hist(np.ravel(im.band_data.data[3]), color='purple', bins=100, histtype='step', linewidth=2, label='NIR')\n",
    "ax3.hist(np.ravel(im.band_data.data[2]), color='blue', bins=100, histtype='step', linewidth=2, label='B')\n",
    "ax3.hist(np.ravel(im.band_data.data[1]), color='green', bins=100, histtype='step', linewidth=2, label='G')\n",
    "ax3.hist(np.ravel(im.band_data.data[0]), color='red', bins=100, histtype='step', linewidth=2, label='R')\n",
    "ax3.grid()\n",
    "ax3.set_xlim(0,1.5)\n",
    "# ax3.set_ylim(0,1e5)\n",
    "ax3.set_yticklabels([])\n",
    "ax3.set_ylabel('Counts')\n",
    "ax3.set_xlabel('Reflectance')\n",
    "# adjusted RGB image\n",
    "ax2 = fig.add_subplot(gs[1:4,1])\n",
    "ax2.imshow(np.dstack([im_adj.Red.data[0], im_adj.Green.data[0], im_adj.Blue.data[0]]),\n",
    "             extent=(np.min(im_adj.x.data)/1e3, np.max(im_adj.x.data)/1e3, \n",
    "                     np.min(im_adj.y.data)/1e3, np.max(im_adj.y.data)/1e3))\n",
    "ax2.set_xticks(ax1.get_xticks())\n",
    "ax2.set_yticks(ax1.get_yticks())\n",
    "ax2.set_xlim((536, 540))\n",
    "ax2.set_ylim((6468.3, 6475))\n",
    "ax2.set_xlabel('Easting [km]')\n",
    "for geom in aoi.geometry[0].geoms:\n",
    "    aoi_line = ax2.plot(np.divide(geom.exterior.coords.xy[0], 1e3), np.divide(geom.exterior.coords.xy[1], 1e3), '-', color=aoi_color)\n",
    "for geom in polygon_top.geoms:\n",
    "    poly_line = ax2.plot(np.divide(geom.exterior.coords.xy[0], 1e3), np.divide(geom.exterior.coords.xy[1], 1e3), '--', color=poly_color)\n",
    "ax2.legend([aoi_line[0], poly_line[0]], ['Glacier boundary', 'Z$_{P80}$ Polygon'], \n",
    "           loc='center right', bbox_to_anchor=[1.55, 0.5, 0.2, 0.2])\n",
    "# adjusted band histograms\n",
    "ax4 = fig.add_subplot(gs[4,1])\n",
    "ax4.hist(np.ravel(im_adj.NIR.data[0]), color='purple', bins=100, histtype='step', linewidth=2, label='NIR')\n",
    "ax4.hist(np.ravel(im_adj.Blue.data[0]), color='blue', bins=100, histtype='step', linewidth=2, label='B')\n",
    "ax4.hist(np.ravel(im_adj.Green.data[0]), color='green', bins=100, histtype='step', linewidth=2, label='G')\n",
    "ax4.hist(np.ravel(im_adj.Red.data[0]), color='red', bins=100, histtype='step', linewidth=2, label='R')\n",
    "ax4.legend(loc='center right', bbox_to_anchor=[1.2, 0.4, 0.2, 0.2])\n",
    "ax4.grid()\n",
    "ax4.set_xlim(0,1.5)\n",
    "ax4.set_ylim(ax3.get_ylim())\n",
    "ax4.set_yticklabels([])\n",
    "ax4.set_xlabel('Reflectance')\n",
    "# add text labels\n",
    "text_labels = ['(a)', '(b)', '(c)', '(d)', '(e)']\n",
    "for ax, text_label, i in list(zip([ax0, ax1, ax2, ax3, ax4], text_labels, np.arange(0,len(text_labels)))):\n",
    "    if i==0:\n",
    "        xscale, yscale = 0.92, 0.2\n",
    "    elif (i==1) or (i==2):\n",
    "        xscale, yscale = 0.85, 0.1\n",
    "    else:\n",
    "        xscale, yscale = 0.87, 0.22\n",
    "    ax.text((ax.get_xlim()[1] - ax.get_xlim()[0])*xscale + ax.get_xlim()[0],\n",
    "            (ax.get_ylim()[1] - ax.get_ylim()[0])*yscale + ax.get_ylim()[0],\n",
    "            text_label, fontweight='bold', fontsize=fontsize+2, bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'figS1_PlanetScope_image_adjustment.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('saved figure to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1615c1e6-56bd-4127-9eb7-3496ad50c000",
   "metadata": {},
   "source": [
    "## Figure S2. Model learning curves\n",
    "\n",
    "Located in `develop_classifiers.ipynb`step 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6eac8a-7a86-4995-b325-21267bed7389",
   "metadata": {},
   "source": [
    "## Figure S4. Impact of slope on ELA sensitivity to air temperature change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5c56a6-b13a-4e2f-b72d-186d699c6c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcca74b-60c6-46de-868b-0cf1706ab0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image query settings\n",
    "site_name = 'LemonCreek'\n",
    "dataset = 'Sentinel-2_SR'\n",
    "im_date = '2021-08-31'\n",
    "date_start, date_end = '2021-08-31', '2021-09-01'\n",
    "month_start, month_end = 7, 10\n",
    "cloud_cover_max = 70\n",
    "mask_clouds = False\n",
    "im_out_path = None\n",
    "im_download = False\n",
    "\n",
    "# Load AOI\n",
    "aoi_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/write-ups/CH1_snow_cover_mapping_methods_manuscript/Aberle_et_al_dataset_submission/' + site_name + '/AOIs/'\n",
    "aoi_fn = glob.glob(os.path.join(aoi_path, '*20211005.shp'))[0]\n",
    "aoi = gpd.read_file(aoi_fn)\n",
    "\n",
    "im_list = f.query_gee_for_imagery(dataset_dict, dataset, aoi, date_start, date_end,\n",
    "                                  month_start, month_end, cloud_cover_max, mask_clouds,\n",
    "                                  im_out_path, im_download)\n",
    "im = im_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e79733-13ed-47bf-b6ce-954958b80768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dry adiabatic lapse rate ~= 9.8 deg C / km\n",
    "# 0.5 deg C / (9.8 deg C / 1000 m) = 51 m / deg C increase in temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e3e4bd-4c70-4c71-8d8e-2cbb600ac42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine whether to plot only contours\n",
    "plot_contours = True\n",
    "\n",
    "# clip image to AOI\n",
    "aoi_sel = aoi.geometry[0].geoms[4]\n",
    "im_clip = im\n",
    "\n",
    "# load DEM\n",
    "dem_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/GIS_data/USGS/DEMs/lemonCreek'\n",
    "dem_fn = glob.glob(os.path.join(dem_path, '*2021*.tif'))[0]\n",
    "dem = xr.open_dataset(dem_fn)\n",
    "# reproject to image CRS\n",
    "dem = dem.rio.reproject('EPSG:'+str(im.rio.crs.to_epsg()))\n",
    "dem = xr.where(dem < 1e38, dem, np.nan)\n",
    "dem = dem.rio.write_crs('EPSG:'+str(im.rio.crs.to_epsg()))\n",
    "# interpolate dem to image coordinates\n",
    "dem_interp = dem.sel(x=im_clip.x.data, y=im_clip.y.data, method='nearest')\n",
    "# mask it using the AOI\n",
    "dem_interp_masked = dem_interp.rio.clip([aoi_sel.buffer(-80)], im.rio.crs, drop=False)\n",
    "z1 = dem_interp.band_data.data[0]\n",
    "z1_masked = dem_interp_masked.band_data.data[0]\n",
    "# remove minimum elevation so it has a minimum of zero\n",
    "z1_min = np.nanmin(z1)\n",
    "z1 = z1 - z1_min\n",
    "z1_masked = z1_masked - z1_min\n",
    "# flip it around to match the RGB image\n",
    "z1 = np.flipud(np.fliplr(z1))\n",
    "z1_masked = np.flipud(np.fliplr(z1_masked))\n",
    "# low slope\n",
    "z2 = z1 / 2\n",
    "z2_masked = z1_masked / 2\n",
    "\n",
    "# set up RGB data\n",
    "red = im_clip['B4'].data[0]\n",
    "green = im_clip['B3'].data[0]\n",
    "blue = im_clip['B2'].data[0]\n",
    "# Ensure color values are between 0 and 1\n",
    "red = np.clip(red, 0, 1)\n",
    "green = np.clip(green, 0, 1)\n",
    "blue = np.clip(blue, 0, 1)\n",
    "# Create the RGB image \n",
    "rgb_image = np.dstack((red, green, blue))\n",
    "# flip around so the glacier flows downhill\n",
    "rgb_image = np.fliplr(np.flipud(rgb_image))\n",
    "# Identify NaN values and set them to the specified color (white)\n",
    "nan_mask = np.isnan(rgb_image).any(axis=2)\n",
    "nan_color = [1, 1, 1]\n",
    "rgb_image[nan_mask] = nan_color\n",
    "cmap = plt.get_cmap('viridis')  # Adjust the colormap as needed\n",
    "cmap.set_bad(color='white')  # Set the color for NaN values to transparent\n",
    "\n",
    "# plot figures\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax1 = fig.add_axes((0.1, 0.1, 0.4, 0.8), projection='3d')\n",
    "ax2 = fig.add_axes((0.46, 0.1, 0.4, 0.8), projection='3d')\n",
    "text_labels = ['(a) High slope', '(b) Low slope']\n",
    "ela_inits = 370, 180\n",
    "for ax, z, z_masked, text_label, ela_init in list(zip([ax1, ax2], [z1, z2], [z1_masked, z2_masked], \n",
    "                                                      text_labels, ela_inits)):\n",
    "    X, Y = np.meshgrid(dem_interp.x.data, dem_interp.y.data)\n",
    "    if not plot_contours:\n",
    "        ax.plot_surface(X, Y, z, rstride=1, cstride=1, alpha=1, cmap=cmap, norm=False, facecolors=rgb_image)\n",
    "    # plot elevation contours\n",
    "    X, Y = np.meshgrid(dem_interp_masked.x.data, dem_interp_masked.y.data)\n",
    "    CS = ax.contour(X, Y, z_masked, levels=[ela_init, ela_init + 51], colors='m', alpha=1, \n",
    "                    linestyles=['solid', 'dashed'], linewidths=2) \n",
    "    # remove the axes\n",
    "    ax.set_axis_off()\n",
    "    ax.view_init(elev=20, azim=310)\n",
    "    ax.set_zlim(0, np.nanmax(z1))\n",
    "    # add text label\n",
    "    ax.set_title(text_label, fontweight='bold', x=0.3, y=0.85)\n",
    "\n",
    "# add legend for contours\n",
    "if plot_contours:\n",
    "    h,_ = CS.legend_elements()\n",
    "    ax1.legend([h[0], h[1]], ['ELA$_1$', 'ELA$_2$'], loc='lower left', bbox_to_anchor=[0.15, 0.15, 0.2, 0.2])\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "if save_figures:\n",
    "    if plot_contours:\n",
    "        fig_fn = os.path.join(figures_out_path, 'LemonCreek_3D_RGB_plot_contours.png')\n",
    "        fig.savefig(fig_fn, dpi=300, bbox_inches='tight', transparent=True)\n",
    "    else:\n",
    "        fig_fn = os.path.join(figures_out_path, 'LemonCreek_3D_RGB_plot.png')\n",
    "        fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "    print('figure saved to file: ' + fig_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0669670b-2377-405e-8fbc-e1bb84f26441",
   "metadata": {},
   "source": [
    "## Figure X (removed). Testing SCA sensitivity to presence of firn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcafec6-5a3d-435d-bed2-ce3e6828ae24",
   "metadata": {},
   "source": [
    "### Slope vs. area of misclassified firn %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbd550f-9cd3-4c3e-a76b-0fc06f91cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ranges in slopes, percents_firn, and calculate changes in SCA\n",
    "slopes = np.arange(0, 51, step=0.5)\n",
    "percents_firn = np.arange(0, 20.1, step=0.5)\n",
    "differences = np.zeros((len(percents_firn), len(slopes)))\n",
    "for i, percent_firn in enumerate(percents_firn):\n",
    "    for j, slope in enumerate(slopes):\n",
    "        differences[i,j] = percent_firn * np.cos(np.radians(slope))\n",
    "# flip array for plotting\n",
    "differences = np.array(differences)\n",
    "differences = np.fliplr(np.flipud(differences))\n",
    "\n",
    "# Plot\n",
    "fontsize=14\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "# distribution of slopes\n",
    "ax1 = fig.add_axes((0.1, 0.7, 0.72, 0.2))\n",
    "hist = ax1.hist(rgis['Slope'].values, bins=50, color='grey', range=(0,50))\n",
    "counts, bins = hist[0], hist[1]\n",
    "ax1.set_xlim(0,50)\n",
    "ax1.axis('off')\n",
    "# heatmap of uncertainties\n",
    "ax2 = fig.add_axes((0.1, 0.1, 0.9, 0.775))\n",
    "diff_im = ax2.imshow(differences, extent=(slopes[0], slopes[-1], percents_firn[0], percents_firn[-1]), \n",
    "                    cmap='inferno', clim=(0,20))\n",
    "ax2.set_ylabel('Misclassified firn [% of SCA]')\n",
    "ax2.set_xlabel('Slope [degrees]')\n",
    "ax2.set_xlim(0,50)\n",
    "ax2.grid()\n",
    "ax2.set_aspect(1.5)\n",
    "ax2.set_yticks(np.arange(0,21, step=5))\n",
    "fig.colorbar(diff_im, ax=ax2, shrink=0.5, label='$\\Delta$SCA [%]', ticks=np.arange(0,21, step=5))\n",
    "# add text labels\n",
    "ax1.text((ax1.get_xlim()[1] - ax1.get_xlim()[0])*0.05,\n",
    "         (ax1.get_ylim()[1] - ax1.get_ylim()[0])*0.1, '(a)', fontweight='bold', fontsize=fontsize+2,\n",
    "         bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "ax2.text((ax2.get_xlim()[1] - ax2.get_xlim()[0])*0.05,\n",
    "         (ax2.get_ylim()[1] - ax2.get_ylim()[0])*0.1, '(b)', fontweight='bold', fontsize=fontsize+2,\n",
    "         bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "# draw box around most common slope\n",
    "box_color = 'black'\n",
    "I = np.argwhere(counts==np.nanmax(counts))[0][0]\n",
    "rect1 = matplotlib.patches.Rectangle((bins[I], 0), width=1, height=np.nanmax(counts), edgecolor=box_color, facecolor='None')\n",
    "ax1.add_patch(rect1)\n",
    "rect2 = matplotlib.patches.Rectangle((bins[I], 0), width=1, height=np.nanmax(percents_firn), edgecolor=box_color, facecolor='None')\n",
    "ax2.add_patch(rect2)\n",
    "# add descriptor text\n",
    "ax1.text(30, np.nanmax(counts)-100, \n",
    "         'Most frequent slopes = (' + str(int(bins[I])) + '\\N{degree sign}, ' + str(int(bins[I+1])) + '\\N{degree sign})',\n",
    "         color=box_color)\n",
    "ax1.text(41.1, np.nanmax(counts)-550,\n",
    "         '$\\Delta$SCA = (' \n",
    "         + str(int(differences[-1, np.argwhere(slopes==bins[I])[0][0]])) + '%, ' \n",
    "         + str(int(differences[0, np.argwhere(slopes==bins[I])[0][0]])) + '%)',\n",
    "         color=box_color)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "if save_figures:\n",
    "    fig_fn = os.path.join(figures_out_path, 'figS2_sensitivity_test_misclassified_firn.png')\n",
    "    fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "    print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57233c12-414d-4b4d-8b04-5aebb96b5bc7",
   "metadata": {},
   "source": [
    "## Figure X (removed). Images used for classification performance assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af0da72-602b-428b-aa1e-9c6114f008c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "\n",
    "data_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/snow_cover_mapping/classified-points/assessment/'\n",
    "\n",
    "# grab Sentinel-2_SR image file names\n",
    "os.chdir(data_path)\n",
    "im_fns = sorted(glob.glob('*Sentinel-2_SR*.tif'))\n",
    "# plot Lemon Creek images on top\n",
    "im_fns = im_fns[2:] + im_fns[0:2]\n",
    "\n",
    "# load glacier outlines from file\n",
    "study_sites_path = '/Users/raineyaberle/Google Drive/My Drive/Research/CryoGARS-Glaciology/Advising/student-research/Alexandra-Friel/snow_cover_mapping_application/study-sites/'\n",
    "AOI_fns = [study_sites_path + 'Emmons/AOIs/Emmons_RGI_outline.shp',\n",
    "           study_sites_path + 'LemonCreek/AOIs/LemonCreek_USGS_glacier_outline_2021.shp']\n",
    "AOIs = [gpd.read_file(AOI_fn) for AOI_fn in AOI_fns]\n",
    "\n",
    "# grab validation point names\n",
    "data_pts_fns = sorted(glob.glob('*.shp'))\n",
    "\n",
    "# set up figure\n",
    "fig, ax = plt.subplots(2, 2, figsize=(8,8))\n",
    "plt.rcParams.update({'font.size':12, 'font.sans-serif':'Arial'})\n",
    "ax = ax.flatten()\n",
    "text_labels = ['(a)', '(b)', '(c)', '(d)']\n",
    "# plot dummy points for legend\n",
    "ax[0].plot(0,0, '.', markersize=8, color=colors_classified[0], label='snow')\n",
    "ax[0].plot(0,0, '.', markersize=8, color=colors_classified[3], label='no snow')\n",
    "\n",
    "# loop through image files\n",
    "for i, im_fn in enumerate(im_fns):\n",
    "    \n",
    "    print(im_fn)\n",
    "    \n",
    "    # open image and plot\n",
    "    im = rxr.open_rasterio(im_fn)\n",
    "    # grab CRS\n",
    "    crs = im.rio.crs.to_epsg()\n",
    "    im = im / 1e4\n",
    "    ax[i].imshow(np.dstack([im.data[3], im.data[2], im.data[1]]),\n",
    "                extent=(np.min(im.x.data), np.max(im.x.data), np.min(im.y.data), np.max(im.y.data)))\n",
    "    \n",
    "    # load data points and plot\n",
    "    site_name = im_fn.split('_')[0]\n",
    "    im_date = im_fn[-12:-4]\n",
    "    data_pts_snow_fn = [x for x in data_pts_fns if (site_name in x) and (im_date[0:6] in x) and ('_snow' in x)]\n",
    "    if len(data_pts_snow_fn) > 0:\n",
    "        data_pts_snow = gpd.read_file(data_pts_snow_fn[0])\n",
    "        data_pts_snow = data_pts_snow.to_crs(im.rio.crs)\n",
    "        data_pts_snow.plot(ax=ax[i], color=colors_classified[0], markersize=1)\n",
    "    data_pts_no_snow_fn = [x for x in data_pts_fns if (site_name in x) and (im_date[0:6] in x) and ('no-snow' in x)]\n",
    "    if len(data_pts_no_snow_fn) > 0:\n",
    "        data_pts_no_snow = gpd.read_file(data_pts_no_snow_fn[0])\n",
    "        data_pts_no_snow = data_pts_no_snow.to_crs(im.rio.crs)\n",
    "        data_pts_no_snow.plot(ax=ax[i], color=colors_classified[3], markersize=1)\n",
    "        \n",
    "    # select AOI, reproject, and plot\n",
    "    if 'Emmons' in im_fn:\n",
    "        AOI = AOIs[0]\n",
    "    elif 'LemonCreek' in im_fn:\n",
    "        AOI = AOIs[1]\n",
    "    AOI = AOI.to_crs('EPSG:'+str(crs))\n",
    "    AOI_color = '#9e9ac8'\n",
    "    if type(AOI.geometry[0])==MultiPolygon:\n",
    "        for j, geom in enumerate(AOI.geometry[0].geoms):\n",
    "            if j==0:\n",
    "                ax[i].plot(*geom.exterior.coords.xy, '-', color=AOI_color, label='glacier boundary')\n",
    "            else:\n",
    "                ax[i].plot(*geom.exterior.coords.xy, '-', color=AOI_color, label='_nolegend')\n",
    "    else:\n",
    "        ax[i].plot(*AOI.geometry[0].exterior.coords.xy, '-', color=AOI_color, label='glacier outline')\n",
    "        \n",
    "    # set axis limits and ticks\n",
    "    if i>=2:\n",
    "        ax[i].set_xlim(593e3, 603e3)\n",
    "        ax[i].set_ylim(5188e3, 5196e3)\n",
    "        ax[i].set_xticks(np.arange(594e3, 603e3, step=2e3))\n",
    "        ax[i].set_yticks(np.arange(5188e3, 5197e3, step=2e3))\n",
    "    else:\n",
    "        ax[i].set_xlim(535e3, 541e3)\n",
    "        ax[i].set_ylim(6468e3, 6475e3)\n",
    "        ax[i].set_xticks(np.arange(536e3, 541e3, step=2e3))\n",
    "        ax[i].set_yticks(np.arange(6468e3, 6475e3, step=2e3))\n",
    "    # change labels from m to km\n",
    "    ax[i].set_xticklabels([str(int(x/1e3)) for x in ax[i].get_xticks()])\n",
    "    ax[i].set_yticklabels([str(int(x/1e3)) for x in ax[i].get_yticks()])\n",
    "        \n",
    "    # add text labels\n",
    "    ax[i].text((ax[i].get_xlim()[1] - ax[i].get_xlim()[0])*0.05 + ax[i].get_xlim()[0],\n",
    "               (ax[i].get_ylim()[1] - ax[i].get_ylim()[0])*0.9 + ax[i].get_ylim()[0],\n",
    "               text_labels[i], backgroundcolor='w')\n",
    "    \n",
    "\n",
    "# add axis labels\n",
    "ax[0].set_ylabel('Northing [km]')\n",
    "ax[2].set_ylabel('Northing [km]')\n",
    "ax[2].set_xlabel('Easting [km]')\n",
    "ax[3].set_xlabel('Easting [km]')\n",
    "\n",
    "# add legendwin\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=(0.27, 0.92), ncols=3)\n",
    "\n",
    "# fig.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "# save figure\n",
    "if save_figures:\n",
    "    fig_fn = os.path.join(figures_out_path, 'f04_classification_performance_assessment_images.png')\n",
    "    fig.savefig(fig_fn, facecolor='w', dpi=300, bbox_inches='tight')\n",
    "    print('figure saved to file: ' + fig_fn)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13352855-a103-42c0-8e81-0a96c1f049b4",
   "metadata": {},
   "source": [
    "## Figure X (removed). SCA, snowlines, and AAR time series variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3654c3-369d-47d2-9271-221f1bcab524",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Settings and display parameters\n",
    "site_names = ['Wolverine', 'Gulkana', 'LemonCreek', 'SouthCascade', 'Sperry']\n",
    "\n",
    "fmt_month = matplotlib.dates.MonthLocator(bymonth=(5, 11)) # minor ticks every month\n",
    "fmt_year = matplotlib.dates.YearLocator() # minor ticks every year\n",
    "\n",
    "# -----Iterate over site names\n",
    "stats_df = pd.DataFrame()\n",
    "values_df = pd.DataFrame()\n",
    "for i, site_name in enumerate(site_names):\n",
    "    \n",
    "    print(site_name)\n",
    "\n",
    "    # Load estimated snow lines  \n",
    "    sl_est_fns = glob.glob(study_sites_path + site_name + '/imagery/snowlines/*snowline.csv')\n",
    "    sl_ests = gpd.GeoDataFrame()\n",
    "    for sl_est_fn in sl_est_fns:\n",
    "        sl_est = pd.read_csv(sl_est_fn)\n",
    "        sl_ests = pd.concat([sl_ests, sl_est])\n",
    "    sl_ests.reset_index(drop=True, inplace=True)\n",
    "    sl_ests['datetime'] = pd.to_datetime(sl_ests['datetime'], format='mixed')\n",
    "        \n",
    "    # Define axis limits\n",
    "    # xmin, xmax = np.datetime64('2016-05-01T00:00:00'), np.datetime64('2022-12-01T00:00:00')\n",
    "    # sl_elev_median_min = np.nanmin(sl_ests['snowline_elevs_median_m'])\n",
    "    # sl_elev_median_max = np.nanmax(sl_ests['snowline_elevs_median_m'])\n",
    "    # ymin1, ymax1 = np.nanmin(sl_ests['SCA_m2']) * 1e-6 * -0.1, np.nanmax(sl_ests['SCA_m2']) * 1e-6 * 1.3\n",
    "    # ymin2 = sl_elev_median_min - 0.1*(sl_elev_median_max - sl_elev_median_min)\n",
    "    # ymax2 = sl_elev_median_max + 0.1*(sl_elev_median_max - sl_elev_median_min)\n",
    "    # ymin3, ymax3 = -1, 125\n",
    "    # yrange1, yrange2, yrange3 = [ymin1, ymax1], [ymin2, ymax2], [ymin3, ymax3]\n",
    "\n",
    "    # Calculate monthly mean and std for Sentinel-2 time series\n",
    "    def custom_rolling_stats(data, data_var, window_size, months_to_include):\n",
    "        filtered_data = data[data['datetime'].dt.month.isin(months_to_include)]\n",
    "        rolling_std = filtered_data[data_var].rolling(window=window_size).apply(lambda x: np.std(x))\n",
    "        return rolling_std\n",
    "    \n",
    "    sl_ests.index = sl_ests.datetime\n",
    "    sl_ests.sort_index(inplace=True) # sort chronologically\n",
    "    \n",
    "    # define settings for rolling stats\n",
    "    time_range = pd.Timedelta(days=7) # window for rolling stats\n",
    "    months_to_include = [5,6,7,8,9,10] # which months to include\n",
    "    SCA_std = custom_rolling_stats(sl_ests, 'SCA_m2', int(time_range.days), months_to_include)\n",
    "    sl_std = custom_rolling_stats(sl_ests, 'snowline_elevs_median_m', int(time_range.days), months_to_include)\n",
    "    AAR_std = custom_rolling_stats(sl_ests, 'AAR', int(time_range.days), months_to_include)\n",
    "    \n",
    "    # append to dataframe\n",
    "    df = pd.DataFrame({'site_name': site_name,\n",
    "                       'SCA_std': SCA_std.values,\n",
    "                       'SCA_std_normalized': np.divide(SCA_std, np.nanmax(sl_ests['SCA_m2'])).values,\n",
    "                       'sl_std': sl_std.values,\n",
    "                       'sl_std_normalized': np.divide(sl_std, (np.nanmax(sl_ests['snowline_elevs_median_m']) - np.nanmin(sl_ests['snowline_elevs_median_m']))).values,\n",
    "                       'AAR_std': AAR_std.values,\n",
    "                       'AAR_std_normalized': np.divide(AAR_std, np.nanmax(sl_ests['AAR'])).values,\n",
    "                      })\n",
    "    stats_df = pd.concat([stats_df, df])\n",
    "    \n",
    "# -----Adjust dataframe\n",
    "# reset index, drop NaN values\n",
    "stats_df.reset_index(drop=True).dropna(inplace=True)\n",
    "# add display name to dataframe for plotting\n",
    "stats_df['display_name'] = [x.replace('C', ' C') for x in stats_df['site_name'].values]\n",
    "# adjust units for SCA (km^2) and AAR (%)\n",
    "stats_df[['SCA_std']] = np.divide(stats_df[['SCA_std']], 1e6)\n",
    "stats_df[['AAR_std']] = np.multiply(stats_df[['AAR_std']], 100)\n",
    "\n",
    "# -----Plot\n",
    "plt.rcParams.update({'font.size':14, 'font.sans-serif': 'Arial'})\n",
    "fig, ax = plt.subplots(3, 2, figsize=(16, 16))\n",
    "ax = ax.flatten()  \n",
    "# define axes settings\n",
    "text_labels = ['(a)', '(b)', '(c)', '(d)', '(e)', '(f)']\n",
    "ylabels = ['SCA [km$^2$]', 'SCA [unitless]',\n",
    "           'AAR [%]', 'AAR [unitless]',\n",
    "           'Median snowline altitude [m]', 'Median snowline altitude [unitless]'\n",
    "           ]\n",
    "data_vars = ['SCA_std', 'SCA_std_normalized', 'AAR_std', 'AAR_std_normalized', 'sl_std', 'sl_std_normalized']\n",
    "colors = [colors_classified[0], colors_classified[0], \n",
    "          '#FFC107', '#FFC107',\n",
    "          '#FB65FB', '#FB65FB']\n",
    "ax[0].set_title('Weekly standard deviation')\n",
    "ax[1].set_title('Normalized weekly standard deviation')\n",
    "ax[1].set_ylim(-0.01, 0.5)\n",
    "ax[2].set_ylim(-1, 50)\n",
    "ax[3].set_ylim(-0.01, 0.5)\n",
    "ax[5].set_ylim(-0.01, 0.5)\n",
    "# iterate over axes\n",
    "for axis, data_var, color, text_label, ylabel in list(zip(ax, data_vars, colors, text_labels, ylabels)):\n",
    "    sns.boxplot(data=stats_df, x='display_name', y=data_var, ax=axis, color=color) \n",
    "    axis.set(xlabel=None)\n",
    "    axis.set_ylabel(ylabel)\n",
    "    axis.text((axis.get_xlim()[1] - axis.get_xlim()[0])*0.935 + axis.get_xlim()[0], \n",
    "               (axis.get_ylim()[1] - axis.get_ylim()[0])*0.903 + axis.get_ylim()[0], \n",
    "               text_label, bbox=dict(facecolor='white', edgecolor='black', pad=5))\n",
    "    axis.set_xticks(axis.get_xticks(), axis.get_xticklabels(), rotation=20)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# -----Save figures\n",
    "if save_figures:\n",
    "    fig_fn = os.path.join(figures_out_path, 'weekly_metric_ranges.png')\n",
    "    fig.savefig(fig_fn, dpi=300, facecolor='w', edgecolor='none', bbox_inches='tight')\n",
    "    print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cbac9f-7b07-45c6-ad29-85340125abf1",
   "metadata": {},
   "source": [
    "## Example SCA time series at South Cascade Glacier for GitHub repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dfd65c-d01b-489c-8fde-d041b0ae2f52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "site_name = 'SouthCascade'\n",
    "\n",
    "# -----Load estimated snowlines  \n",
    "sl_ests_full = pd.DataFrame()    \n",
    "sl_est_fns = glob.glob(study_sites_path + site_name + '/imagery/snowlines/*snowline.csv')\n",
    "sl_ests = pd.DataFrame()\n",
    "for sl_est_fn in sl_est_fns:\n",
    "    sl_est = pd.read_csv(sl_est_fn)\n",
    "    sl_ests = pd.concat([sl_ests, sl_est])\n",
    "sl_ests.reset_index(drop=True, inplace=True)\n",
    "sl_ests['datetime'] = pd.to_datetime(sl_ests['datetime'], format='mixed')\n",
    "# reset index and add week-of-year column\n",
    "sl_ests.reset_index(drop=True, inplace=True)\n",
    "sl_ests['Week'] = sl_ests['datetime'].dt.isocalendar().week\n",
    "# convert SCA from m2 to km2\n",
    "sl_ests['SCA_km2'] = np.divide(sl_ests['SCA_m2'].values, 1e6)\n",
    "\n",
    "# -----Set up figure\n",
    "fontsize=12\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "# time series: SCA\n",
    "fig1, ax1 = plt.subplots(3, 2, figsize=(10, 10), gridspec_kw=dict(width_ratios=[4,1]))\n",
    "# fmt_year = matplotlib.dates.YearLocator() # minor ticks every year\n",
    "fmt_year = matplotlib.dates.DateFormatter(\"%Y\")\n",
    "alpha = 0.9\n",
    "\n",
    "# -----Define axis limits\n",
    "xmin, xmax = np.datetime64('2013-05-01T00:00:00'), np.datetime64('2022-12-01T00:00:00')\n",
    "sl_elev_median_min = np.nanmin(sl_ests['snowline_elevs_median_m'])\n",
    "sl_elev_median_max = np.nanmax(sl_ests['snowline_elevs_median_m'])\n",
    "yrange1 = [np.nanmax(sl_ests['SCA_km2']) * -0.1, np.nanmax(sl_ests['SCA_km2']) * 1.1]\n",
    "yrange2 = [-0.1, 1.1]\n",
    "yrange3 = [np.nanmin(sl_ests['snowline_elevs_median_m']) * 0.98, np.nanmax(sl_ests['snowline_elevs_median_m']) * 1.02]\n",
    "        \n",
    "# -----Plot time series of SCA, AAR, and median snowline elevations\n",
    "for column, ylabel, yrange, i in list(zip(['SCA_km2', 'AAR', 'snowline_elevs_median_m'],\n",
    "                                          ['SCA [km$^2$]', 'AAR', 'Median snowline elevation [m]'],\n",
    "                                          [yrange1, yrange2, yrange3],\n",
    "                                          np.arange(0,3))):\n",
    "    # PlanetScope\n",
    "    ax1[i,0].plot(sl_ests['datetime'].loc[sl_ests['dataset']=='PlanetScope'], \n",
    "                   sl_ests[column].loc[sl_ests['dataset']=='PlanetScope'].values, \n",
    "                   '.', markeredgecolor='w', markerfacecolor=color_PlanetScope, \n",
    "                alpha=alpha, markersize=10, markeredgewidth=1, label='PlanetScope')\n",
    "    # Sentinel-2 SR\n",
    "    ax1[i,0].plot(sl_ests['datetime'].loc[sl_ests['dataset']=='Sentinel-2_SR'], \n",
    "                  sl_ests[column].loc[sl_ests['dataset']=='Sentinel-2_SR'].values, \n",
    "                  'D', markeredgecolor='w', markerfacecolor=color_Sentinel2, \n",
    "                  alpha=alpha, markersize=4, markeredgewidth=1, label='Sentinel-2 SR')\n",
    "    # Sentinel-2 TOA\n",
    "    ax1[i,0].plot(sl_ests['datetime'].loc[sl_ests['dataset']=='Sentinel-2_TOA'], \n",
    "                  sl_ests[column].loc[sl_ests['dataset']=='Sentinel-2_TOA'].values, \n",
    "                  'D', markeredgecolor=color_Sentinel2, markerfacecolor='None', \n",
    "                  alpha=alpha, markersize=3, markeredgewidth=1.2, label='Sentinel-2 TOA')  \n",
    "    # Landsat\n",
    "    ax1[i,0].plot(sl_ests['datetime'].loc[sl_ests['dataset']=='Landsat'], \n",
    "                  sl_ests[column].loc[sl_ests['dataset']=='Landsat'].values, \n",
    "                  '^', markeredgecolor='w', markerfacecolor=color_Landsat, \n",
    "                  alpha=alpha, markersize=6, markeredgewidth=1, label='Landsat')   \n",
    "    # adjust axis\n",
    "    ax1[i,0].set_ylabel(ylabel)\n",
    "    ax1[i,0].set_xlim(xmin, xmax)\n",
    "    ax1[i,0].set_ylim(yrange[0], yrange[1])\n",
    "    ax1[i,0].grid(True)\n",
    "\n",
    "    # -----Plot light grey boxes where no observations exist \n",
    "    years = np.arange(2013, 2022, step=1)\n",
    "    for year in years:\n",
    "        min_date, max_date = np.datetime64(str(year) + '-11-01'), np.datetime64(str(year+1) + '-05-01')\n",
    "        rect = matplotlib.patches.Rectangle((min_date, yrange[0]), width=max_date-min_date, height=yrange[1]-yrange[0], color='#d9d9d9')\n",
    "        ax1[i,0].add_patch(rect)\n",
    "    \n",
    "    # -----Calculate median and interquartile ranges for weekly trends\n",
    "    q1, q3 = 0.25, 0.75 # define quartiles\n",
    "    # calculate weekly trends using only Sentinel-2 snowlines\n",
    "    sl_ests_noPS = sl_ests.loc[sl_ests['dataset']!='PlanetScope']   \n",
    "    weekly = sl_ests_noPS.groupby(by='Week')[column].agg(['median', lambda x: x.quantile(q1), lambda x: x.quantile(q3)])\n",
    "    weekly.columns = ['Median', 'Q1', 'Q3'] # Rename the columns for clarity\n",
    "    weekly.index = weekly.index.astype(float)\n",
    "    # plot\n",
    "    ax1[i,1].fill_between(weekly.index, weekly['Q1'], weekly['Q3'].values, color='k', alpha=0.5)\n",
    "    ax1[i,1].plot(weekly.index, weekly['Median'], color='k', linewidth=2)\n",
    "    ax1[i,1].grid(True)\n",
    "    # adjust axis\n",
    "    ax1[i,1].set_xlim(15, 45)\n",
    "    ax1[i,1].set_xticks([18, 31, 44])\n",
    "    ax1[i,1].set_xticklabels([])\n",
    "    ax1[i,1].set_xticklabels(['May', 'Aug', 'Nov'])\n",
    "    ax1[i,1].set_ylim(yrange[0], yrange[1])\n",
    "\n",
    "ax1[0,1].set_title('Weekly median trend')\n",
    "\n",
    "# -----Plot glacier area on SCA plots\n",
    "# AOI_fn = glob.glob(os.path.join(study_sites_path, site_name, 'AOIs', site_name + '*USGS*.shp'))[0]\n",
    "# AOI = gpd.read_file(AOI_fn)\n",
    "# ax1[0,0].plot([xmin, xmax], [AOI.geometry[0].area / 1e6, AOI.geometry[0].area / 1e6], '--', color='grey')\n",
    "# ax1[0,1].plot([xmin, xmax], [AOI.geometry[0].area / 1e6, AOI.geometry[0].area / 1e6], '--', color='grey')\n",
    "        \n",
    "# -----Add legend to axis 1\n",
    "ax1[0,0].legend(loc='center', bbox_to_anchor=(0.5, 1.1), handletextpad=0.1, labelspacing=0.5, markerscale=2, ncol=4)\n",
    "fig1.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----Save figures\n",
    "if save_figures:\n",
    "    fig1_fn = os.path.join(figures_out_path, 'timeseries_SouthCascade_Glacier.png')\n",
    "    fig1.savefig(fig1_fn, dpi=300, facecolor='w', edgecolor='none', bbox_inches='tight')\n",
    "    print('figure 1 saved to file: ' + fig1_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9430d0c4",
   "metadata": {},
   "source": [
    "## Snow cover products comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1083e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----Load Landsat fSCA\n",
    "# LS_fn = base_path+'../study-sites/Wolverine/imagery/Landsat/fSCA/LC08_AK_016008_20210829_20210913_02_SNOW/LC08_AK_016008_20210829_20210913_02_VIEWABLE_SNOW_UTM.TIF'\n",
    "# LS = rxr.open_rasterio(LS_fn)\n",
    "# # remove no-data values\n",
    "# LS = LS.where(LS != -9999)\n",
    "# # account for image multiplier\n",
    "# LS_scalar = 0.001\n",
    "# LS = LS * LS_scalar\n",
    "# crs = LS.rio.crs.to_string()\n",
    "\n",
    "# # -----Load MODIS fSCA\n",
    "# M_fn = base_path+'../study-sites/Wolverine/imagery/MODIS/Terra_fSCA/2021_08_15.tif'\n",
    "# M = rxr.open_rasterio(M_fn)\n",
    "# # grab snow cover band\n",
    "# M_fSCA = M.isel(band=0)\n",
    "# # remove no data values\n",
    "# M_fSCA = M_fSCA.where(M_fSCA != -3.2768e04)\n",
    "# # reproject \n",
    "# M_fSCA= M_fSCA.rio.reproject(crs)\n",
    "\n",
    "# # -----Load PlanetScope image and snow\n",
    "# # RGB image\n",
    "# PS_path = base_path+'../study-sites/Wolverine/imagery/PlanetScope/adjusted-filtered/'\n",
    "# PS_fn = '20210815_20_adj.tif'\n",
    "# PS = rxr.open_rasterio(PS_path + PS_fn)\n",
    "# PS = PS / 1e4\n",
    "# # classify image\n",
    "# clf_fn = base_path+'/inputs-outputs/PS_classifier_all_sites.sav'\n",
    "# clf = pickle.load(open(clf_fn, 'rb'))\n",
    "# feature_cols_fn = base_path+'inputs-outputs/PS_feature_cols.pkl'\n",
    "# feature_cols = pickle.load(open(feature_cols_fn,'rb'))\n",
    "# sys.path.insert(1, base_path+'functions/')\n",
    "# from ps_pipeline_utils import classify_image\n",
    "# im_classified_fn, im = classify_image(PS_fn, PS_path, clf, feature_cols, False, None, out_path)\n",
    "# # load classified image\n",
    "# im_classified = rxr.open_rasterio(out_path + im_classified_fn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3180d701",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----Create snow colormap\n",
    "# color_snow = '#4eb3d3'\n",
    "# color_no_snow = 'w'\n",
    "# # create colormap\n",
    "# colors = [color_no_snow, color_snow]\n",
    "# cmp = cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n",
    "\n",
    "# # -----Plot\n",
    "# fig, ax = plt.subplots(2, 2, figsize=(10,10))\n",
    "# ax = ax.flatten()\n",
    "# plt.rcParams.update({'font.size':16, 'font.sans-serif':'Arial'})\n",
    "# xmin, xmax, ymin, ymax = 391, 399, 6694, 6702\n",
    "# # MODIS\n",
    "# M_im = ax[0].imshow(M_fSCA.data, cmap=cmp, clim=(0,100),\n",
    "#                     extent=(np.min(M_fSCA.x.data)/1000, np.max(M_fSCA.x.data)/1000, \n",
    "#                             np.min(M_fSCA.y.data)/1000, np.max(M_fSCA.y.data)/1000))\n",
    "# ax[0].set_xticks(np.linspace(392, 398, num=4))\n",
    "# ax[0].set_yticks(np.linspace(6694, 6702, num=5))\n",
    "# ax[0].set_xticklabels([])\n",
    "# ax[0].set_xlim(xmin, xmax)\n",
    "# ax[0].set_ylim(ymin, ymax)\n",
    "# ax[0].set_ylabel('Northing [km]')\n",
    "# ax[0].set_title('a) MODIS f$_{SCA}$')\n",
    "# # LS\n",
    "# LS_im = ax[1].imshow(LS_fSCA, cmap=cmp, clim=(0,1),\n",
    "#                    extent=(np.min(LS_x)/1000, np.max(LS_x)/1000, np.min(LS_y)/1000, np.max(LS_y)/1000))\n",
    "# ax[1].set_xticks(np.linspace(392, 398, num=4))\n",
    "# ax[1].set_yticks(np.linspace(6694, 6702, num=5))\n",
    "# ax[1].set_xticklabels([])\n",
    "# ax[1].set_yticklabels([])\n",
    "# ax[1].set_xlim(xmin, xmax)\n",
    "# ax[1].set_ylim(ymin, ymax)\n",
    "# ax[1].set_title('b) Landsat 8 f$_{SCA}$')\n",
    "# # PS RGB\n",
    "# ax[2].imshow(np.dstack([PS.data[2], PS.data[1], PS.data[0]]),\n",
    "#            extent=(np.min(PS.x.data)/1000, np.max(PS.x.data)/1000, np.min(PS.y.data)/1000, np.max(PS.y.data)/1000))\n",
    "# ax[2].set_xticks(np.linspace(392, 398, num=4))\n",
    "# ax[2].set_yticks(np.linspace(6694, 6702, num=5))\n",
    "# ax[2].set_xlim(xmin, xmax)\n",
    "# ax[2].set_ylim(ymin, ymax)\n",
    "# ax[2].set_ylabel('Northing [km]')\n",
    "# ax[2].set_xlabel('Easting [km]')\n",
    "# ax[2].set_title('c) PlanetScope RGB')\n",
    "# # PS snow\n",
    "# im_classified = im_classified.where(im_classified!=-9999)\n",
    "# im_binary = xr.where(im_classified<=2, 1, 0)\n",
    "# PS_snow_im = ax[3].imshow(im_binary.data[0], cmap=cmp, clim=(0,1),\n",
    "#                    extent=(np.min(PS.x.data)/1000, np.max(PS.x.data)/1000, np.min(PS.y.data)/1000, np.max(PS.y.data)/1000))\n",
    "# ax[3].set_xticks(np.linspace(392, 398, num=4))\n",
    "# ax[3].set_yticks(np.linspace(6694, 6702, num=5))\n",
    "# ax[3].set_yticklabels([])\n",
    "# ax[3].set_xlim(xmin, xmax)\n",
    "# ax[3].set_ylim(ymin, ymax)\n",
    "# ax[3].set_xlabel('Easting [km]')\n",
    "# ax[3].set_title('d) PlanetScope SCA')\n",
    "# # colorbar\n",
    "# cbar_ax = fig.add_axes([0.92, 0.35, 0.02, 0.3])\n",
    "# fig.colorbar(M_im, cax=cbar_ax)\n",
    "# plt.show()\n",
    "\n",
    "# if save_figures:\n",
    "#     fig.savefig(out_path+'comparing_SCA_products.png', dpi=300, facecolor='white', edgecolor='none')\n",
    "#     print('figure saved to file')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
