{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff1eb620",
   "metadata": {},
   "source": [
    "# Notebook to make figures for presentations, manuscripts, etc.\n",
    "\n",
    "Rainey Aberle\n",
    "\n",
    "2022/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e9e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import contextily as cx\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from skimage.measure import find_contours\n",
    "import ee\n",
    "import sys\n",
    "from shapely.geometry import Point, LineString, Polygon, MultiPolygon\n",
    "import rasterio as rio\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap, LightSource\n",
    "import matplotlib\n",
    "import glob\n",
    "import wxee as wx\n",
    "import matplotlib\n",
    "import pickle\n",
    "from scipy.signal import medfilt\n",
    "from scipy.stats import iqr\n",
    "import os\n",
    "import glob\n",
    "import operator\n",
    "import json\n",
    "from ast import literal_eval\n",
    "import seaborn as sns\n",
    "import wxee as wx\n",
    "import geedim as gd\n",
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "from shapely import wkt\n",
    "\n",
    "# path to snow-cover-mapping/\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/'\n",
    "\n",
    "# path to study-sites/\n",
    "study_sites_path = '/Users/raineyaberle/Google Drive/My Drive/Research/CryoGARS-Glaciology/Advising/student-research/Alexandra-Friel/snow_cover_mapping_application/study-sites/'\n",
    "\n",
    "# determine whether to save output figures\n",
    "save_figures = True\n",
    "\n",
    "# path for saving output figures\n",
    "figures_out_path = os.path.join(base_path, 'figures')\n",
    "\n",
    "# add path to functions\n",
    "sys.path.insert(1, os.path.join(base_path, 'functions'))\n",
    "import pipeline_utils as f\n",
    "\n",
    "# load dataset dictionary\n",
    "dataset_dict = json.load(open(os.path.join(base_path,'inputs-outputs', 'datasets_characteristics.json')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4579e3a-9da2-4e1a-a7b4-da00e6a7cb0a",
   "metadata": {},
   "source": [
    "### Define some colormaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc8b362-d077-481f-b2eb-edb6bca4a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Imagery Datasets\n",
    "color_Landsat = '#ff7f00'\n",
    "color_Sentinel2 = '#984ea3'\n",
    "color_PlanetScope = '#4daf4a'\n",
    "\n",
    "ListedColormap([color_Landsat, color_Sentinel2, color_PlanetScope])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775c7849-b09b-4c83-9477-2af67f9ac92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Classified images\n",
    "# Indicies: 0 = snow, 1 = shadowed snow, 2 = ice, 3 = bare ground, 4 = water\n",
    "colors_classified = list(dataset_dict['classified_image']['class_colors'].values())\n",
    "ListedColormap(colors_classified)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013784a0-a641-4ae4-b9e7-24d20d8afe1f",
   "metadata": {},
   "source": [
    "## Figure 1. Spectral signatures for earth materials and satellite band ranges and example NDSI thresholding applied to an image at Wolverine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9d1ed4-cb10-4ba7-9620-8ebc4a38b268",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# -----Set up figure\n",
    "# define colors for different materials\n",
    "color_snow = colors_classified[0]\n",
    "color_firn = '#12617a'\n",
    "color_ice = colors_classified[2]\n",
    "color_dirty_ice = '#1a2c40'\n",
    "color_veg = '#006d2c'\n",
    "color_rock = colors_classified[3]\n",
    "color_water = colors_classified[4]\n",
    "# plot\n",
    "fontsize = 16\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig = plt.figure(figsize=(12,14))\n",
    "linewidth=2\n",
    "# define axes layout using gridspec\n",
    "gs = gridspec.GridSpec(2, 2, height_ratios=[1, 1])\n",
    "ax = [fig.add_subplot(gs[0, :]), fig.add_subplot(gs[1, 0]), fig.add_subplot(gs[1, 1])]\n",
    "\n",
    "# -----Plot satellite band ranges\n",
    "def draw_boxes(axis, band_ranges, NDSI_indices, y0=0.2, box_height=0.04, \n",
    "               facecolor='#bdbdbd', edgecolor='k', alpha=1.0, NDSI_label=False):\n",
    "    labeled = False\n",
    "    # loop over band ranges\n",
    "    for i, band_range in enumerate(band_ranges):\n",
    "        # convert from nanometers to micrometers\n",
    "        x0, x1 = band_range[0], band_range[1]\n",
    "        # calculate width\n",
    "        box_width = x1-x0\n",
    "        # create rectangle and add to axes\n",
    "        axis.add_patch(Rectangle((x0, y0), width=box_width, height=box_height, \n",
    "                       facecolor=facecolor, edgecolor=edgecolor, alpha=alpha))\n",
    "        # plot star on NDSI bands\n",
    "        if i in NDSI_indices:\n",
    "            if (not labeled) and NDSI_label:\n",
    "                label = 'NDSI bands'\n",
    "                labeled = True\n",
    "            else:\n",
    "                label='_nolegend_'\n",
    "            axis.plot(x0 + box_width/2, y0 + box_height/2, '*k', markersize=10, label=label)\n",
    "\n",
    "    # add one rectangle to contain all bands\n",
    "    x0, x1 = band_ranges[0][0], band_ranges[-1][-1]\n",
    "    box_width = x1-x0\n",
    "    axis.add_patch(Rectangle((x0, y0), width=box_width, height=box_height, facecolor='none', edgecolor='k', alpha=1.0))\n",
    "    return\n",
    "\n",
    "# Landsat 8/9 OLI\n",
    "L_band_ranges = [[0.45, 0.51], [0.53, 0.59], [0.64, 0.67], [0.85, 0.88], # 2, 3, 4, 5\n",
    "                 [1.57, 1.65], [2.11, 2.29]] # 6, 7\n",
    "L_band_names = ['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2']#, 'TIRS1', 'TIRS2']\n",
    "L_NDSI_band_indices = [1, 4]\n",
    "draw_boxes(ax[0], L_band_ranges, L_NDSI_band_indices, y0=1.001, NDSI_label=True, facecolor=color_Landsat)\n",
    "ax[0].text(2.32, 1.005, 'Landsat 8/9 (30 m)')\n",
    "# Sentinel-2 MSI\n",
    "S2_20_band_ranges = [[0.69, 0.718], [0.727, 0.755], [0.764, 0.802], # B5, B6, B7\n",
    "                     [0.845, 0.85], [1.52, 1.70], [2.010, 2.37]] # B8A, B11 (SWIR1), B12 (SWIR2)\n",
    "S2_20_NDSI_band_indices = [4]\n",
    "draw_boxes(ax[0], S2_20_band_ranges, S2_20_NDSI_band_indices, y0=1.101, facecolor=color_Sentinel2)\n",
    "ax[0].text(2.4, 1.105, 'Sentinel-2 (20 m)')\n",
    "\n",
    "S2_10_band_ranges = [[0.425, 0.555], [0.525, 0.595], [0.635, 0.695], # B2 B3 B4 \n",
    "                     [0.728, 1.038]] # B8 (NIR)\n",
    "S2_10_NDSI_band_indices = [1]\n",
    "draw_boxes(ax[0], S2_10_band_ranges, S2_10_NDSI_band_indices, y0=1.201, facecolor=color_Sentinel2)\n",
    "ax[0].text(1.068, 1.205, 'Sentinel-2 (10 m)')\n",
    "# PlanetScope 4-band\n",
    "PS_band_ranges = [[0.455, 0.515], [0.51, 0.59], [0.590, 0.670], [0.780, 0.860]]\n",
    "PS_NDSI_indices = [1, 3]\n",
    "draw_boxes(ax[0], PS_band_ranges, PS_NDSI_indices, y0=1.301, facecolor=color_PlanetScope)\n",
    "ax[0].text(0.90, 1.305, 'PlanetScope 4-band (3-5 m)')\n",
    "\n",
    "# -----Load spectral signatures data and plot\n",
    "# Painter et al. (2009): snow, coarse- to fine-grained\n",
    "spec_path_painter = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/write-ups/CH1_snow_cover_mapping_methods_manuscript/figures/spectral_signatures_Painter_et_al_2009'\n",
    "os.chdir(spec_path_painter)\n",
    "coarse_snow = pd.read_csv('coarse_snow_Painter_et_al_2009.csv', header=None)\n",
    "fine_snow = pd.read_csv('fine_snow_Painter_et_al_2009.csv', header=None)\n",
    "# interpolate to same x values\n",
    "x_snow = np.linspace(0, 2.5, num=100)\n",
    "y_coarse = np.interp(x_snow, coarse_snow[0].values, coarse_snow[1].values)\n",
    "y_fine = np.interp(x_snow, fine_snow[0].values, fine_snow[1].values)\n",
    "y_med = np.array([np.nanmean([y1, y2]) for y1, y2 in list(zip(y_coarse, y_fine))])\n",
    "# plot\n",
    "ax[0].fill_between(x_snow, y_fine, y_coarse, color=color_snow, alpha=0.4)\n",
    "ax[0].plot(x_snow, y_med, '-', color=color_snow, linewidth=linewidth, label='snow')\n",
    "# Salvatori et al. (2022): ice \n",
    "spec_path_salv = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/write-ups/CH1_snow_cover_mapping_methods_manuscript/figures/spectral_signatures_Salvatori_et_al_2022'\n",
    "os.chdir(spec_path_salv)\n",
    "ice = pd.read_csv('ice.csv', header=None)\n",
    "# interpolate min, max, and mean values\n",
    "x_ice = np.linspace(0.4, 2.4, num=50)\n",
    "dx = x_ice[1] - x_ice[0]\n",
    "y_ice_min, y_ice_max, y_ice_mean = np.zeros(len(x_ice)), np.zeros(len(x_ice)), np.zeros(len(x_ice))\n",
    "for i in range(0,len(x_ice)-1):\n",
    "    x_window = [x_ice[i] - dx/2, x_ice[i] + dx/2]\n",
    "    if np.any((ice[0].values > x_window[0]) & (ice[0].values < x_window[1])):\n",
    "        y_ice_min[i] = np.nanmin(ice[1].values[(ice[0].values > x_window[0]) & (ice[0].values < x_window[1])])\n",
    "        y_ice_max[i] = np.nanmax(ice[1].values[(ice[0].values > x_window[0]) & (ice[0].values < x_window[1])])\n",
    "        y_ice_mean[i] = np.nanmean(ice[1].values[(ice[0].values > x_window[0]) & (ice[0].values < x_window[1])])\n",
    "    else:\n",
    "        y_ice_min[i], y_ice_max[i], y_ice_mean[i] = np.nan, np.nan, np.nan\n",
    "# plot\n",
    "ax[0].fill_between(x_ice, y_ice_min, y_ice_max, facecolor=color_ice, edgecolor=None, alpha=0.4)\n",
    "ax[0].plot(x_ice, y_ice_mean, '-', color=color_ice, linewidth=linewidth, label='ice and firn')\n",
    "# Zeng et al. (1984) / Hendriks et al. (2003): firn\n",
    "# spec_path_zeng = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/write-ups/CH1_snow_cover_mapping_methods_manuscript/figures/spectral_signatures_firn_Hendriks_et_al_2003'\n",
    "# firn = pd.read_csv(os.path.join(spec_path_zeng, 'Firn.csv'), header=None)\n",
    "# firn.rename(columns={0:'Wavelength', 1:'Reflectance'}, inplace=True)\n",
    "# firn.sort_values(by='Wavelength', inplace=True)\n",
    "# firn['Wavelength'] = firn['Wavelength'] / 1000\n",
    "# firn.loc[firn['Reflectance'] <= 0] = np.nan\n",
    "# firn.dropna(inplace=True)\n",
    "# firn.reset_index(drop=True, inplace=True)\n",
    "# dirty_ice = pd.read_csv(os.path.join(spec_path_zeng, 'Dirty glacier ice.csv'), header=None)\n",
    "# dirty_ice.rename(columns={0:'Wavelength', 1:'Reflectance'}, inplace=True)\n",
    "# dirty_ice.sort_values(by='Wavelength', inplace=True)\n",
    "# dirty_ice['Wavelength'] = dirty_ice['Wavelength'] / 1000\n",
    "# dirty_ice.reset_index(drop=True, inplace=True)\n",
    "# # plot\n",
    "# ax[0].plot(firn['Wavelength'], firn['Reflectance'], '-', color=color_firn, label='firn')\n",
    "# ax[0].plot(dirty_ice['Wavelength'], dirty_ice['Reflectance'], '-', color=color_dirty_ice, label='dirty glacier ice')\n",
    "\n",
    "# USGS: vegetation, soil, seawater\n",
    "colors = [color_veg, color_rock, color_water]\n",
    "spec_path_usgs = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/write-ups/CH1_snow_cover_mapping_methods_manuscript/figures/spectral_signatures_USGS/'\n",
    "os.chdir(spec_path_usgs)\n",
    "# define prefixes used in file names for each material\n",
    "prefixes = ['Aspen', 'Basalt', 'Seawater']\n",
    "# define labels for plot\n",
    "labels = ['vegetation', 'soil', 'seawater']\n",
    "# loop through prefixes\n",
    "for i, prefix in enumerate(prefixes):\n",
    "    # grab folder name\n",
    "    folder = glob.glob('*'+prefix+'*')[0]\n",
    "    # load wavelengths\n",
    "    wave_fn = glob.glob(folder + '/*Wavelengths*.txt')[0]\n",
    "    wave = pd.read_csv(wave_fn)\n",
    "    wave = wave[wave.keys()[0]].values\n",
    "    if prefix=='Basalt':\n",
    "        refl_fn = glob.glob(folder + '/*'+prefix+'*.txt')[1]\n",
    "    else:\n",
    "        refl_fn = glob.glob(folder + '/*'+prefix+'*.txt')[0]\n",
    "    refl = pd.read_csv(refl_fn)\n",
    "    refl = refl[refl.keys()[0]].values\n",
    "    refl[refl<0] = np.nan\n",
    "    # plot\n",
    "    ax[0].plot(wave, refl, '-', color=colors[i], linewidth=linewidth, label=labels[i])\n",
    "    \n",
    "ax[0].grid(True)\n",
    "ax[0].set_xlim(0.4, 3.3)\n",
    "ax[0].set_ylim(0, 1.4)\n",
    "ax[0].set_yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax[0].legend(loc='center right', bbox_to_anchor=[0.8, 0.3, 0.2, 0.2])\n",
    "ax[0].set_xlabel('Wavelength [$\\mu$m]')\n",
    "ax[0].set_ylabel('Reflectance')\n",
    "ax[0].text((ax[0].get_xlim()[1] - ax[0].get_xlim()[0])*0.94 + ax[0].get_xlim()[0],\n",
    "           (ax[0].get_ylim()[1] - ax[0].get_ylim()[0])*0.07 + ax[0].get_ylim()[0], 'a', fontweight='bold', fontsize=fontsize+2, \n",
    "           bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "\n",
    "# -----Load Landsat image at Wolverine\n",
    "# image datetime\n",
    "dt = np.datetime64('2020-08-17')\n",
    "# load AOI\n",
    "AOI_fn = glob.glob(study_sites_path + 'Wolverine/AOIs/*USGS*outline*.shp')[0]\n",
    "AOI_UTM = gpd.read_file(AOI_fn)\n",
    "# initialize GEE\n",
    "ee.Initialize()\n",
    "\n",
    "def query_gee_for_image(dt, aoi_utm):\n",
    "    # -----Grab datetime from snowline df\n",
    "    date_start = str(dt - np.timedelta64(1, 'D'))\n",
    "    date_end = str(dt + np.timedelta64(1, 'D'))\n",
    "    # -----Buffer AOI by 1km\n",
    "    aoi_utm_buffer = aoi_utm.buffer(1e3)\n",
    "    # determine bounds for image plotting\n",
    "    bounds = aoi_utm_buffer.geometry[0].bounds\n",
    "    # -----Reformat AOI for image filtering\n",
    "    # reproject CRS from AOI to WGS\n",
    "    aoi_wgs = aoi_utm.to_crs('EPSG:4326')\n",
    "    aoi_buffer_wgs = aoi_utm_buffer.to_crs('EPSG:4326')\n",
    "    # prepare AOI for querying geedim (AOI bounding box)\n",
    "    region = {'type': 'Polygon',\n",
    "              'coordinates': [[[aoi_buffer_wgs.geometry.bounds.minx[0], aoi_buffer_wgs.geometry.bounds.miny[0]],\n",
    "                               [aoi_buffer_wgs.geometry.bounds.maxx[0], aoi_buffer_wgs.geometry.bounds.miny[0]],\n",
    "                               [aoi_buffer_wgs.geometry.bounds.maxx[0], aoi_buffer_wgs.geometry.bounds.maxy[0]],\n",
    "                               [aoi_buffer_wgs.geometry.bounds.minx[0], aoi_buffer_wgs.geometry.bounds.maxy[0]],\n",
    "                               [aoi_buffer_wgs.geometry.bounds.minx[0], aoi_buffer_wgs.geometry.bounds.miny[0]]\n",
    "                               ]]}\n",
    "    region_buffer_ee = ee.Geometry.Polygon([[[aoi_buffer_wgs.geometry.bounds.minx[0], aoi_buffer_wgs.geometry.bounds.miny[0]],\n",
    "                                              [aoi_buffer_wgs.geometry.bounds.maxx[0], aoi_buffer_wgs.geometry.bounds.miny[0]],\n",
    "                                              [aoi_buffer_wgs.geometry.bounds.maxx[0], aoi_buffer_wgs.geometry.bounds.maxy[0]],\n",
    "                                              [aoi_buffer_wgs.geometry.bounds.minx[0], aoi_buffer_wgs.geometry.bounds.maxy[0]],\n",
    "                                              [aoi_buffer_wgs.geometry.bounds.minx[0], aoi_buffer_wgs.geometry.bounds.miny[0]]\n",
    "                                            ]])\n",
    "\n",
    "    # -----Query GEE for Landsat 8 imagery\n",
    "    im_col_gd = gd.MaskedCollection.from_name('LANDSAT/LC08/C02/T1_L2').search(start_date=date_start,\n",
    "                                                                               end_date=date_end,\n",
    "                                                                               mask=True,\n",
    "                                                                               region=region,\n",
    "                                                                               fill_portion=50)\n",
    "    im_col_ee = im_col_gd.ee_collection\n",
    "    \n",
    "    # -----Return first image as xarray.Dataset\n",
    "     # Grab first image\n",
    "    im_ee = im_col_ee.first()\n",
    "    # create MaskedImage from ID\n",
    "    im_gd = gd.MaskedImage.from_id(im_ee.getInfo()['id'], mask=False, region=region)\n",
    "    # convert to ee.Image\n",
    "    im_ee = ee.Image(im_gd.ee_image).select(im_gd.refl_bands)\n",
    "    # convert to xarray.Datasets\n",
    "    crs = str(AOI_UTM.crs.to_epsg())\n",
    "    im_xr = im_ee.wx.to_xarray(scale=30, region=region, crs='EPSG: '+ crs)\n",
    "    # account for image scalar\n",
    "    im_xr = xr.where(im_xr != dataset_dict['Landsat']['no_data_value'],\n",
    "                     im_xr / dataset_dict['Landsat']['image_scalar'], np.nan)\n",
    "    # set CRS\n",
    "    im_xr.rio.write_crs('EPSG:' + crs, inplace=True)\n",
    "    \n",
    "    return im_xr\n",
    "\n",
    "im_xr = query_gee_for_image(dt, AOI_UTM)\n",
    "\n",
    "# -----Calcualte NDSI\n",
    "NDSI = ((im_xr[dataset_dict['Landsat']['NDSI_bands'][0]].data[0] - im_xr[dataset_dict['Landsat']['NDSI_bands'][1]].data[0])\n",
    "        / (im_xr[dataset_dict['Landsat']['NDSI_bands'][0]].data[0] + im_xr[dataset_dict['Landsat']['NDSI_bands'][1]].data[0]))\n",
    "NDSI_thresh = np.where(NDSI > 0.4, 1, 0)\n",
    "NDSI_cmap = ListedColormap(['white', dataset_dict['classified_image']['class_colors']['Snow']])\n",
    "\n",
    "# ----- Plot\n",
    "xticks = np.arange(int(np.min(im_xr.x.data)/1e3)+1, np.max(im_xr.x.data)/1e3, step=2)\n",
    "yticks = np.arange(int(np.min(im_xr.y.data)/1e3)+1, np.max(im_xr.y.data)/1e3, step=2)\n",
    "ax[1].imshow(np.dstack([im_xr['SR_B4'].data[0], im_xr['SR_B3'].data[0], im_xr['SR_B2'].data[0]]),\n",
    "             extent=(np.min(im_xr.x.data)/1e3, np.max(im_xr.x.data)/1e3, np.min(im_xr.y.data)/1e3, np.max(im_xr.y.data)/1e3))\n",
    "ax[1].set_xticks(xticks)\n",
    "ax[1].set_yticks(yticks)\n",
    "ax[1].grid(False)\n",
    "ax[1].set_xlabel('Easting [km]')\n",
    "ax[1].set_ylabel('Northing [km]')\n",
    "ax[1].text((ax[1].get_xlim()[1] - ax[1].get_xlim()[0])*0.9 + ax[1].get_xlim()[0],\n",
    "           (ax[1].get_ylim()[1] - ax[1].get_ylim()[0])*0.07 + ax[1].get_ylim()[0], 'b', fontweight='bold', fontsize=fontsize+2, \n",
    "           bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "ax[2].imshow(NDSI_thresh, cmap=NDSI_cmap,\n",
    "             extent=(np.min(im_xr.x.data)/1e3, np.max(im_xr.x.data)/1e3, np.min(im_xr.y.data)/1e3, np.max(im_xr.y.data)/1e3))\n",
    "ax[2].set_xticks(xticks)\n",
    "ax[2].set_yticks(yticks)\n",
    "ax[2].grid(False)\n",
    "ax[2].set_xlabel('Easting [km]')\n",
    "ax[2].text((ax[2].get_xlim()[1] - ax[2].get_xlim()[0])*0.9 + ax[2].get_xlim()[0],\n",
    "           (ax[2].get_ylim()[1] - ax[2].get_ylim()[0])*0.07 + ax[2].get_ylim()[0], 'c', fontweight='bold', fontsize=fontsize+2, \n",
    "           bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "# plot dummy points for legend\n",
    "xlim, ylim = ax[2].get_xlim(), ax[2].get_ylim()\n",
    "ax[2].plot(0, 0, 's', markersize=15, markerfacecolor=NDSI_cmap(1), markeredgecolor='k', linewidth=1, label='classified as snow')\n",
    "ax[2].plot(0, 0, 's', markersize=15, markerfacecolor='w', markeredgecolor='k', linewidth=1, label='classified as no snow')\n",
    "ax[2].legend(loc='lower left')\n",
    "# reset axis limits\n",
    "ax[2].set_xlim(xlim)\n",
    "ax[2].set_ylim(ylim)\n",
    "\n",
    "# annotations\n",
    "# snow\n",
    "ax[1].arrow(397, 6699, -2, 1, color=colors_classified[0], width=0.05)\n",
    "ax[1].text(397, 6698.9, 'snow', color=colors_classified[0], fontsize='large',\n",
    "           bbox=dict(facecolor='white', edgecolor=colors_classified[0], linewidth=3, pad=5))\n",
    "# firn\n",
    "ax[1].arrow(397, 6697.5, -1.8, 1.6, color=color_firn, width=0.05)\n",
    "ax[1].text(397, 6697.4, 'firn', color=color_firn, fontsize='large',\n",
    "           bbox=dict(facecolor='white', edgecolor=color_firn, linewidth=3, pad=5))\n",
    "# ice\n",
    "ax[1].arrow(397, 6696, -2, 1, color=colors_classified[2], width=0.05)\n",
    "ax[1].text(397, 6695.9, 'ice', color=colors_classified[2], fontsize='large',\n",
    "           bbox=dict(facecolor='white', edgecolor=colors_classified[2], linewidth=3, pad=5))\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----Save figure to file\n",
    "if save_figures:\n",
    "    fig_fn = os.path.join(figures_out_path, 'spectral_signatures_satellite_bands.png')\n",
    "    fig.savefig(fig_fn, facecolor='w', dpi=300, bbox_inches='tight')\n",
    "    print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593f951a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Figure 2. Study sites - USGS Benchmark Glaciers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc8c5b-b16b-4abf-8cf4-307a8b589a30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Load RGI data\n",
    "# path to RGI data\n",
    "rgi_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/GIS_data/RGI/'\n",
    "# RGI shapefile names\n",
    "rgi_fns = ['01_rgi60_Alaska/01_rgi60_Alaska.shp', \n",
    "           '02_rgi60_WesternCanadaUS/02_rgi60_WesternCanadaUS.shp']\n",
    "# load and combine rgis\n",
    "rgis = gpd.GeoDataFrame()\n",
    "for rgi_fn in rgi_fns:\n",
    "    rgi = gpd.read_file(rgi_path + rgi_fn)\n",
    "    rgis = pd.concat([rgis, rgi])\n",
    "rgis.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302aee73-a407-4cc7-9665-4f24cb931f4a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Count number of glaciers with areas < 1000 km^2 (for manuscript)\n",
    "print('Total # of glaciers in RGI regions 1 and 2 = ', len(rgis))\n",
    "print('Number of glaciers with areas < 1 km^2 = ', len(rgis.loc[rgis['Area'] < 1]))\n",
    "print('Percentage of glaciers with areas < 1 km^2 = ', len(rgis.loc[rgis['Area'] < 1]) / len(rgis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a209a3d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Define site specifics\n",
    "site_names = ['Gulkana', 'Wolverine', 'LemonCreek', 'Sperry', 'SouthCascade']\n",
    "site_names_display = [x.replace('C', ' C') for x in site_names]\n",
    "site_colors = ['#1f78b4', '#33a02c', '#fec44f', '#cc4c02', '#984ea3']\n",
    "text_labels = ['a)', 'b)', 'c)', 'd)', 'e)']\n",
    "\n",
    "# -----Define colormap for elevations\n",
    "cmap_elev = plt.cm.terrain(np.linspace(0, 1, 100))\n",
    "cmap_elev = ListedColormap(cmap_elev[25:, :])\n",
    "\n",
    "# ----Function for formatting contour labels\n",
    "def fmt(x):\n",
    "    s = f\"{x:.1f}\"\n",
    "    if s.endswith(\"0\"):\n",
    "        s = f\"{x:.0f}\"\n",
    "    return rf\"{s} m\" if plt.rcParams[\"text.usetex\"] else f\"{s} m\"\n",
    "\n",
    "# -----Set up figure\n",
    "fontsize = 18\n",
    "fig, ax = plt.subplots(2, 3, figsize=(16, 12), layout='constrained')\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "ax = ax.flatten()\n",
    "epsg_A = 32610\n",
    "\n",
    "# -----Loop through sites\n",
    "i=0\n",
    "for site_name, site_color, site_name_display, text_label in list(zip(site_names, site_colors, site_names_display, text_labels)):\n",
    "    ### AOI\n",
    "    # load file\n",
    "    AOI_fn = glob.glob(study_sites_path + site_name + '/AOIs/' + site_name + '_USGS_*.shp')[0]\n",
    "    AOI = gpd.read_file(AOI_fn)\n",
    "    AOI_WGS = AOI.to_crs(4326)\n",
    "    # solve for optimal UTM zone\n",
    "    AOI_centroid = [AOI_WGS.geometry[0].centroid.xy[0][0],\n",
    "                    AOI_WGS.geometry[0].centroid.xy[1][0]]\n",
    "    epsg_UTM = f.convert_wgs_to_utm(AOI_centroid[0], AOI_centroid[1])\n",
    "    # reproject\n",
    "    AOI_UTM = AOI_WGS.to_crs(epsg_UTM)\n",
    "    AOI_A = AOI.to_crs(epsg_A)\n",
    "    ### DEM\n",
    "    # DEM_fn = glob.glob(study_sites_path + site_name + '/DEMs/' + site_name + '*_clip.tif')[0]\n",
    "    # DEM = xr.open_dataset(DEM_fn)\n",
    "    # DEM = DEM.rename({'band_data': 'elevation'})\n",
    "    # # reproject \n",
    "    # DEM = DEM.rio.reproject(str('EPSG:'+epsg_UTM))\n",
    "    # create meshgrid of coordinates\n",
    "    # X_mesh, Y_mesh = np.meshgrid(DEM.x.data, DEM.y.data)\n",
    "    ### Plot\n",
    "    # A) Study sites map\n",
    "    ax[0].plot(AOI_A.geometry[0].centroid.xy[0][0], AOI_A.geometry[0].centroid.xy[1][0], \n",
    "            '.', markerfacecolor=site_color, markeredgecolor='k', markersize=5)\n",
    "    ax[0].text(AOI_A.geometry[0].centroid.xy[0][0], AOI_A.geometry[0].centroid.xy[1][0],\n",
    "               text_labels[i].replace(')',''), bbox=dict(facecolor='white', edgecolor='black', pad=5))\n",
    "    # Individual glacier plot\n",
    "    AOI_UTM.plot(ax=ax[i+1], edgecolor='k', facecolor='none', linewidth=2)\n",
    "    # CS = ax[i+1].contour(X_mesh, Y_mesh, DEM.elevation.data[0], levels=4, colors='grey')\n",
    "    # ax[i+1].clabel(CS, CS.levels, inline=True, fmt=fmt, fontsize=10, colors='grey')\n",
    "    if AOI.geometry[0].geom_type=='MultiPolygon':\n",
    "        xmin_AOI = np.min([np.min(geom.exterior.coords.xy[0]) for geom in AOI.geometry[0].geoms])\n",
    "        xmax_AOI = np.max([np.max(geom.exterior.coords.xy[0]) for geom in AOI.geometry[0].geoms])\n",
    "        ymin_AOI = np.min([np.min(geom.exterior.coords.xy[1]) for geom in AOI.geometry[0].geoms])\n",
    "        ymax_AOI = np.max([np.max(geom.exterior.coords.xy[1]) for geom in AOI.geometry[0].geoms])      \n",
    "    else:\n",
    "        xmin_AOI = np.min(AOI.geometry[0].exterior.coords.xy[0])\n",
    "        xmax_AOI = np.max(AOI.geometry[0].exterior.coords.xy[0])\n",
    "        ymin_AOI = np.min(AOI.geometry[0].exterior.coords.xy[1])\n",
    "        ymax_AOI = np.max(AOI.geometry[0].exterior.coords.xy[1])  \n",
    "    xmin = xmin_AOI - 0.1*(xmax_AOI - xmin_AOI)\n",
    "    xmax = xmax_AOI + 0.1*(xmax_AOI - xmin_AOI)\n",
    "    ymin = ymin_AOI - 0.1*(ymax_AOI - ymin_AOI)\n",
    "    ymax = ymax_AOI + 0.1*(ymax_AOI - ymin_AOI) \n",
    "    # change x and y tick labels to km\n",
    "    ax[i+1].set_xlim(xmin, xmax)\n",
    "    ax[i+1].set_ylim(ymin, ymax)\n",
    "    if i < 3:\n",
    "        ax[i+1].set_xticks(np.arange(np.round(xmin,-3), np.round(xmax,-3), 2e3))\n",
    "        ax[i+1].set_yticks(np.arange(np.round(ymin,-3), np.round(ymax,-3), 2e3)) \n",
    "    else:\n",
    "        ax[i+1].set_xticks(np.arange(np.round(xmin,-3), np.round(xmax,-3), 1e3))\n",
    "        ax[i+1].set_yticks(np.arange(np.round(ymin,-3), np.round(ymax,-3), 1e3)) \n",
    "    ax[i+1].set_xticklabels([str(int(x/1e3)) for x in ax[i+1].get_xticks()])\n",
    "    ax[i+1].set_yticklabels([str(int(y/1e3)) for y in ax[i+1].get_yticks()])\n",
    "    ax[i+1].set_title(text_label + ' ' + site_name_display + ' Glacier')\n",
    "    ax[i+1].grid()\n",
    "    # add axes labels\n",
    "    if (i==1) or (i==3):\n",
    "        ax[i].set_ylabel('Northing [km]')\n",
    "    if i > 1:\n",
    "        ax[i+1].set_xlabel('Easting [km]')\n",
    "    cx.add_basemap(ax[i+1], crs='EPSG:'+str(epsg_UTM), source=cx.providers.Esri.WorldImagery, attribution=False)\n",
    "\n",
    "    # increase loop counter\n",
    "    i+=1\n",
    "\n",
    "# A: study sites map\n",
    "ax[0].set_xlim(-2000000, 1500000)\n",
    "ax[0].set_ylim(5000000, 7800000)\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "cx.add_basemap(ax[0], crs='EPSG:'+str(epsg_A), source=cx.providers.Esri.WorldGrayCanvas, attribution=False)\n",
    "rgis_reproj = rgis.to_crs('EPSG:'+str(epsg_A))\n",
    "rgis_reproj.plot(ax=ax[0], facecolor=colors_classified[2], edgecolor=colors_classified[2])\n",
    "# fig.colorbar(DEM_im, ax=[ax[2], ax[5]], shrink=0.5, label='Elevation [m]')\n",
    "plt.show()\n",
    "\n",
    "if save_figures:\n",
    "    fig_fn = os.path.join(figures_out_path, 'study_sites.png')\n",
    "    fig.savefig(fig_fn, dpi=300, facecolor='white', edgecolor='none', bbox_inches='tight')\n",
    "    print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d29c3a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Figure 3. Methods workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00177f0c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "font_size = 32\n",
    "save_figures = 1\n",
    "\n",
    "# -----Load dataset dictionary\n",
    "with open(base_path + 'inputs-outputs/datasets_characteristics.json') as fn:\n",
    "    dataset_dict = json.load(fn)\n",
    "dataset = 'PlanetScope'\n",
    "\n",
    "# -----Image settings\n",
    "# site name\n",
    "site_name = 'SouthCascade'\n",
    "# create colormap for classified image\n",
    "cmp = ListedColormap(colors_classified)\n",
    "\n",
    "# -----Load AOI as gpd.GeoDataFrame\n",
    "AOI_fn = study_sites_path + site_name + '/AOIs/' + site_name + '_USGS_*.shp'\n",
    "AOI_fn = glob.glob(AOI_fn)[0]\n",
    "AOI = gpd.read_file(AOI_fn)\n",
    "# reproject the AOI to WGS to solve for the optimal UTM zone\n",
    "AOI_WGS = AOI.to_crs(4326)\n",
    "AOI_WGS_centroid = [AOI_WGS.geometry[0].centroid.xy[0][0],\n",
    "                    AOI_WGS.geometry[0].centroid.xy[1][0]]\n",
    "epsg_UTM = f.convert_wgs_to_utm(AOI_WGS_centroid[0], AOI_WGS_centroid[1])\n",
    "# reproject AOI to UTM\n",
    "AOI_UTM = AOI.to_crs(str(epsg_UTM))\n",
    "\n",
    "# -----Load DEM as Xarray DataSet\n",
    "DEM_fn = study_sites_path + site_name + '/DEMs/' + site_name + '*_DEM*.tif'\n",
    "# load DEM as xarray DataSet\n",
    "DEM_fn = glob.glob(DEM_fn)[0]\n",
    "DEM = xr.open_dataset(DEM_fn)\n",
    "DEM = DEM.rename({'band_data': 'elevation'})\n",
    "# reproject the DEM to the optimal UTM zone\n",
    "DEM = DEM.rio.reproject(str('EPSG:'+epsg_UTM))\n",
    "# remove unnecessary data (possible extra bands from ArcticDEM or other DEM)\n",
    "if len(np.shape(DEM.elevation.data))>2:\n",
    "    DEM['elevation'] = DEM.elevation[0]\n",
    "    \n",
    "# -----1. Raw image\n",
    "im_path = study_sites_path + site_name + '/imagery/PlanetScope/mosaics/'\n",
    "im_fn = '20210924_18.tif'\n",
    "im = xr.open_dataset(im_path + im_fn)\n",
    "# determine image date from image mosaic file name\n",
    "im_date = im_fn[0:4] + '-' + im_fn[4:6] + '-' + im_fn[6:8] + 'T' + im_fn[9:11] + ':00:00'\n",
    "im_dt = np.datetime64(im_date)\n",
    "xmin, xmax, ymin, ymax = np.min(im.x.data), np.max(im.x.data), np.min(im.y.data), np.max(im.y.data)\n",
    "# plot\n",
    "fig1, ax1 = plt.subplots(figsize=(8,8))\n",
    "ax1.imshow(np.dstack([im.band_data.data[2]/1e4, im.band_data.data[1]/1e4, im.band_data.data[0]/1e4]), \n",
    "           extent=(xmin, xmax, ymin, ymax))\n",
    "AOI.plot(ax=ax1, facecolor='none', edgecolor='k', linewidth=3)\n",
    "ax1.set_xlim(xmin, xmax)\n",
    "ax1.set_ylim(ymin, ymax)\n",
    "ax1.axis('off')\n",
    "\n",
    "# -----2. Adjusted image\n",
    "polygon_top, polygon_bottom = f.create_aoi_elev_polys(AOI_UTM, DEM)\n",
    "im_adj, im_adj_method = f.planetscope_adjust_image_radiometry(im, im_dt, polygon_top, polygon_bottom, dataset_dict, skip_clipped=False)\n",
    "# plot\n",
    "fig2, ax2 = plt.subplots(figsize=(8,8))\n",
    "ax2.imshow(np.dstack([im_adj.Red.data[0], im_adj.Green.data[0], im_adj.Blue.data[0]]), \n",
    "           extent=(xmin, xmax, ymin, ymax))\n",
    "AOI_UTM.plot(ax=ax2, facecolor='none', edgecolor='k', linewidth=3)\n",
    "ax2.set_xlim(xmin, xmax)\n",
    "ax2.set_ylim(ymin, ymax)\n",
    "ax2.axis('off')\n",
    "\n",
    "# -----3. Classified image\n",
    "im_classified_path = study_sites_path + site_name + '/imagery/classified/'\n",
    "im_classified_fn = '20210924T180000_SouthCascade_PlanetScope_classified.nc'\n",
    "im_classified = xr.open_dataset(im_classified_path + im_classified_fn)\n",
    "# remove no data values\n",
    "im_classified = xr.where(im_classified==-9999, np.nan, im_classified)\n",
    "# plot\n",
    "fig3, ax3 = plt.subplots(figsize=(8,8))\n",
    "plt.rcParams.update({'font.size':font_size, 'font.sans-serif':'Arial'})\n",
    "ax3.imshow(im_classified.classified.data[0], cmap=cmp, vmin=1, vmax=5,\n",
    "           extent=(xmin, xmax, ymin, ymax))\n",
    "AOI.plot(ax=ax3, facecolor='none', edgecolor='k', linewidth=3)\n",
    "# plot dummy points for legend\n",
    "ax3.scatter(0, 0, marker='s', color=colors_classified[0], s=300, label='snow')\n",
    "ax3.scatter(0, 0, marker='s', color=colors_classified[1], s=300, label='shadowed snow')\n",
    "ax3.scatter(0, 0, marker='s', color=colors_classified[2], s=300, label='ice')\n",
    "ax3.scatter(0, 0, marker='s', color=colors_classified[3], s=300, label='rock')\n",
    "ax3.scatter(0, 0, marker='s', color=colors_classified[4], s=300, label='water')\n",
    "ax3.set_xlim(xmin, xmax+300)\n",
    "ax3.set_ylim(ymin, ymax)\n",
    "ax3.legend(loc='center right', bbox_to_anchor=[1.3, 0.7, 0.2, 0.2])\n",
    "ax3.axis('off')\n",
    "\n",
    "# -----4. Snow edges\n",
    "# create binary snow matrix\n",
    "im_binary = xr.where(im_classified.classified.data[0] <=2, 1, 0)\n",
    "# Find contours at a constant value of 0.5 (between 0 and 1)\n",
    "contours = find_contours(im_binary, 0.5)\n",
    "# convert contour points to image coordinates\n",
    "contours_coords = []\n",
    "for contour in contours: \n",
    "    ix = np.round(contour[:,1]).astype(int)\n",
    "    iy = np.round(contour[:,0]).astype(int)\n",
    "    coords = (im_adj.isel(x=ix, y=iy).x.data, # image x coordinates\n",
    "              im_adj.isel(x=ix, y=iy).y.data) # image y coordinates\n",
    "    # zip points together\n",
    "    xy = list(zip([x for x in coords[0]], \n",
    "                  [y for y in coords[1]]))\n",
    "    contours_coords = contours_coords + [xy]\n",
    "# plot\n",
    "fig4, ax4 = plt.subplots(figsize=(8,8))\n",
    "plt.rcParams.update({'font.size':font_size, 'font.sans-serif':'Arial'})\n",
    "binary_plt = ax4.imshow(im_binary, cmap='Greys')\n",
    "for i, contour in list(zip(np.arange(0,len(contours)), contours)):\n",
    "    if i==0:\n",
    "        plt.plot(contour[:,1], contour[:,0], '-c', label='edges', linewidth=2)\n",
    "    else:\n",
    "        plt.plot(contour[:,1], contour[:,0], '-c', label='_nolegend', linewidth=2)\n",
    "# plot dummy points for legend\n",
    "ax4.scatter(np.array([-10, -9]),np.array([-10, -9]), edgecolor='k', facecolor='k', s=100, label='snow')\n",
    "ax4.scatter(np.array([-10, -9]),np.array([-10, -9]), edgecolor='k', facecolor='w', s=100, label='no snow')\n",
    "ax4.set_xlim(0,len(im.x.data)+300)\n",
    "ax4.set_ylim(len(im.y.data), 0)\n",
    "ax4.legend(loc='upper right', bbox_to_anchor=[0.9, 0.8, 0.2, 0.2])\n",
    "ax4.axis('off')\n",
    "\n",
    "# -----5. Snow line\n",
    "snowlines_fn = study_sites_path + site_name + '/imagery/snowlines/20210924T180000_SouthCascade_PlanetScope_snowline.csv'\n",
    "snowlines = pd.read_csv(snowlines_fn)\n",
    "snowlines_X = snowlines.snowlines_coords_X.apply(literal_eval)[0]\n",
    "snowlines_Y = snowlines.snowlines_coords_Y.apply(literal_eval)[0]\n",
    "\n",
    "# plot\n",
    "fig5, ax5 = plt.subplots(figsize=(8,8))\n",
    "plt.rcParams.update({'font.size':font_size, 'font.sans-serif':'Arial'})\n",
    "binary_plt = ax5.imshow(im_binary, \n",
    "                        extent=(xmin, xmax, ymin, ymax),\n",
    "                        cmap='Greys')\n",
    "ax5.plot(snowlines_X, snowlines_Y, '.m', label='_nolegend', markersize=10)\n",
    "ax5.plot(-20, -20, 'm', label='snowline', linewidth=5)\n",
    "# plot dummy points for legend\n",
    "ax5.scatter(np.array([-10, -9]),np.array([-10, -9]), edgecolor='k', facecolor='k', s=100, label='snow')\n",
    "ax5.scatter(np.array([-10, -9]),np.array([-10, -9]), edgecolor='k', facecolor='w', s=100, label='no snow')\n",
    "ax5.set_xlim(xmin, xmax+300)\n",
    "ax5.set_ylim(ymin, ymax)\n",
    "ax5.legend(loc='center right', bbox_to_anchor=[1.0, 0.6, 0.2, 0.2])\n",
    "ax5.axis('off')\n",
    "plt.show()\n",
    "\n",
    "if save_figures:\n",
    "    fig_fns = ['methods_workflow_1.png', 'methods_workflow_2.png', 'methods_workflow_3.png', \n",
    "               'methods_workflow_4.png', 'methods_workflow_5.png']\n",
    "    for fig_fn, fig in list(zip(fig_fns, [fig1, fig2, fig3, fig4, fig5])):\n",
    "        fig_fn = os.path.join(figures_out_path, fig_fn)\n",
    "        fig.savefig(fig_fn, dpi=300, facecolor='white', edgecolor='none', bbox_inches='tight')\n",
    "        print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57233c12-414d-4b4d-8b04-5aebb96b5bc7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Figure 4. Images used for classification performance assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af0da72-602b-428b-aa1e-9c6114f008c7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "\n",
    "data_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/snow_cover_mapping/classified-points/assessment/'\n",
    "\n",
    "# grab Sentinel-2_SR image file names\n",
    "os.chdir(data_path)\n",
    "im_fns = sorted(glob.glob('*Sentinel-2_SR*.tif'))\n",
    "# plot Lemon Creek images on top\n",
    "im_fns = im_fns[2:] + im_fns[0:2]\n",
    "\n",
    "# load glacier outlines from file\n",
    "study_sites_path = '/Users/raineyaberle/Google Drive/My Drive/Research/CryoGARS-Glaciology/Advising/student-research/Alexandra-Friel/snow_cover_mapping_application/study-sites/'\n",
    "AOI_fns = [study_sites_path + 'Emmons/AOIs/Emmons_RGI_outline.shp',\n",
    "           study_sites_path + 'LemonCreek/AOIs/LemonCreek_USGS_glacier_outline_2021.shp']\n",
    "AOIs = [gpd.read_file(AOI_fn) for AOI_fn in AOI_fns]\n",
    "\n",
    "# grab validation point names\n",
    "data_pts_fns = sorted(glob.glob('*.shp'))\n",
    "\n",
    "# set up figure\n",
    "fig, ax = plt.subplots(2, 2, figsize=(8,8))\n",
    "plt.rcParams.update({'font.size':12, 'font.sans-serif':'Arial'})\n",
    "ax = ax.flatten()\n",
    "text_labels = ['a)', 'b)', 'c)', 'd)']\n",
    "# plot dummy points for legend\n",
    "ax[0].plot(0,0, '.', markersize=8, color=colors_classified[0], label='snow')\n",
    "ax[0].plot(0,0, '.', markersize=8, color=colors_classified[3], label='no snow')\n",
    "\n",
    "# loop through image files\n",
    "for i, im_fn in enumerate(im_fns):\n",
    "    \n",
    "    print(im_fn)\n",
    "    \n",
    "    # open image and plot\n",
    "    im = rxr.open_rasterio(im_fn)\n",
    "    # grab CRS\n",
    "    crs = im.rio.crs.to_epsg()\n",
    "    im = im / 1e4\n",
    "    ax[i].imshow(np.dstack([im.data[3], im.data[2], im.data[1]]),\n",
    "                extent=(np.min(im.x.data), np.max(im.x.data), np.min(im.y.data), np.max(im.y.data)))\n",
    "    \n",
    "    # load data points and plot\n",
    "    site_name = im_fn.split('_')[0]\n",
    "    im_date = im_fn[-12:-4]\n",
    "    data_pts_snow_fn = [x for x in data_pts_fns if (site_name in x) and (im_date[0:6] in x) and ('_snow' in x)]\n",
    "    if len(data_pts_snow_fn) > 0:\n",
    "        data_pts_snow = gpd.read_file(data_pts_snow_fn[0])\n",
    "        data_pts_snow = data_pts_snow.to_crs(im.rio.crs)\n",
    "        data_pts_snow.plot(ax=ax[i], color=colors_classified[0], markersize=1)\n",
    "    data_pts_no_snow_fn = [x for x in data_pts_fns if (site_name in x) and (im_date[0:6] in x) and ('no-snow' in x)]\n",
    "    if len(data_pts_no_snow_fn) > 0:\n",
    "        data_pts_no_snow = gpd.read_file(data_pts_no_snow_fn[0])\n",
    "        data_pts_no_snow = data_pts_no_snow.to_crs(im.rio.crs)\n",
    "        data_pts_no_snow.plot(ax=ax[i], color=colors_classified[3], markersize=1)\n",
    "        \n",
    "    # select AOI, reproject, and plot\n",
    "    if 'Emmons' in im_fn:\n",
    "        AOI = AOIs[0]\n",
    "    elif 'LemonCreek' in im_fn:\n",
    "        AOI = AOIs[1]\n",
    "    AOI = AOI.to_crs('EPSG:'+str(crs))\n",
    "    if type(AOI.geometry[0])==MultiPolygon:\n",
    "        for j, geom in enumerate(AOI.geometry[0].geoms):\n",
    "            if j==0:\n",
    "                ax[i].plot(*geom.exterior.coords.xy, '-m', label='glacier outline')\n",
    "            else:\n",
    "                ax[i].plot(*geom.exterior.coords.xy, '-m', label='_nolegend')\n",
    "    else:\n",
    "        ax[i].plot(*AOI.geometry[0].exterior.coords.xy, '-m', label='glacier outline')\n",
    "        \n",
    "    # set axis limits and ticks\n",
    "    if i>=2:\n",
    "        ax[i].set_xlim(593e3, 603e3)\n",
    "        ax[i].set_ylim(5188e3, 5196e3)\n",
    "        ax[i].set_xticks(np.arange(594e3, 603e3, step=2e3))\n",
    "        ax[i].set_yticks(np.arange(5188e3, 5197e3, step=2e3))\n",
    "    else:\n",
    "        ax[i].set_xlim(535e3, 541e3)\n",
    "        ax[i].set_ylim(6468e3, 6475e3)\n",
    "        ax[i].set_xticks(np.arange(536e3, 541e3, step=2e3))\n",
    "        ax[i].set_yticks(np.arange(6468e3, 6475e3, step=2e3))\n",
    "    # change labels from m to km\n",
    "    ax[i].set_xticklabels([str(int(x/1e3)) for x in ax[i].get_xticks()])\n",
    "    ax[i].set_yticklabels([str(int(x/1e3)) for x in ax[i].get_yticks()])\n",
    "        \n",
    "    # add text labels\n",
    "    ax[i].text((ax[i].get_xlim()[1] - ax[i].get_xlim()[0])*0.05 + ax[i].get_xlim()[0],\n",
    "               (ax[i].get_ylim()[1] - ax[i].get_ylim()[0])*0.9 + ax[i].get_ylim()[0],\n",
    "               text_labels[i], backgroundcolor='w')\n",
    "    \n",
    "\n",
    "# add axis labels\n",
    "ax[0].set_ylabel('Northing [km]')\n",
    "ax[2].set_ylabel('Northing [km]')\n",
    "ax[2].set_xlabel('Easting [km]')\n",
    "ax[3].set_xlabel('Easting [km]')\n",
    "\n",
    "# add legendwin\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=(0.27, 0.92), ncols=3)\n",
    "\n",
    "# fig.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "# save figure\n",
    "if save_figures:\n",
    "    fig_fn = os.path.join(figures_out_path, 'classification_performance_assessment_images.png')\n",
    "    fig.savefig(ofig_fn, facecolor='w', dpi=300, bbox_inches='tight')\n",
    "    print('figure saved to file: ' + fig_fn)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0669670b-2377-405e-8fbc-e1bb84f26441",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Figure 5. Testing SCA sensitivity to presence of firn as a function of % firn of total SCA and surface slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbd550f-9cd3-4c3e-a76b-0fc06f91cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ranges in slopes, percents_firn, and calculate changes in SCA\n",
    "slopes = np.arange(0, 51, step=0.5)\n",
    "percents_firn = np.arange(0, 20.1, step=0.5)\n",
    "differences = np.zeros((len(percents_firn), len(slopes)))\n",
    "for i, percent_firn in enumerate(percents_firn):\n",
    "    for j, slope in enumerate(slopes):\n",
    "        differences[i,j] = percent_firn * np.cos(np.radians(slope))\n",
    "# flip array for plotting\n",
    "differences = np.array(differences)\n",
    "differences = np.fliplr(np.flipud(differences))\n",
    "\n",
    "# Plot\n",
    "fontsize=14\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "# distribution of slopes\n",
    "ax1 = fig.add_axes((0.1, 0.7, 0.72, 0.2))\n",
    "hist = ax1.hist(rgi['Slope'].values, bins=50, color='grey', range=(0,50))\n",
    "counts, bins = hist[0], hist[1]\n",
    "ax1.set_xlim(0,50)\n",
    "ax1.axis('off')\n",
    "# heatmap of uncertainties\n",
    "ax2 = fig.add_axes((0.1, 0.1, 0.9, 0.775))\n",
    "diff_im = ax2.imshow(differences, extent=(slopes[0], slopes[-1], percents_firn[0], percents_firn[-1]), \n",
    "                    cmap='inferno', clim=(0,20))\n",
    "ax2.set_ylabel('Misclassified firn [% of SCA]')\n",
    "ax2.set_xlabel('Slope [degrees]')\n",
    "ax2.set_xlim(0,50)\n",
    "ax2.grid()\n",
    "ax2.set_aspect(1.5)\n",
    "ax2.set_yticks(np.arange(0,21, step=5))\n",
    "fig.colorbar(diff_im, ax=ax2, shrink=0.5, label='$\\Delta$SCA [%]', ticks=np.arange(0,21, step=5))\n",
    "# add text labels\n",
    "ax1.text((ax1.get_xlim()[1] - ax1.get_xlim()[0])*0.05,\n",
    "         (ax1.get_ylim()[1] - ax1.get_ylim()[0])*0.1, 'a', fontweight='bold', fontsize=fontsize+2,\n",
    "         bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "ax2.text((ax2.get_xlim()[1] - ax2.get_xlim()[0])*0.05,\n",
    "         (ax2.get_ylim()[1] - ax2.get_ylim()[0])*0.1, 'b', fontweight='bold', fontsize=fontsize+2,\n",
    "         bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "# draw box around most common slope\n",
    "box_color = 'black'\n",
    "I = np.argwhere(counts==np.nanmax(counts))[0][0]\n",
    "rect1 = matplotlib.patches.Rectangle((bins[I], 0), width=1, height=np.nanmax(counts), edgecolor=box_color, facecolor='None')\n",
    "ax1.add_patch(rect1)\n",
    "rect2 = matplotlib.patches.Rectangle((bins[I], 0), width=1, height=np.nanmax(percents_firn), edgecolor=box_color, facecolor='None')\n",
    "ax2.add_patch(rect2)\n",
    "# add descriptor text\n",
    "ax1.text(30, np.nanmax(counts)-50, \n",
    "         'Most frequent slopes = (' + str(int(bins[I])) + '\\N{degree sign}, ' + str(int(bins[I+1])) + '\\N{degree sign})',\n",
    "         color=box_color)\n",
    "ax1.text(41.1, np.nanmax(counts)-250,\n",
    "         '$\\Delta$SCA = (' \n",
    "         + str(int(differences[-1, np.argwhere(slopes==bins[I])[0][0]])) + '%, ' \n",
    "         + str(int(differences[0, np.argwhere(slopes==bins[I])[0][0]])) + '%)',\n",
    "         color=box_color)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "if save_figures:\n",
    "    fig_fn = os.path.join(figures_out_path, 'sensitivity_test_misclassified_firn.png')\n",
    "    fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "    print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d78494-d9a0-4fc9-b39c-f07441646475",
   "metadata": {},
   "source": [
    "## Figures 6-7. Timseries of SCA, weekly median trends for SCA, AAR, and median snowline elevations for the USGS Benchmark Glaciers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8578ec6e-b1c6-449c-bade-37cfb9d94685",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Load and compile snowlines from Sentinel-2\n",
    "sl_ests_full = pd.DataFrame()\n",
    "for site_name in site_names:\n",
    "\n",
    "    print(site_name)\n",
    "    \n",
    "    # load estimated snowlines  \n",
    "    sl_est_fns = glob.glob(study_sites_path + site_name + '/imagery/snowlines/*snowline.csv')\n",
    "    sl_ests = pd.DataFrame()\n",
    "    for sl_est_fn in sl_est_fns:\n",
    "        sl_est = pd.read_csv(sl_est_fn)\n",
    "        sl_ests = pd.concat([sl_ests, sl_est])\n",
    "    sl_ests.reset_index(drop=True, inplace=True)\n",
    "    sl_ests['datetime'] = pd.to_datetime(sl_ests['datetime'], format='mixed')\n",
    "\n",
    "    # concatenate to full df\n",
    "    sl_ests_full = pd.concat([sl_ests_full, sl_ests])\n",
    "\n",
    "# reset index, add week column\n",
    "sl_ests_full.reset_index(drop=True, inplace=True)\n",
    "sl_ests_full['Week'] = sl_ests_full['datetime'].dt.isocalendar().week\n",
    "sl_ests_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c15fcf-cc5d-4bcb-b942-39d0c42b4a66",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Settings and display parameters\n",
    "site_names = ['Gulkana', 'Wolverine', 'LemonCreek', 'Sperry', 'SouthCascade']\n",
    "site_names_display = [x.replace('C', ' C') for x in site_names]\n",
    "text_labels1 = ['a)', 'b)', 'c)', 'd)', 'e)']\n",
    "text_labels2 = [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i'], ['j', 'k', 'l'], ['m', 'n', 'o']]\n",
    "\n",
    "# -----Set up figures\n",
    "fontsize=12\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "# time series: SCA\n",
    "fig1, ax1 = plt.subplots(5, 1, figsize=(10, 12))\n",
    "# weekly median trends: SCA, AAR, median snowline elevation\n",
    "fig2, ax2 = plt.subplots(5, 3, figsize=(10, 12))\n",
    "# fmt_year = matplotlib.dates.YearLocator() # minor ticks every year\n",
    "fmt_year = matplotlib.dates.DateFormatter(\"%Y\")\n",
    "alpha = 0.9\n",
    "color_sca, color_aar, color_snowline = colors_classified[0], '#FFC107', '#FB65FB'\n",
    "\n",
    "# -----Loop through sites\n",
    "for site_name, site_name_display, text_label, i in list(zip(site_names, site_names_display, text_labels, np.arange(0,len(site_names)))):\n",
    "    \n",
    "    print(site_name)\n",
    "    \n",
    "    # subset snowlines\n",
    "    sl_ests = sl_ests_full.loc[sl_ests_full['site_name']==site_name].reset_index(drop=True)\n",
    "        \n",
    "    # -----Define axis limits\n",
    "    xmin, xmax = np.datetime64('2013-05-01T00:00:00'), np.datetime64('2022-12-01T00:00:00')\n",
    "    sl_elev_median_min = np.nanmin(sl_ests['snowline_elevs_median_m'])\n",
    "    sl_elev_median_max = np.nanmax(sl_ests['snowline_elevs_median_m'])\n",
    "    yrange1 = [np.nanmax(sl_ests['SCA_m2']) * 1e-6 * -0.1, np.nanmax(sl_ests['SCA_m2']) * 1e-6 * 1.1]\n",
    "    yrange2_sca = yrange1\n",
    "    yrange2_aar = [-0.1, 1.1]\n",
    "    min_snowline_elev, max_snowline_elev = np.nanmin(sl_ests['snowline_elevs_median_m']), np.nanmax(sl_ests['snowline_elevs_median_m'])\n",
    "    yrange2_snowline = [min_snowline_elev * 1e-3 * 0.95, max_snowline_elev * 1e-3 * 1.05]\n",
    "\n",
    "    # -----Plot light grey boxes where no observations exist on SCA plots\n",
    "    years = np.arange(2013, 2022, step=1)\n",
    "    for year in years:\n",
    "        min_date, max_date = np.datetime64(str(year) + '-11-01'), np.datetime64(str(year+1) + '-05-01')\n",
    "        rect = matplotlib.patches.Rectangle((min_date, yrange1[0]), width=max_date-min_date, height=yrange1[1]-yrange1[0], color='#d9d9d9')\n",
    "        ax1[i].add_patch(rect)\n",
    "        \n",
    "    # -----Plot SCA time series\n",
    "    # PlanetScope\n",
    "    ax1[i].plot(sl_ests['datetime'].loc[sl_ests['dataset']=='PlanetScope'], \n",
    "                np.divide(sl_ests['SCA_m2'].loc[sl_ests['dataset']=='PlanetScope'].values, 1e6), \n",
    "                '.', markeredgecolor='w', markerfacecolor=color_PlanetScope, \n",
    "                alpha=alpha, markersize=10, markeredgewidth=1, label='PlanetScope')\n",
    "    # Sentinel-2 SR\n",
    "    ax1[i].plot(sl_ests['datetime'].loc[sl_ests['dataset']=='Sentinel-2_SR'], \n",
    "                np.divide(sl_ests['SCA_m2'].loc[sl_ests['dataset']=='Sentinel-2_SR'].values, 1e6), \n",
    "                'D', markeredgecolor='w', markerfacecolor=color_Sentinel2, \n",
    "                alpha=alpha, markersize=4, markeredgewidth=1, label='Sentinel-2 SR')\n",
    "    # Sentinel-2 TOA\n",
    "    ax1[i].plot(sl_ests['datetime'].loc[sl_ests['dataset']=='Sentinel-2_TOA'], \n",
    "                np.divide(sl_ests['SCA_m2'].loc[sl_ests['dataset']=='Sentinel-2_TOA'].values, 1e6), \n",
    "                'D', markeredgecolor=color_Sentinel2, markerfacecolor='None', \n",
    "                alpha=alpha, markersize=3, markeredgewidth=1.2, label='Sentinel-2 TOA')  \n",
    "    # Landsat\n",
    "    ax1[i].plot(sl_ests['datetime'].loc[sl_ests['dataset']=='Landsat'], \n",
    "                np.divide(sl_ests['SCA_m2'].loc[sl_ests['dataset']=='Landsat'].values, 1e6), \n",
    "                '^', markeredgecolor='w', markerfacecolor=color_Landsat, \n",
    "                alpha=alpha, markersize=6, markeredgewidth=1, label='Landsat')   \n",
    "\n",
    "    # -----Plot glacier area on SCA plots\n",
    "    AOI_fn = glob.glob(os.path.join(study_sites_path, site_name, 'AOIs', site_name + '*USGS*.shp'))[0]\n",
    "    AOI = gpd.read_file(AOI_fn)\n",
    "    ax1[i].plot([xmin, xmax], [AOI.geometry[0].area / 1e6, AOI.geometry[0].area / 1e6], '--', color='grey')\n",
    "    ax2[i,0].plot([15, 45], [AOI.geometry[0].area / 1e6, AOI.geometry[0].area / 1e6], '--', color='grey')\n",
    "\n",
    "    # -----Plot min and max elevations on snowline plots\n",
    "    ax2[i,2].plot([15, 45], [min_snowline_elev * 1e-3, min_snowline_elev * 1e-3], '--', color='grey')\n",
    "    ax2[i,2].plot([15, 45], [max_snowline_elev * 1e-3, max_snowline_elev * 1e-3], '--', color='grey')\n",
    "    \n",
    "    # -----Adjust axes display settings\n",
    "    ax1[i].set_xlim(xmin, xmax)\n",
    "    ax1[i].set_ylim(yrange1[0], yrange1[1])\n",
    "    ax1[i].xaxis.set_major_formatter(fmt_year)\n",
    "    ax1[i].grid(True)\n",
    "    ax1[i].text(np.datetime64('2013-06-01'),\n",
    "                (ax1[i].get_ylim()[1] - ax1[i].get_ylim()[0]) * 0.06 + ax1[i].get_ylim()[0],\n",
    "                text_label + ' ' + site_name_display + ' (N=' + str(len(sl_ests)) + ')',\n",
    "                bbox=dict(facecolor='white', edgecolor='black', pad=5))\n",
    "    if site_name=='SouthCascade':\n",
    "        ax1[i].set_yticks(np.arange(0, 2.4, step=0.4))\n",
    "    if i==2:\n",
    "        ax1[i].set_ylabel('Snow-covered area [km$^2$]', fontsize=fontsize+2)\n",
    "    ax2[i,0].set_ylabel(site_name_display)\n",
    "        \n",
    "    # -----Calculate median and interquartile ranges for weekly trends\n",
    "    q1, q3 = 0.25, 0.75 # define quartiles\n",
    "    # calculate weekly trends using only Sentinel-2 snowlines\n",
    "    sl_ests_noPS = sl_ests.loc[sl_ests['dataset']!='PlanetScope']   \n",
    "    for ax, column, color, yrange, ylabel in list(zip([ax2[i,0], ax2[i,1], ax2[i,2]], \n",
    "                                                      ['SCA_m2', 'AAR', 'snowline_elevs_median_m'],\n",
    "                                                      [color_sca, color_aar, color_snowline],\n",
    "                                                      [yrange2_sca, yrange2_aar, yrange2_snowline],\n",
    "                                                      ['SCA [km$^2$]', 'AAR', 'Median snowline elevation [km]'])):\n",
    "        weekly = sl_ests_noPS.groupby(by='Week')[column].agg(['median', lambda x: x.quantile(q1), lambda x: x.quantile(q3)])\n",
    "        weekly.columns = ['Median', 'Q1', 'Q3'] # Rename the columns for clarity\n",
    "        weekly.index = weekly.index.astype(float)\n",
    "        # plot\n",
    "        if column=='SCA_m2':\n",
    "            weekly = weekly / 1e6  # convert from m^2 to km^2\n",
    "        elif column=='snowline_elevs_median_m':\n",
    "            weekly = weekly / 1e3  # convert from m to km\n",
    "        ax.fill_between(weekly.index, weekly['Q1'], weekly['Q3'].values, color=color, alpha=0.5)\n",
    "        ax.plot(weekly.index, weekly['Median'], color=color, linewidth=2)\n",
    "        ax.grid(True)\n",
    "        # adjust x axis\n",
    "        ax.set_xlim(15, 45)\n",
    "        ax.set_xticks([18, 31, 44])\n",
    "        ax.set_xticklabels([])\n",
    "        if i==len(site_names)-1:\n",
    "            ax.set_xticklabels(['May', 'Aug', 'Nov'])\n",
    "        # adjust y axis\n",
    "        ax.set_ylim(yrange[0], yrange[1])\n",
    "        # if column=='SCA_m2':\n",
    "        #     ax.set_xticks(\n",
    "        if column=='AAR':\n",
    "            ax.set_yticks([0, 0.25, 0.5, 0.75, 1])\n",
    "            ax.set_yticklabels(['0.0', '', '0.5', '', '1.0'])\n",
    "        if i==0:\n",
    "            ax.set_title(ylabel)\n",
    "\n",
    "# -----Add text labels to figure 2\n",
    "for i in np.arange(0,len(site_names)):\n",
    "    for j in np.arange(0, 3):\n",
    "        ax2[i,j].text((ax2[i,j].get_xlim()[1] - ax2[i,j].get_xlim()[0]) * 0.05 + ax2[i,j].get_xlim()[0],\n",
    "                      (ax2[i,j].get_ylim()[1] - ax2[i,j].get_ylim()[0]) * 0.15 + ax2[i,j].get_ylim()[0],\n",
    "                      text_labels2[i][j], fontweight='bold', fontsize=fontsize+2,\n",
    "                      bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "\n",
    "# -----Add legend to figure 1\n",
    "\n",
    "ax1[0].legend(loc='center', bbox_to_anchor=(0.5, 1.1), handletextpad=0.1, labelspacing=0.5, markerscale=2, ncol=4)\n",
    "\n",
    "fig1.tight_layout()\n",
    "fig2.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----Save figures\n",
    "if save_figures:\n",
    "    fig1_fn = os.path.join(figures_out_path, 'timeseries_SCA.png')\n",
    "    fig1.savefig(fig1_fn, dpi=300, facecolor='w', edgecolor='none', bbox_inches='tight')\n",
    "    print('figure 1 saved to file: ' + fig1_fn)\n",
    "    fig2_fn = os.path.join(figures_out_path, 'weekly_median_trends_no_PlanetScope.png')\n",
    "    fig2.savefig(fig2_fn, dpi=300, facecolor='w', edgecolor='none', bbox_inches='tight')\n",
    "    print('figure 2 saved to file: ' + fig2_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3055b332-fbcd-442b-b0e1-4b0d8e420517",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot only USGS ELAs over time\n",
    "usgs_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/GIS_data/USGS/benchmarkGlacier_massBalance/'\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "col = plt.cm.viridis\n",
    "for i, site_name in enumerate(site_names):\n",
    "    usgs_fn = usgs_path + site_name+'/Output_'+site_name+'_Glacier_Wide_solutions_calibrated.csv'\n",
    "    usgs_file = pd.read_csv(usgs_fn)\n",
    "    ELA = usgs_file['ELA']\n",
    "    ELA_date = usgs_file['Ba_Date'].astype('datetime64[ns]')\n",
    "    plt.plot(ELA_date, ELA, '.-', color=col((i+1)/len(site_names)), label=site_name)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a62121d-8836-402f-8cce-576543d218c9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print stats for SCA\n",
    "\n",
    "# -----Loop through sites\n",
    "for site_name in site_names:\n",
    "    \n",
    "    print(site_name)\n",
    "    \n",
    "    # load estimated snow lines  \n",
    "    sl_est_fns = glob.glob(study_sites_path + site_name + '/imagery/snowlines/*snowline.csv')\n",
    "    sl_ests = gpd.GeoDataFrame()\n",
    "    for sl_est_fn in sl_est_fns:\n",
    "        sl_est = pd.read_csv(sl_est_fn)\n",
    "        sl_ests = pd.concat([sl_ests, sl_est])\n",
    "    sl_ests.reset_index(drop=True, inplace=True)\n",
    "    sl_ests['datetime'] = pd.to_datetime(sl_ests['datetime'], format='mixed')\n",
    "    \n",
    "    # identify min and max SCAs\n",
    "    imin = np.argwhere(sl_ests['SCA_m2'].values==np.nanmin(sl_ests['SCA_m2'].values))[0][0]\n",
    "    SCA_min, SCA_min_date = sl_ests.iloc[imin]['SCA_m2'], sl_ests.iloc[imin]['datetime']\n",
    "    print('Minimum SCA: ' + str(SCA_min) + ' m^2 on ' + str(SCA_min_date))\n",
    "    imax = np.argwhere(sl_ests['SCA_m2'].values==np.nanmax(sl_ests['SCA_m2'].values))[0][0]\n",
    "    SCA_max, SCA_max_date = sl_ests.iloc[imax]['SCA_m2'], sl_ests.iloc[imax]['datetime']\n",
    "    print('Maximum SCA: ' + str(SCA_max) + ' m^2 on ' + str(SCA_max_date)       )  \n",
    "    print(' ')\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13352855-a103-42c0-8e81-0a96c1f049b4",
   "metadata": {},
   "source": [
    "## Figure 9. SCA, snowlines, and AAR time series variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3654c3-369d-47d2-9271-221f1bcab524",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Settings and display parameters\n",
    "site_names = ['Wolverine', 'Gulkana', 'LemonCreek', 'SouthCascade', 'Sperry']\n",
    "\n",
    "fmt_month = matplotlib.dates.MonthLocator(bymonth=(5, 11)) # minor ticks every month\n",
    "fmt_year = matplotlib.dates.YearLocator() # minor ticks every year\n",
    "\n",
    "# -----Iterate over site names\n",
    "stats_df = pd.DataFrame()\n",
    "values_df = pd.DataFrame()\n",
    "for i, site_name in enumerate(site_names):\n",
    "    \n",
    "    print(site_name)\n",
    "\n",
    "    # Load estimated snow lines  \n",
    "    sl_est_fns = glob.glob(study_sites_path + site_name + '/imagery/snowlines/*snowline.csv')\n",
    "    sl_ests = gpd.GeoDataFrame()\n",
    "    for sl_est_fn in sl_est_fns:\n",
    "        sl_est = pd.read_csv(sl_est_fn)\n",
    "        sl_ests = pd.concat([sl_ests, sl_est])\n",
    "    sl_ests.reset_index(drop=True, inplace=True)\n",
    "    sl_ests['datetime'] = pd.to_datetime(sl_ests['datetime'], format='mixed')\n",
    "        \n",
    "    # Define axis limits\n",
    "    # xmin, xmax = np.datetime64('2016-05-01T00:00:00'), np.datetime64('2022-12-01T00:00:00')\n",
    "    # sl_elev_median_min = np.nanmin(sl_ests['snowline_elevs_median_m'])\n",
    "    # sl_elev_median_max = np.nanmax(sl_ests['snowline_elevs_median_m'])\n",
    "    # ymin1, ymax1 = np.nanmin(sl_ests['SCA_m2']) * 1e-6 * -0.1, np.nanmax(sl_ests['SCA_m2']) * 1e-6 * 1.3\n",
    "    # ymin2 = sl_elev_median_min - 0.1*(sl_elev_median_max - sl_elev_median_min)\n",
    "    # ymax2 = sl_elev_median_max + 0.1*(sl_elev_median_max - sl_elev_median_min)\n",
    "    # ymin3, ymax3 = -1, 125\n",
    "    # yrange1, yrange2, yrange3 = [ymin1, ymax1], [ymin2, ymax2], [ymin3, ymax3]\n",
    "\n",
    "    # Calculate monthly mean and std for Sentinel-2 time series\n",
    "    def custom_rolling_stats(data, data_var, window_size, months_to_include):\n",
    "        filtered_data = data[data['datetime'].dt.month.isin(months_to_include)]\n",
    "        rolling_std = filtered_data[data_var].rolling(window=window_size).apply(lambda x: np.std(x))\n",
    "        return rolling_std\n",
    "    \n",
    "    sl_ests.index = sl_ests.datetime\n",
    "    sl_ests.sort_index(inplace=True) # sort chronologically\n",
    "    \n",
    "    # define settings for rolling stats\n",
    "    time_range = pd.Timedelta(days=7) # window for rolling stats\n",
    "    months_to_include = [5,6,7,8,9,10] # which months to include\n",
    "    SCA_std = custom_rolling_stats(sl_ests, 'SCA_m2', int(time_range.days), months_to_include)\n",
    "    sl_std = custom_rolling_stats(sl_ests, 'snowline_elevs_median_m', int(time_range.days), months_to_include)\n",
    "    AAR_std = custom_rolling_stats(sl_ests, 'AAR', int(time_range.days), months_to_include)\n",
    "    \n",
    "    # append to dataframe\n",
    "    df = pd.DataFrame({'site_name': site_name,\n",
    "                       'SCA_std': SCA_std.values,\n",
    "                       'SCA_std_normalized': np.divide(SCA_std, np.nanmax(sl_ests['SCA_m2'])).values,\n",
    "                       'sl_std': sl_std.values,\n",
    "                       'sl_std_normalized': np.divide(sl_std, (np.nanmax(sl_ests['snowline_elevs_median_m']) - np.nanmin(sl_ests['snowline_elevs_median_m']))).values,\n",
    "                       'AAR_std': AAR_std.values,\n",
    "                       'AAR_std_normalized': np.divide(AAR_std, np.nanmax(sl_ests['AAR'])).values,\n",
    "                      })\n",
    "    stats_df = pd.concat([stats_df, df])\n",
    "    \n",
    "# -----Adjust dataframe\n",
    "# reset index, drop NaN values\n",
    "stats_df.reset_index(drop=True).dropna(inplace=True)\n",
    "# add display name to dataframe for plotting\n",
    "stats_df['display_name'] = [x.replace('C', ' C') for x in stats_df['site_name'].values]\n",
    "# adjust units for SCA (km^2) and AAR (%)\n",
    "stats_df[['SCA_std']] = np.divide(stats_df[['SCA_std']], 1e6)\n",
    "stats_df[['AAR_std']] = np.multiply(stats_df[['AAR_std']], 100)\n",
    "\n",
    "# -----Plot\n",
    "plt.rcParams.update({'font.size':14, 'font.sans-serif': 'Arial'})\n",
    "fig, ax = plt.subplots(3, 2, figsize=(16, 16))\n",
    "ax = ax.flatten()  \n",
    "# define axes settings\n",
    "text_labels = ['a)', 'b)', 'c)', 'd)', 'e)', 'f)']\n",
    "ylabels = ['SCA [km$^2$]', 'SCA [unitless]',\n",
    "           'AAR [%]', 'AAR [unitless]',\n",
    "           'Median snowline altitude [m]', 'Median snowline altitude [unitless]'\n",
    "           ]\n",
    "data_vars = ['SCA_std', 'SCA_std_normalized', 'AAR_std', 'AAR_std_normalized', 'sl_std', 'sl_std_normalized']\n",
    "colors = [colors_classified[0], colors_classified[0], \n",
    "          '#FFC107', '#FFC107',\n",
    "          '#FB65FB', '#FB65FB']\n",
    "ax[0].set_title('Weekly standard deviation')\n",
    "ax[1].set_title('Normalized weekly standard deviation')\n",
    "ax[1].set_ylim(-0.01, 0.5)\n",
    "ax[2].set_ylim(-1, 50)\n",
    "ax[3].set_ylim(-0.01, 0.5)\n",
    "ax[5].set_ylim(-0.01, 0.5)\n",
    "# iterate over axes\n",
    "for axis, data_var, color, text_label, ylabel in list(zip(ax, data_vars, colors, text_labels, ylabels)):\n",
    "    sns.boxplot(data=stats_df, x='display_name', y=data_var, ax=axis, color=color) \n",
    "    axis.set(xlabel=None)\n",
    "    axis.set_ylabel(ylabel)\n",
    "    axis.text((axis.get_xlim()[1] - axis.get_xlim()[0])*0.935 + axis.get_xlim()[0], \n",
    "               (axis.get_ylim()[1] - axis.get_ylim()[0])*0.903 + axis.get_ylim()[0], \n",
    "               text_label, bbox=dict(facecolor='white', edgecolor='black', pad=5))\n",
    "    axis.set_xticks(axis.get_xticks(), axis.get_xticklabels(), rotation=20)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# -----Save figures\n",
    "if save_figures:\n",
    "    fig_fn = os.path.join(figures_out_path, 'weekly_metric_ranges.png')\n",
    "    fig.savefig(fig_fn, dpi=300, facecolor='w', edgecolor='none', bbox_inches='tight')\n",
    "    print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2549bdb5-a00c-4810-94c8-e1909c14532b",
   "metadata": {},
   "source": [
    "## Figure 10. South Cascade snowline cover elevations distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f9c438-716d-4b3d-b711-0f3dd11bd879",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ee.Initialize()\n",
    "date_start, date_end = '2022-10-02', '2022-10-03'\n",
    "dataset = 'Sentinel-2_SR'\n",
    "site_name = 'SouthCascade'\n",
    "\n",
    "# load AOI\n",
    "AOI_fn = os.path.join(study_sites_path, site_name, 'AOIs', 'SouthCascade_USGS_glacier_outline_2021.shp')\n",
    "AOI = gpd.read_file(AOI_fn)\n",
    "\n",
    "# query GEE for image\n",
    "im_xr = f.query_gee_for_imagery(dataset_dict, dataset, AOI, date_start, date_end, \n",
    "                                month_start=1, month_end=12, cloud_cover_max=70, mask_clouds=True)[0]\n",
    "im_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d4f6b4-e98e-4f82-84e6-bc7859a76f5c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# load EGM96 geoid heights\n",
    "egm96_fn = os.path.join(base_path, 'inputs-outputs', 'us_nga_egm96_15.tif')\n",
    "egm96 = xr.open_dataset(egm96_fn)\n",
    "egm96 = egm96.rename({'band_data': 'geoid_height'})\n",
    "\n",
    "# load classified image\n",
    "im_classified_fn = os.path.join(study_sites_path, site_name, 'imagery', 'classified',\n",
    "                                '20221002T132103_SouthCascade_Sentinel-2_SR_classified.nc')\n",
    "im_classified = xr.open_dataset(im_classified_fn)\n",
    "im_classified = im_classified.rio.reproject('EPSG:'+str(AOI.crs.to_epsg()))\n",
    "\n",
    "# load snowline\n",
    "snowline_fn = os.path.join(study_sites_path, site_name, 'imagery', 'snowlines',\n",
    "                           '20221002T132103_SouthCascade_Sentinel-2_SR_snowline.csv')\n",
    "snowline = pd.read_csv(snowline_fn)\n",
    "snowline['geometry'] = snowline['geometry'].apply(wkt.loads)\n",
    "snowline_gdf = gpd.GeoDataFrame(snowline, geometry=snowline['geometry'], crs='EPSG:4326')\n",
    "snowline_gdf['snowline_elevs_m'] = [pd.eval(snowline_gdf['snowline_elevs_m'][0].replace('\\n','').replace(' ',', '))]\n",
    "# reference elevations to the ellipsoid instead for direct comparison\n",
    "geoid_heights = [egm96.sel(x=x, y=y, method='nearest').geoid_height.data[0] for x,y in \n",
    "                 list(zip(snowline_gdf.geometry[0].coords.xy[0], snowline_gdf.geometry[0].coords.xy[1]))]\n",
    "snowline_elevs_m_ellipsoid = [(x+y) for x,y in list(zip(snowline_gdf['snowline_elevs_m'][0], geoid_heights))]\n",
    "\n",
    "# load DEM\n",
    "DEM_fn = os.path.join(study_sites_path, site_name, 'DEMs',\n",
    "                      'SouthCascade_20210813_USGS_DEM_filled.tif')\n",
    "DEM = xr.open_dataset(DEM_fn)\n",
    "DEM = DEM.rio.reproject('EPSG:'+str(AOI.crs.to_epsg()))\n",
    "DEM = xr.where(DEM >= 1e38, np.nan, DEM)\n",
    "DEM = DEM.rio.write_crs('EPSG:'+str(AOI.crs.to_epsg()))\n",
    "\n",
    "# clip DEM to AOI and interpolate to classified image coordinates\n",
    "dem_aoi = DEM.rio.clip(AOI.geometry, AOI.crs)\n",
    "dem_aoi_interp = dem_aoi.interp(x=im_classified.x.data, y=im_classified.y.data, method='linear')\n",
    "# add elevation as a band to classified image for convenience\n",
    "im_classified['elevation'] = (('time', 'y', 'x'), dem_aoi_interp.band_data.data)\n",
    "\n",
    "# determine snow covered elevations\n",
    "all_elev = np.ravel(dem_aoi_interp.band_data.data)\n",
    "all_elev = all_elev[~np.isnan(all_elev)]  # remove NaNs\n",
    "snow_est_elev = np.ravel(im_classified.where((im_classified.classified <= 2))\n",
    "                         .where(im_classified.classified != -9999).elevation.data)\n",
    "snow_est_elev = snow_est_elev[~np.isnan(snow_est_elev)]  # remove NaNs\n",
    "\n",
    "# -----Create elevation histograms\n",
    "# determine bins to use in histograms\n",
    "elev_min = np.fix(np.nanmin(np.ravel(im_classified.elevation.data)) / 10) * 10\n",
    "elev_max = np.round(np.nanmax(np.ravel(im_classified.elevation.data)) / 10) * 10\n",
    "bin_edges = np.linspace(elev_min, elev_max, num=int((elev_max - elev_min) / 10 + 1))\n",
    "bin_centers = (bin_edges[1:] + bin_edges[0:-1]) / 2\n",
    "# calculate elevation histograms\n",
    "hist_elev = np.histogram(all_elev, bins=bin_edges)[0]\n",
    "hist_snow_est_elev = np.histogram(snow_est_elev, bins=bin_edges)[0]\n",
    "hist_snow_est_elev_norm = hist_snow_est_elev / hist_elev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b37fb-30ce-4184-a8f8-07d5d29fad95",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# PLOT\n",
    "plt.rcParams.update({'font.size':18, 'font.sans-serif':'Arial'})\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16,8))\n",
    "# RGB image\n",
    "xmin, xmax = np.min(im_xr.x.data)/1e3, np.max(im_xr.x.data)/1e3\n",
    "ymin, ymax = np.min(im_xr.y.data)/1e3, np.max(im_xr.y.data)/1e3\n",
    "ax[0].imshow(np.dstack([im_xr['B4'].data[0], im_xr['B3'].data[0], im_xr['B2'].data[0]]),\n",
    "             extent=(xmin, xmax, ymin, ymax))\n",
    "snowline_gdf = snowline_gdf.to_crs(snowline_gdf['HorizontalCRS'][0])\n",
    "ax[0].plot(np.divide(snowline_gdf.geometry[0].coords.xy[0], 1e3), \n",
    "           np.divide(snowline_gdf.geometry[0].coords.xy[1], 1e3), '.m', markersize=2)\n",
    "x_mesh, y_mesh = np.meshgrid(im_classified.x.data, im_classified.y.data)\n",
    "ax[0].contour(np.divide(x_mesh, 1e3), np.divide(y_mesh, 1e3), im_classified.elevation.data[0], \n",
    "              levels=[np.nanmedian(snowline_elevs_m_ellipsoid)], colors='#ff7f00', linewidths=3)\n",
    "# dummy lines for legend\n",
    "ax[0].plot([0,1], [0,1], '-m', linewidth=2, label='Snowline')\n",
    "ax[0].plot([0,1], [0,1], '-', color='#ff7f00', linewidth=2, label='Median snowline altitude')\n",
    "ax[0].set_xlim(xmin, xmax)\n",
    "ax[0].set_ylim(ymin, ymax)\n",
    "ax[0].legend(loc='lower center', bbox_to_anchor=[0.4, 1.02, 0.2, 0.2], framealpha=1)\n",
    "ax[0].set_xlabel('Easting [km]')\n",
    "ax[0].set_ylabel('Northing [km]')\n",
    "ax[0].set_yticks([5356.5, 5357, 5357.5, 5358, 5358.5])\n",
    "ax[0].text((xmax-xmin)*0.08 + xmin, (ymax - ymin)*0.08 + ymin, \n",
    "           'a)', bbox=dict(facecolor='white', edgecolor='black', pad=5))\n",
    "# histograms\n",
    "sl_elev_min = np.nanmin(snowline_elevs_m_ellipsoid)\n",
    "sl_elev_max = np.nanmax(snowline_elevs_m_ellipsoid)\n",
    "rect = Rectangle((sl_elev_min, 0), width=sl_elev_max-sl_elev_min, height=950, facecolor='m', alpha=0.2)\n",
    "ax[1].add_patch(rect)\n",
    "ax[1].bar(bin_centers, hist_elev, width=(bin_centers[1] - bin_centers[0]), color='#238443', \n",
    "          align='center', label='All elevations')\n",
    "ax[1].bar(bin_centers, hist_snow_est_elev, width=(bin_centers[1] - bin_centers[0]), \n",
    "          color=colors_classified[0], align='center', label='Snow-covered elevations')\n",
    "ax[1].plot([np.nanmedian(snowline_elevs_m_ellipsoid), np.nanmedian(snowline_elevs_m_ellipsoid)], [0, 1000], '-',\n",
    "           color='#ff7f00', linewidth=3, label='Median snowline altitude')\n",
    "ax[1].grid()\n",
    "ax[1].set_xlabel('Elevation [m]')\n",
    "ax[1].set_ylabel('Count')\n",
    "ax[1].set_ylim(0, 950)\n",
    "xmin, xmax = ax[1].get_xlim()\n",
    "ymin, ymax = ax[1].get_ylim()\n",
    "ax[1].text((xmax-xmin)*0.05 + xmin, (ymax - ymin)*0.08 + ymin, \n",
    "           'b)', bbox=dict(facecolor='white', edgecolor='black', pad=5))\n",
    "ax[1].text(1812, 900, 'Snowline elevation range', color='m', fontweight='bold', \n",
    "           bbox=dict(facecolor='w', edgecolor='None', pad=3))\n",
    "ax[1].arrow(1825, 910, -50, 0, color='m', width=5, length_includes_head=True, head_length=7, head_width=25)\n",
    "ax[1].arrow(2035, 910, 50, 0, color='m', width=5, length_includes_head=True, head_length=7, head_width=25)\n",
    "\n",
    "ax[1].legend(loc='lower center', bbox_to_anchor=[0.4, 1.02, 0.2, 0.2], framealpha=1)\n",
    "ax[1].set_position([0.55, 0.17, 0.5, 0.65])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'SouthCascade_snow_covered_elevations.png')\n",
    "fig.savefig(fig_fn, dpi=300, facecolor='w', edgecolor='none', bbox_inches='tight')\n",
    "print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06ea65e-4d61-45a1-82ba-e97bea948dd2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# load EGM96 geoid heights\n",
    "egm96_fn = os.path.join(base_path, 'inputs-outputs', 'us_nga_egm96_15.tif')\n",
    "egm96 = xr.open_dataset(egm96_fn)\n",
    "egm96 = egm96.rename({'band_data': 'geoid_height'})\n",
    "\n",
    "# load classified image\n",
    "im_classified_fn = os.path.join(study_sites_path, site_name, 'imagery', 'classified',\n",
    "                                '20221002T132103_SouthCascade_Sentinel-2_SR_classified.nc')\n",
    "im_classified = xr.open_dataset(im_classified_fn)\n",
    "im_classified = im_classified.rio.reproject('EPSG:'+str(AOI.crs.to_epsg()))\n",
    "\n",
    "# load snowline\n",
    "snowline_fn = os.path.join(study_sites_path, site_name, 'imagery', 'snowlines',\n",
    "                           '20221002T132103_SouthCascade_Sentinel-2_SR_snowline.csv')\n",
    "snowline = pd.read_csv(snowline_fn)\n",
    "snowline['geometry'] = snowline['geometry'].apply(wkt.loads)\n",
    "snowline_gdf = gpd.GeoDataFrame(snowline, geometry=snowline['geometry'], crs='EPSG:4326')\n",
    "snowline_gdf['snowline_elevs_m'] = [pd.eval(snowline_gdf['snowline_elevs_m'][0].replace('\\n','').replace(' ',', '))]\n",
    "# reference elevations to the ellipsoid instead for direct comparison\n",
    "geoid_heights = [egm96.sel(x=x, y=y, method='nearest').geoid_height.data[0] for x,y in \n",
    "                 list(zip(snowline_gdf.geometry[0].coords.xy[0], snowline_gdf.geometry[0].coords.xy[1]))]\n",
    "snowline_elevs_m_ellipsoid = [(x+y) for x,y in list(zip(snowline_gdf['snowline_elevs_m'][0], geoid_heights))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5125424-b058-4fc9-b22a-99c84ef7711b",
   "metadata": {},
   "source": [
    "## Figure S1. PlanetScope image adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856b2b11-76b0-4dce-97ff-c0541c91846e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ce8d43f-fe2e-4b81-8c00-2918286386de",
   "metadata": {},
   "source": [
    "## Figure S2. Example shortcomings and successes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26980931-eba9-4714-8b5e-3c4e7df04d0c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Initialize GEE\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()\n",
    "\n",
    "# -----Difficult conditions (unsuccessful)\n",
    "im_difficult_dates = ['20190808T152744', '20180809T180000']\n",
    "im_difficult_site_names = ['Gulkana', 'Sperry']\n",
    "im_difficult_datasets = ['Sentinel-2_SR', 'PlanetScope']\n",
    "text_labels = [['a)', 'b)'], ['e)', 'f)']]\n",
    "i=0 # loop counter\n",
    "for im_date, site_name, dataset, text_label in list(zip(im_difficult_dates, im_difficult_site_names, im_difficult_datasets, text_labels)):\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16,8))\n",
    "    \n",
    "    # classified image\n",
    "    im_classified_path = study_sites_path + site_name + '/imagery/classified/'\n",
    "    im_classified_fn = glob.glob(im_classified_path + im_date + '_' + site_name + '_' + dataset + '*.nc')[0]\n",
    "    im_classified = xr.open_dataset(im_classified_fn)\n",
    "    im_classified = xr.where(im_classified!=-9999, im_classified, np.nan)\n",
    "    # snowline\n",
    "    sl_path = study_sites_path + site_name + '/imagery/snowlines/'\n",
    "    sl_fn = glob.glob(sl_path + im_date + '_' + site_name + '_' + dataset + '*.csv')[0]\n",
    "    sl = pd.read_csv(sl_fn)\n",
    "    # RGB image\n",
    "    if dataset=='PlanetScope':\n",
    "        im_path = study_sites_path + site_name + '/imagery/PlanetScope/mosaics/'\n",
    "        im_fn = glob.glob(im_path + im_date[0:8] + '*.tif')[1]\n",
    "        im_da = rxr.open_rasterio(im_fn)\n",
    "        im_ds = im_da.to_dataset('band') # convert to xarray.DataSet\n",
    "        band_names = list(dataset_dict[dataset]['refl_bands'].keys())\n",
    "        im_ds = im_ds.rename({i + 1: name for i, name in enumerate(band_names)})\n",
    "        im_ds = xr.where(im_ds != dataset_dict[dataset]['no_data_value'],\n",
    "                         im_ds / dataset_dict[dataset]['image_scalar'], np.nan)\n",
    "        # plot\n",
    "        ax[0].imshow(np.dstack([im_ds[dataset_dict[dataset]['RGB_bands'][0]].data, \n",
    "                                  im_ds[dataset_dict[dataset]['RGB_bands'][1]].data, \n",
    "                                  im_ds[dataset_dict[dataset]['RGB_bands'][2]].data]),\n",
    "                      extent=(np.min(im_ds.x.data), np.max(im_ds.x.data), \n",
    "                              np.min(im_ds.y.data), np.max(im_ds.y.data)))\n",
    "    else:\n",
    "        # load AOI\n",
    "        AOI_path = study_sites_path + site_name + '/AOIs/'\n",
    "        AOI_fn = glob.glob(AOI_path + '*USGS*.shp')[0]\n",
    "        AOI_UTM = gpd.read_file(AOI_fn)\n",
    "        # load image from GEE\n",
    "        im_dt = np.datetime64(im_date[0:4] + '-' + im_date[4:6] + '-' + im_date[6:8])\n",
    "        date_start, date_end = str(im_dt), str(im_dt + np.timedelta64(1, 'D'))\n",
    "        month_start, month_end = 1, 12\n",
    "        cloud_cover_max = 100\n",
    "        mask_clouds=True\n",
    "        im_ds = f.query_gee_for_imagery(dataset_dict, dataset, AOI_UTM, date_start, date_end, month_start, month_end, cloud_cover_max, mask_clouds)[0]\n",
    "        print(im_ds.rio.crs.to_epsg())\n",
    "        # plot\n",
    "        ax[0].imshow(np.dstack([im_ds[dataset_dict[dataset]['RGB_bands'][0]].data[0], \n",
    "                                  im_ds[dataset_dict[dataset]['RGB_bands'][1]].data[0], \n",
    "                                  im_ds[dataset_dict[dataset]['RGB_bands'][2]].data[0]]),\n",
    "                      extent=(np.min(im_ds.x.data), np.max(im_ds.x.data), \n",
    "                              np.min(im_ds.y.data), np.max(im_ds.y.data)))    \n",
    "\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    ax[1].imshow(im_classified.classified.data[0], cmap=ListedColormap(colors_classified), clim=(1,5),\n",
    "                   extent=(np.min(im_classified.x.data), np.max(im_classified.x.data), \n",
    "                           np.min(im_classified.y.data), np.max(im_classified.y.data)))\n",
    "    ax[1].set_xticks([])\n",
    "    ax[1].set_yticks([])\n",
    "    # reset axes on RGB image\n",
    "    ax[0].set_xlim(ax[1].get_xlim())\n",
    "    ax[0].set_ylim(ax[1].get_ylim())   \n",
    "    if sl['geometry'][0] != '[]':\n",
    "        sl['geometry'] = gpd.GeoSeries.from_wkt(sl['geometry'])\n",
    "        ax[0].plot(*sl['geometry'][0].coords.xy, '.m', markersize=1)\n",
    "        ax[1].plot(*sl['geometry'][0].coords.xy, '.m', markersize=1)\n",
    "        \n",
    "    plt.show()\n",
    "        \n",
    "    # save figure\n",
    "    if save_figures:\n",
    "        fig_fn = 'successes+shortcomings_' + site_name + '_' + im_date + '_' + dataset + '.png'\n",
    "        fig.savefig(out_path + fig_fn, dpi=300)\n",
    "        print('figure saved to file: ' + out_path + fig_fn)\n",
    "                                        \n",
    "    i+=1\n",
    "\n",
    "# -----Ideal conditions (successes)\n",
    "im_ideal_dates = ['20190706T151748', '20190731T125046']\n",
    "im_ideal_site_names = ['Gulkana', 'Sperry']\n",
    "im_ideal_datasets = ['Sentinel-2_SR', 'Sentinel-2_SR']\n",
    "text_labels = [['c)', 'd)'], ['g)', 'h)']]\n",
    "i=0 # loop counter\n",
    "for im_date, site_name, dataset, text_label in list(zip(im_ideal_dates, im_ideal_site_names, im_ideal_datasets, text_labels)):\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16,8))\n",
    "\n",
    "    # classified image\n",
    "    im_classified_path = study_sites_path + site_name + '/imagery/classified/'\n",
    "    im_classified_fn = glob.glob(im_classified_path + im_date + '_' + site_name + '_' + dataset + '*.nc')[0]\n",
    "    im_classified = xr.open_dataset(im_classified_fn)\n",
    "    im_classified = xr.where(im_classified!=-9999, im_classified, np.nan)\n",
    "    # snowline\n",
    "    sl_path = study_sites_path + site_name + '/imagery/snowlines/'\n",
    "    sl_fn = glob.glob(sl_path + im_date + '_' + site_name + '_' + dataset + '*.csv')[0]\n",
    "    sl = pd.read_csv(sl_fn)\n",
    "    # AOI\n",
    "    AOI_path = study_sites_path + site_name + '/AOIs/'\n",
    "    AOI_fn = glob.glob(AOI_path + '*USGS*.shp')[0]\n",
    "    AOI_UTM = gpd.read_file(AOI_fn)\n",
    "    # RGB image\n",
    "    if dataset=='PlanetScope':\n",
    "        im_path = study_sites_path + site_name + '/imagery/PlanetScope/mosaics/'\n",
    "        im_fn = glob.glob(im_path + im_date[0:8] + '*.tif')[1]\n",
    "        im_da = rxr.open_rasterio(im_fn)\n",
    "        im_ds = im_da.to_dataset('band') # convert to xarray.DataSet\n",
    "        band_names = list(dataset_dict[dataset]['refl_bands'].keys())\n",
    "        im_ds = im_ds.rename({i + 1: name for i, name in enumerate(band_names)})\n",
    "        im_ds = xr.where(im_ds != dataset_dict[dataset]['no_data_value'],\n",
    "                         im_ds / dataset_dict[dataset]['image_scalar'], np.nan)  \n",
    "        # plot\n",
    "        ax[0].imshow(np.dstack([im_ds[dataset_dict[dataset]['RGB_bands'][0]].data, \n",
    "                                  im_ds[dataset_dict[dataset]['RGB_bands'][1]].data, \n",
    "                                  im_ds[dataset_dict[dataset]['RGB_bands'][2]].data]),\n",
    "                      extent=(np.min(im_ds.x.data), np.max(im_ds.x.data), \n",
    "                              np.min(im_ds.y.data), np.max(im_ds.y.data)))\n",
    "    else:\n",
    "\n",
    "        # load image from GEE\n",
    "        im_dt = np.datetime64(im_date[0:4] + '-' + im_date[4:6] + '-' + im_date[6:8])\n",
    "        date_start, date_end = str(im_dt), str(im_dt + np.timedelta64(1, 'D'))\n",
    "        month_start, month_end = 1, 12\n",
    "        cloud_cover_max = 100\n",
    "        mask_clouds=True\n",
    "        im_ds = f.query_gee_for_imagery(dataset_dict, dataset, AOI_UTM, date_start, date_end, month_start, month_end, cloud_cover_max, mask_clouds)[0]\n",
    "        # plot\n",
    "        ax[0].imshow(np.dstack([im_ds[dataset_dict[dataset]['RGB_bands'][0]].data[0], \n",
    "                                  im_ds[dataset_dict[dataset]['RGB_bands'][1]].data[0], \n",
    "                                  im_ds[dataset_dict[dataset]['RGB_bands'][2]].data[0]]),\n",
    "                      extent=(np.min(im_ds.x.data), np.max(im_ds.x.data), \n",
    "                              np.min(im_ds.y.data), np.max(im_ds.y.data)))\n",
    "    \n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    ax[1].imshow(im_classified.classified.data[0], cmap=ListedColormap(colors_classified), clim=(1,5),\n",
    "                   extent=(np.min(im_classified.x.data), np.max(im_classified.x.data), \n",
    "                           np.min(im_classified.y.data), np.max(im_classified.y.data)))\n",
    "    ax[1].set_xticks([])\n",
    "    ax[1].set_yticks([])    \n",
    "    # reset axes on RGB image\n",
    "    ax[0].set_xlim(ax[1].get_xlim())\n",
    "    ax[0].set_ylim(ax[1].get_ylim())\n",
    "\n",
    "    if site_name=='Sperry':\n",
    "        markersize=5\n",
    "    else:\n",
    "        markersize=2\n",
    "    if sl['geometry'][0] != '[]':\n",
    "        sl['geometry'] = gpd.GeoSeries.from_wkt(sl['geometry'])\n",
    "        ax[0].plot(*sl['geometry'][0].coords.xy, '.m', markersize=markersize)\n",
    "        ax[1].plot(*sl['geometry'][0].coords.xy, '.m', markersize=markersize)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # save figure\n",
    "    if save_figures:\n",
    "        fig_fn = 'successes+shortcomings_' + site_name + '_' + im_date + '_' + dataset + '.png'\n",
    "        fig.savefig(out_path + fig_fn, dpi=300)\n",
    "        print('figure saved to file: ' + out_path + fig_fn)\n",
    "                                        \n",
    "    i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c99bedf-4300-4adf-983f-0ebe0ecf97c8",
   "metadata": {},
   "source": [
    "## Figure S1. Firn detection at Wolverine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5774a18e-5089-4481-9f77-e648a58bf1f5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Initialize GEE\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()\n",
    "\n",
    "# -----Set up figure\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = [fig.add_axes([0.05, 0.4, 0.35, 0.3]), \n",
    "      fig.add_axes([0.43, 0.4, 0.35, 0.3]),\n",
    "      fig.add_axes([0.05, 0.05, 0.35, 0.3]),\n",
    "      fig.add_axes([0.43, 0.05, 0.35, 0.3])\n",
    "     ]\n",
    "plt.rcParams.update({'font.size':14, 'font.sans-serif':'Arial'})\n",
    "\n",
    "# -----Difficult conditions (unsuccessful)\n",
    "im_date = '20200822T152833'\n",
    "site_name = 'Wolverine'\n",
    "dataset = 'Sentinel-2_TOA'\n",
    "# AOI\n",
    "AOI_path = study_sites_path + site_name + '/AOIs/'\n",
    "AOI_fn = glob.glob(AOI_path + '*USGS*.shp')[0]\n",
    "AOI_UTM = gpd.read_file(AOI_fn)\n",
    "# image\n",
    "im_dt = np.datetime64(im_date[0:4] + '-' + im_date[4:6] + '-' + im_date[6:8])\n",
    "date_start, date_end = str(im_dt), str(im_dt + np.timedelta64(1, 'D'))\n",
    "month_start, month_end = 1, 12\n",
    "cloud_cover_max = 100\n",
    "mask_clouds=True\n",
    "im_ds = f.query_gee_for_imagery(dataset_dict, dataset, AOI_UTM, date_start, date_end, month_start, month_end, cloud_cover_max, mask_clouds)[0]\n",
    "# classified image\n",
    "im_classified_path = study_sites_path + site_name + '/imagery/classified/'\n",
    "im_classified_fn = glob.glob(im_classified_path + im_date + '_' + site_name + '_' + dataset + '*.nc')[0]\n",
    "im_classified = xr.open_dataset(im_classified_fn)\n",
    "im_classified = xr.where(im_classified!=-9999, im_classified, np.nan)\n",
    "# snowline\n",
    "sl_path = study_sites_path + site_name + '/imagery/snowlines/'\n",
    "sl_fn = glob.glob(sl_path + im_date + '_' + site_name + '_' + dataset + '*.csv')[0]\n",
    "sl = pd.read_csv(sl_fn)\n",
    "# plot\n",
    "ax[0].imshow(np.dstack([im_ds[dataset_dict[dataset]['RGB_bands'][0]].data[0], \n",
    "                          im_ds[dataset_dict[dataset]['RGB_bands'][1]].data[0], \n",
    "                          im_ds[dataset_dict[dataset]['RGB_bands'][2]].data[0]]),\n",
    "              extent=(np.min(im_ds.x.data), np.max(im_ds.x.data), \n",
    "                      np.min(im_ds.y.data), np.max(im_ds.y.data)))\n",
    "ax[1].imshow(im_classified.classified.data[0], cmap=ListedColormap(colors_classified), clim=(1,5),\n",
    "               extent=(np.min(im_classified.x.data), np.max(im_classified.x.data), \n",
    "                       np.min(im_classified.y.data), np.max(im_classified.y.data)))\n",
    "# zoom in on firn area\n",
    "ax[0].set_ylim(6698*1e3, 6701.5*1e3)\n",
    "ax[1].set_ylim(6698*1e3, 6701.5*1e3) \n",
    "if sl['geometry'][0] != '[]':\n",
    "    sl['geometry'] = gpd.GeoSeries.from_wkt(sl['geometry'])\n",
    "    ax[0].plot(*sl['geometry'][0].coords.xy, '.m', markersize=1)\n",
    "    ax[1].plot(*sl['geometry'][0].coords.xy, '.m', markersize=1)\n",
    "                                        \n",
    "# -----Ideal conditions (successes)\n",
    "im_date = '20200822T152833'\n",
    "dataset = 'Sentinel-2_SR'\n",
    "# image\n",
    "im_dt = np.datetime64(im_date[0:4] + '-' + im_date[4:6] + '-' + im_date[6:8])\n",
    "date_start, date_end = str(im_dt), str(im_dt + np.timedelta64(1, 'D'))\n",
    "month_start, month_end = 1, 12\n",
    "cloud_cover_max = 100\n",
    "mask_clouds=True\n",
    "im_ds = f.query_gee_for_imagery(dataset_dict, dataset, AOI_UTM, date_start, date_end, month_start, month_end, cloud_cover_max, mask_clouds)[0]\n",
    "# classified image\n",
    "im_classified_path = study_sites_path + site_name + '/imagery/classified/'\n",
    "im_classified_fn = glob.glob(im_classified_path + im_date + '_' + site_name + '_' + dataset + '*.nc')[0]\n",
    "im_classified = xr.open_dataset(im_classified_fn)\n",
    "im_classified = xr.where(im_classified!=-9999, im_classified, np.nan)\n",
    "# snowline\n",
    "sl_path = study_sites_path + site_name + '/imagery/snowlines/'\n",
    "sl_fn = glob.glob(sl_path + im_date + '_' + site_name + '_' + dataset + '*.csv')[0]\n",
    "sl = pd.read_csv(sl_fn)\n",
    "# plot\n",
    "ax[2].imshow(np.dstack([im_ds[dataset_dict[dataset]['RGB_bands'][0]].data[0], \n",
    "                          im_ds[dataset_dict[dataset]['RGB_bands'][1]].data[0], \n",
    "                          im_ds[dataset_dict[dataset]['RGB_bands'][2]].data[0]]),\n",
    "              extent=(np.min(im_ds.x.data), np.max(im_ds.x.data), \n",
    "                      np.min(im_ds.y.data), np.max(im_ds.y.data)))\n",
    "ax[3].imshow(im_classified.classified.data[0], cmap=ListedColormap(colors_classified), clim=(1,5),\n",
    "               extent=(np.min(im_classified.x.data), np.max(im_classified.x.data), \n",
    "                       np.min(im_classified.y.data), np.max(im_classified.y.data)))\n",
    "if sl['geometry'][0] != '[]':\n",
    "    sl['geometry'] = gpd.GeoSeries.from_wkt(sl['geometry'])\n",
    "    ax[2].plot(*sl['geometry'][0].coords.xy, '.m', markersize=1)\n",
    "    ax[3].plot(*sl['geometry'][0].coords.xy, '.m', markersize=1)\n",
    "# zoom in on firn area\n",
    "ax[2].set_ylim(6698*1e3, 6701.5*1e3)\n",
    "ax[3].set_ylim(6698*1e3, 6701.5*1e3)\n",
    " \n",
    "\n",
    "# remove axis ticks and labels\n",
    "for axis in ax:\n",
    "    axis.set_xticks([])\n",
    "    axis.set_yticks([])\n",
    "    \n",
    "# add dummy points for legend\n",
    "xmin, xmax = ax[0].get_xlim()\n",
    "ymin, ymax = ax[0].get_ylim()\n",
    "ax[3].plot([-10, -10], [-10, -20], '-m', linewidth=3, label='snowline')\n",
    "ax[3].plot(-10, -10, 's', markersize=20, markerfacecolor=colors_classified[0], \n",
    "             markeredgecolor='k', markeredgewidth=1, label='snow')\n",
    "ax[3].plot(-10, -10, 's', markersize=20, markerfacecolor=colors_classified[1], \n",
    "             markeredgecolor='k', markeredgewidth=1, label='shadowed snow')\n",
    "ax[3].plot(-10, -10, 's', markersize=20, markerfacecolor=colors_classified[2], \n",
    "             markeredgecolor='k', markeredgewidth=1, label='ice')\n",
    "ax[3].plot(-10, -10, 's', markersize=20, markerfacecolor=colors_classified[3], \n",
    "             markeredgecolor='k', markeredgewidth=1, label='rock')\n",
    "ax[3].plot(-10, -10, 's', markersize=20, markerfacecolor=colors_classified[4], \n",
    "             markeredgecolor='k', markeredgewidth=1, label='water')\n",
    "ax[3].set_xlim(xmin, xmax)\n",
    "ax[3].set_ylim(ymin, ymax)\n",
    "\n",
    "# add text labels\n",
    "ax[0].text(ax[0].get_xlim()[0] + 0.89*(ax[0].get_xlim()[1] - ax[0].get_xlim()[0]),\n",
    "             ax[0].get_ylim()[0] + 0.1*(ax[0].get_ylim()[1] - ax[0].get_ylim()[0]),\n",
    "             'a)', backgroundcolor='w')\n",
    "ax[1].text(ax[1].get_xlim()[0] + 0.89*(ax[1].get_xlim()[1] - ax[1].get_xlim()[0]),\n",
    "             ax[1].get_ylim()[0] + 0.1*(ax[1].get_ylim()[1] - ax[1].get_ylim()[0]),\n",
    "             'b)', backgroundcolor='w')   \n",
    "ax[2].text(ax[2].get_xlim()[0] + 0.89*(ax[2].get_xlim()[1] - ax[2].get_xlim()[0]),\n",
    "             ax[2].get_ylim()[0] + 0.1*(ax[2].get_ylim()[1] - ax[2].get_ylim()[0]),\n",
    "             'c)', backgroundcolor='w')\n",
    "ax[3].text(ax[3].get_xlim()[0] + 0.89*(ax[3].get_xlim()[1] - ax[1].get_xlim()[0]),\n",
    "             ax[3].get_ylim()[0] + 0.1*(ax[3].get_ylim()[1] - ax[1].get_ylim()[0]),\n",
    "             'd)', backgroundcolor='w') \n",
    "\n",
    "# add legend\n",
    "handles, labels = ax[3].get_legend_handles_labels()\n",
    "leg = fig.legend(handles, labels, loc = (0.78, 0.35))\n",
    "\n",
    "# add arrows indicating firn\n",
    "for axis in ax:\n",
    "    axis.arrow(395*1e3, 6698.88*1e3, 0, 0.5e3, color='white', width=0.5, head_width=150, head_length=150, length_includes_head=True, zorder=10)\n",
    "    axis.arrow(393.8*1e3, 6698.88*1e3, 0, 0.5e3, color='white', width=0.5, head_width=150, head_length=150, length_includes_head=True, zorder=10)\n",
    "    axis.arrow(395.52*1e3, 6698.7*1e3, 0.4e3, 0.4e3, color='white', width=0.5, head_width=150, head_length=150, length_includes_head=True, zorder=10)\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# -----Save figure\n",
    "if save_figures:\n",
    "    fig_fn = 'example_firn_detection.png'\n",
    "    fig.savefig(out_path + fig_fn, dpi=300, facecolor='w', edgecolor='none', bbox_inches='tight')\n",
    "    print('figure saved to file: '+ out_path + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cbac9f-7b07-45c6-ad29-85340125abf1",
   "metadata": {},
   "source": [
    "## Example SCA time series at South Cascade Glacier for GitHub repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dfd65c-d01b-489c-8fde-d041b0ae2f52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "site_name = 'SouthCascade'\n",
    "\n",
    "# -----Load estimated snowlines  \n",
    "sl_ests_full = pd.DataFrame()    \n",
    "sl_est_fns = glob.glob(study_sites_path + site_name + '/imagery/snowlines/*snowline.csv')\n",
    "sl_ests = pd.DataFrame()\n",
    "for sl_est_fn in sl_est_fns:\n",
    "    sl_est = pd.read_csv(sl_est_fn)\n",
    "    sl_ests = pd.concat([sl_ests, sl_est])\n",
    "sl_ests.reset_index(drop=True, inplace=True)\n",
    "sl_ests['datetime'] = pd.to_datetime(sl_ests['datetime'], format='mixed')\n",
    "# reset index and add week-of-year column\n",
    "sl_ests.reset_index(drop=True, inplace=True)\n",
    "sl_ests['Week'] = sl_ests['datetime'].dt.isocalendar().week\n",
    "# convert SCA from m2 to km2\n",
    "sl_ests['SCA_km2'] = np.divide(sl_ests['SCA_m2'].values, 1e6)\n",
    "\n",
    "# -----Set up figure\n",
    "fontsize=12\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "# time series: SCA\n",
    "fig1, ax1 = plt.subplots(3, 2, figsize=(10, 10), gridspec_kw=dict(width_ratios=[4,1]))\n",
    "# fmt_year = matplotlib.dates.YearLocator() # minor ticks every year\n",
    "fmt_year = matplotlib.dates.DateFormatter(\"%Y\")\n",
    "alpha = 0.9\n",
    "\n",
    "# -----Define axis limits\n",
    "xmin, xmax = np.datetime64('2013-05-01T00:00:00'), np.datetime64('2022-12-01T00:00:00')\n",
    "sl_elev_median_min = np.nanmin(sl_ests['snowline_elevs_median_m'])\n",
    "sl_elev_median_max = np.nanmax(sl_ests['snowline_elevs_median_m'])\n",
    "yrange1 = [np.nanmax(sl_ests['SCA_km2']) * -0.1, np.nanmax(sl_ests['SCA_km2']) * 1.1]\n",
    "yrange2 = [-0.1, 1.1]\n",
    "yrange3 = [np.nanmin(sl_ests['snowline_elevs_median_m']) * 0.98, np.nanmax(sl_ests['snowline_elevs_median_m']) * 1.02]\n",
    "        \n",
    "# -----Plot time series of SCA, AAR, and median snowline elevations\n",
    "for column, ylabel, yrange, i in list(zip(['SCA_km2', 'AAR', 'snowline_elevs_median_m'],\n",
    "                                          ['SCA [km$^2$]', 'AAR', 'Median snowline elevation [m]'],\n",
    "                                          [yrange1, yrange2, yrange3],\n",
    "                                          np.arange(0,3))):\n",
    "    # PlanetScope\n",
    "    ax1[i,0].plot(sl_ests['datetime'].loc[sl_ests['dataset']=='PlanetScope'], \n",
    "                   sl_ests[column].loc[sl_ests['dataset']=='PlanetScope'].values, \n",
    "                   '.', markeredgecolor='w', markerfacecolor=color_PlanetScope, \n",
    "                alpha=alpha, markersize=10, markeredgewidth=1, label='PlanetScope')\n",
    "    # Sentinel-2 SR\n",
    "    ax1[i,0].plot(sl_ests['datetime'].loc[sl_ests['dataset']=='Sentinel-2_SR'], \n",
    "                  sl_ests[column].loc[sl_ests['dataset']=='Sentinel-2_SR'].values, \n",
    "                  'D', markeredgecolor='w', markerfacecolor=color_Sentinel2, \n",
    "                  alpha=alpha, markersize=4, markeredgewidth=1, label='Sentinel-2 SR')\n",
    "    # Sentinel-2 TOA\n",
    "    ax1[i,0].plot(sl_ests['datetime'].loc[sl_ests['dataset']=='Sentinel-2_TOA'], \n",
    "                  sl_ests[column].loc[sl_ests['dataset']=='Sentinel-2_TOA'].values, \n",
    "                  'D', markeredgecolor=color_Sentinel2, markerfacecolor='None', \n",
    "                  alpha=alpha, markersize=3, markeredgewidth=1.2, label='Sentinel-2 TOA')  \n",
    "    # Landsat\n",
    "    ax1[i,0].plot(sl_ests['datetime'].loc[sl_ests['dataset']=='Landsat'], \n",
    "                  sl_ests[column].loc[sl_ests['dataset']=='Landsat'].values, \n",
    "                  '^', markeredgecolor='w', markerfacecolor=color_Landsat, \n",
    "                  alpha=alpha, markersize=6, markeredgewidth=1, label='Landsat')   \n",
    "    # adjust axis\n",
    "    ax1[i,0].set_ylabel(ylabel)\n",
    "    ax1[i,0].set_xlim(xmin, xmax)\n",
    "    ax1[i,0].set_ylim(yrange[0], yrange[1])\n",
    "    ax1[i,0].grid(True)\n",
    "\n",
    "    # -----Plot light grey boxes where no observations exist \n",
    "    years = np.arange(2013, 2022, step=1)\n",
    "    for year in years:\n",
    "        min_date, max_date = np.datetime64(str(year) + '-11-01'), np.datetime64(str(year+1) + '-05-01')\n",
    "        rect = matplotlib.patches.Rectangle((min_date, yrange[0]), width=max_date-min_date, height=yrange[1]-yrange[0], color='#d9d9d9')\n",
    "        ax1[i,0].add_patch(rect)\n",
    "    \n",
    "    # -----Calculate median and interquartile ranges for weekly trends\n",
    "    q1, q3 = 0.25, 0.75 # define quartiles\n",
    "    # calculate weekly trends using only Sentinel-2 snowlines\n",
    "    sl_ests_noPS = sl_ests.loc[sl_ests['dataset']!='PlanetScope']   \n",
    "    weekly = sl_ests_noPS.groupby(by='Week')[column].agg(['median', lambda x: x.quantile(q1), lambda x: x.quantile(q3)])\n",
    "    weekly.columns = ['Median', 'Q1', 'Q3'] # Rename the columns for clarity\n",
    "    weekly.index = weekly.index.astype(float)\n",
    "    # plot\n",
    "    ax1[i,1].fill_between(weekly.index, weekly['Q1'], weekly['Q3'].values, color='k', alpha=0.5)\n",
    "    ax1[i,1].plot(weekly.index, weekly['Median'], color='k', linewidth=2)\n",
    "    ax1[i,1].grid(True)\n",
    "    # adjust axis\n",
    "    ax1[i,1].set_xlim(15, 45)\n",
    "    ax1[i,1].set_xticks([18, 31, 44])\n",
    "    ax1[i,1].set_xticklabels([])\n",
    "    ax1[i,1].set_xticklabels(['May', 'Aug', 'Nov'])\n",
    "    ax1[i,1].set_ylim(yrange[0], yrange[1])\n",
    "\n",
    "ax1[0,1].set_title('Weekly median trend')\n",
    "\n",
    "# -----Plot glacier area on SCA plots\n",
    "# AOI_fn = glob.glob(os.path.join(study_sites_path, site_name, 'AOIs', site_name + '*USGS*.shp'))[0]\n",
    "# AOI = gpd.read_file(AOI_fn)\n",
    "# ax1[0,0].plot([xmin, xmax], [AOI.geometry[0].area / 1e6, AOI.geometry[0].area / 1e6], '--', color='grey')\n",
    "# ax1[0,1].plot([xmin, xmax], [AOI.geometry[0].area / 1e6, AOI.geometry[0].area / 1e6], '--', color='grey')\n",
    "        \n",
    "# -----Add legend to axis 1\n",
    "ax1[0,0].legend(loc='center', bbox_to_anchor=(0.5, 1.1), handletextpad=0.1, labelspacing=0.5, markerscale=2, ncol=4)\n",
    "fig1.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----Save figures\n",
    "if save_figures:\n",
    "    fig1_fn = os.path.join(figures_out_path, 'timeseries_SouthCascade_Glacier.png')\n",
    "    fig1.savefig(fig1_fn, dpi=300, facecolor='w', edgecolor='none', bbox_inches='tight')\n",
    "    print('figure 1 saved to file: ' + fig1_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9430d0c4",
   "metadata": {},
   "source": [
    "## Snow cover products comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1083e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----Load Landsat fSCA\n",
    "# LS_fn = base_path+'../study-sites/Wolverine/imagery/Landsat/fSCA/LC08_AK_016008_20210829_20210913_02_SNOW/LC08_AK_016008_20210829_20210913_02_VIEWABLE_SNOW_UTM.TIF'\n",
    "# LS = rxr.open_rasterio(LS_fn)\n",
    "# # remove no-data values\n",
    "# LS = LS.where(LS != -9999)\n",
    "# # account for image multiplier\n",
    "# LS_scalar = 0.001\n",
    "# LS = LS * LS_scalar\n",
    "# crs = LS.rio.crs.to_string()\n",
    "\n",
    "# # -----Load MODIS fSCA\n",
    "# M_fn = base_path+'../study-sites/Wolverine/imagery/MODIS/Terra_fSCA/2021_08_15.tif'\n",
    "# M = rxr.open_rasterio(M_fn)\n",
    "# # grab snow cover band\n",
    "# M_fSCA = M.isel(band=0)\n",
    "# # remove no data values\n",
    "# M_fSCA = M_fSCA.where(M_fSCA != -3.2768e04)\n",
    "# # reproject \n",
    "# M_fSCA= M_fSCA.rio.reproject(crs)\n",
    "\n",
    "# # -----Load PlanetScope image and snow\n",
    "# # RGB image\n",
    "# PS_path = base_path+'../study-sites/Wolverine/imagery/PlanetScope/adjusted-filtered/'\n",
    "# PS_fn = '20210815_20_adj.tif'\n",
    "# PS = rxr.open_rasterio(PS_path + PS_fn)\n",
    "# PS = PS / 1e4\n",
    "# # classify image\n",
    "# clf_fn = base_path+'/inputs-outputs/PS_classifier_all_sites.sav'\n",
    "# clf = pickle.load(open(clf_fn, 'rb'))\n",
    "# feature_cols_fn = base_path+'inputs-outputs/PS_feature_cols.pkl'\n",
    "# feature_cols = pickle.load(open(feature_cols_fn,'rb'))\n",
    "# sys.path.insert(1, base_path+'functions/')\n",
    "# from ps_pipeline_utils import classify_image\n",
    "# im_classified_fn, im = classify_image(PS_fn, PS_path, clf, feature_cols, False, None, out_path)\n",
    "# # load classified image\n",
    "# im_classified = rxr.open_rasterio(out_path + im_classified_fn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3180d701",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----Create snow colormap\n",
    "# color_snow = '#4eb3d3'\n",
    "# color_no_snow = 'w'\n",
    "# # create colormap\n",
    "# colors = [color_no_snow, color_snow]\n",
    "# cmp = cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n",
    "\n",
    "# # -----Plot\n",
    "# fig, ax = plt.subplots(2, 2, figsize=(10,10))\n",
    "# ax = ax.flatten()\n",
    "# plt.rcParams.update({'font.size':16, 'font.sans-serif':'Arial'})\n",
    "# xmin, xmax, ymin, ymax = 391, 399, 6694, 6702\n",
    "# # MODIS\n",
    "# M_im = ax[0].imshow(M_fSCA.data, cmap=cmp, clim=(0,100),\n",
    "#                     extent=(np.min(M_fSCA.x.data)/1000, np.max(M_fSCA.x.data)/1000, \n",
    "#                             np.min(M_fSCA.y.data)/1000, np.max(M_fSCA.y.data)/1000))\n",
    "# ax[0].set_xticks(np.linspace(392, 398, num=4))\n",
    "# ax[0].set_yticks(np.linspace(6694, 6702, num=5))\n",
    "# ax[0].set_xticklabels([])\n",
    "# ax[0].set_xlim(xmin, xmax)\n",
    "# ax[0].set_ylim(ymin, ymax)\n",
    "# ax[0].set_ylabel('Northing [km]')\n",
    "# ax[0].set_title('a) MODIS f$_{SCA}$')\n",
    "# # LS\n",
    "# LS_im = ax[1].imshow(LS_fSCA, cmap=cmp, clim=(0,1),\n",
    "#                    extent=(np.min(LS_x)/1000, np.max(LS_x)/1000, np.min(LS_y)/1000, np.max(LS_y)/1000))\n",
    "# ax[1].set_xticks(np.linspace(392, 398, num=4))\n",
    "# ax[1].set_yticks(np.linspace(6694, 6702, num=5))\n",
    "# ax[1].set_xticklabels([])\n",
    "# ax[1].set_yticklabels([])\n",
    "# ax[1].set_xlim(xmin, xmax)\n",
    "# ax[1].set_ylim(ymin, ymax)\n",
    "# ax[1].set_title('b) Landsat 8 f$_{SCA}$')\n",
    "# # PS RGB\n",
    "# ax[2].imshow(np.dstack([PS.data[2], PS.data[1], PS.data[0]]),\n",
    "#            extent=(np.min(PS.x.data)/1000, np.max(PS.x.data)/1000, np.min(PS.y.data)/1000, np.max(PS.y.data)/1000))\n",
    "# ax[2].set_xticks(np.linspace(392, 398, num=4))\n",
    "# ax[2].set_yticks(np.linspace(6694, 6702, num=5))\n",
    "# ax[2].set_xlim(xmin, xmax)\n",
    "# ax[2].set_ylim(ymin, ymax)\n",
    "# ax[2].set_ylabel('Northing [km]')\n",
    "# ax[2].set_xlabel('Easting [km]')\n",
    "# ax[2].set_title('c) PlanetScope RGB')\n",
    "# # PS snow\n",
    "# im_classified = im_classified.where(im_classified!=-9999)\n",
    "# im_binary = xr.where(im_classified<=2, 1, 0)\n",
    "# PS_snow_im = ax[3].imshow(im_binary.data[0], cmap=cmp, clim=(0,1),\n",
    "#                    extent=(np.min(PS.x.data)/1000, np.max(PS.x.data)/1000, np.min(PS.y.data)/1000, np.max(PS.y.data)/1000))\n",
    "# ax[3].set_xticks(np.linspace(392, 398, num=4))\n",
    "# ax[3].set_yticks(np.linspace(6694, 6702, num=5))\n",
    "# ax[3].set_yticklabels([])\n",
    "# ax[3].set_xlim(xmin, xmax)\n",
    "# ax[3].set_ylim(ymin, ymax)\n",
    "# ax[3].set_xlabel('Easting [km]')\n",
    "# ax[3].set_title('d) PlanetScope SCA')\n",
    "# # colorbar\n",
    "# cbar_ax = fig.add_axes([0.92, 0.35, 0.02, 0.3])\n",
    "# fig.colorbar(M_im, cax=cbar_ax)\n",
    "# plt.show()\n",
    "\n",
    "# if save_figures:\n",
    "#     fig.savefig(out_path+'comparing_SCA_products.png', dpi=300, facecolor='white', edgecolor='none')\n",
    "#     print('figure saved to file')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4215bd39-59b9-4a53-be64-0c5db99554fc",
   "metadata": {},
   "source": [
    "## Study sites: RGI regions 1 and 2 (Alaska, the Western U.S. and Canada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d55f3-12d3-4b55-aef0-f04546239ee6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Define paths in directory\n",
    "# path to RGI data\n",
    "RGI_path = '/Volumes/GoogleDrive/My Drive/Research/PhD/GIS_data/RGI/'\n",
    "# RGI shapefile names\n",
    "RGI_fns = ['01_rgi60_Alaska/01_rgi60_Alaska.shp', \n",
    "           '02_rgi60_WesternCanadaUS/02_rgi60_WesternCanadaUS.shp']\n",
    "\n",
    "# -----Load, format, filter, plot RGI glacier outlines\n",
    "# Create geopandas.DataFrame for storing RGIs\n",
    "RGI = gpd.GeoDataFrame()\n",
    "# Read RGI files\n",
    "for RGI_fn in RGI_fns:\n",
    "    file = gpd.read_file(RGI_path + RGI_fn)\n",
    "    RGI = pd.concat([RGI, file])\n",
    "# subset to glaciers with area > 5 km^2\n",
    "RGI_gt5 = RGI.loc[RGI['Area'] > 5].reset_index(drop=True)\n",
    "# change int data types to float for saving\n",
    "RGI_gt5[['Zmin', 'Zmax', 'Zmed', 'Slope', 'Aspect', 'Lmax', 'Status', 'Connect', \n",
    "         'Form', 'TermType', 'Surging', 'Linkages']] = RGI_gt5[['Zmin', 'Zmax', 'Zmed', 'Slope', 'Aspect', 'Lmax', \n",
    "                                                            'Status', 'Connect', 'Form', 'TermType', 'Surging', 'Linkages']].astype(float)\n",
    "\n",
    "# -----Grab list of all unique regions and subregions in dataset\n",
    "regions_subregions = sorted(RGI_gt5[['O1Region', 'O2Region']].drop_duplicates().values,\n",
    "                            key=operator.itemgetter(0, 1))\n",
    "subregions_names = ['Brooks Range', 'Alaska Range', 'Aleutians', 'W. Chugach Mtns.', 'St. Elias Mtns.', \n",
    "                    'N. Coast Ranges', 'N. Rockies', 'N. Cascades', 'S. Rockies', 'S. Cascades']\n",
    "subregions_colors = ['c', '#1f78b4', '#b2df8a', '#33a02c', '#fb9a99', '#e31a1c', \n",
    "                     '#fdbf6f', '#ff7f00', '#cab2d6', '#6a3d9a']\n",
    "\n",
    "# -----Plot all sites with color distinguishing subregions\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12,10))\n",
    "plt.rcParams.update({'font.size':12, 'font.sans-serif':'Arial'})\n",
    "crs = 'EPSG:9822' # Albers Equal Conic projection\n",
    "i=0\n",
    "for region, subregion in regions_subregions:\n",
    "    RGI_gt5_subregion = RGI_gt5.loc[(RGI_gt5['O1Region']==region) & (RGI_gt5['O2Region']==subregion)]\n",
    "    RGI_gt5_subregion_reproj = RGI_gt5_subregion.to_crs(crs)\n",
    "    for j in range(0, len(RGI_gt5_subregion)):\n",
    "        polygon = RGI_gt5_subregion_reproj.iloc[j]['geometry']\n",
    "        if j==0:\n",
    "            label=subregions_names[i]\n",
    "        else:\n",
    "            label='_nolegend_'\n",
    "        ax.plot(*polygon.exterior.xy, label=label, color=subregions_colors[i])\n",
    "    i+=1\n",
    "cx.add_basemap(ax, crs=crs, source=cx.providers.Esri.WorldShadedRelief, attribution=False)\n",
    "ax.legend(loc='center right', title='RGI Subregions', bbox_to_anchor=[1.25, 0.5, 0.2, 0.2])\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "ax.grid()\n",
    "plt.show()\n",
    "\n",
    "# -----Save figure\n",
    "fig.savefig(out_path + 'RGI_regions_1+2.png', dpi=300, facecolor='w')\n",
    "print('figure saved to file')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e224916",
   "metadata": {},
   "source": [
    "## Median snow line elevations for individual sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cf63bb-73e1-437b-b3c8-7b9e98c16148",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Settings and display parameters\n",
    "site_name = 'Gulkana' # as shown in folder name\n",
    "# site_name_display = 'Gulkana' # how to display on figure\n",
    "dataset_colors = {'Landsat': '#33a02c',\n",
    "                  'Sentinel2_TOA': '#1f78b4',\n",
    "                  'Sentinel2_SR': '#1f78b4',\n",
    "                  'PlanetScope': '#a6cee3'\n",
    "                 }\n",
    "\n",
    "# -----Path to USGS mass balance data\n",
    "usgs_path = '/Volumes/GoogleDrive/My Drive/Research/PhD/GIS_data/USGS/benchmarkGlacier_massBalance/'\n",
    "    \n",
    "# -----Load estimated snow lines  \n",
    "sl_est_fns = glob.glob(base_path + '../study-sites/' + site_name + '/imagery/snowlines/*snowline.pkl')\n",
    "sl_ests = pd.DataFrame()\n",
    "for sl_est_fn in sl_est_fns:\n",
    "    sl_est = pd.read_pickle(sl_est_fn)\n",
    "    sl_ests = pd.concat([sl_ests, sl_est])\n",
    "sl_ests = sl_ests.reset_index(drop=True)\n",
    "sl_ests['datetime'] = sl_ests['datetime'].astype(np.datetime64)\n",
    "\n",
    "# -----Plot\n",
    "# Set up figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 8))\n",
    "plt.rcParams.update({'font.size':16, 'font.sans-serif':'Arial'})\n",
    "fmt_month = matplotlib.dates.MonthLocator(bymonth=(5, 11)) # minor ticks every month\n",
    "fmt_year = matplotlib.dates.YearLocator() # minor ticks every year\n",
    "# PlanetScope\n",
    "ax.plot(sl_ests['datetime'].loc[sl_ests['dataset']=='PlanetScope'], \n",
    "           sl_ests['snowline_elevs_median'].loc[sl_ests['dataset']=='PlanetScope'], \n",
    "           '.', markeredgecolor='w', markerfacecolor=dataset_colors['PlanetScope'], \n",
    "           markersize=20, markeredgewidth=1, label='_nolegend_')\n",
    "# Sentinel-2 TOA\n",
    "ax.plot(sl_ests['datetime'].loc[sl_ests['dataset']=='Sentinel2_TOA'], \n",
    "           sl_ests['snowline_elevs_median'].loc[sl_ests['dataset']=='Sentinel2_TOA'], \n",
    "           '*', markeredgecolor='w', markerfacecolor=dataset_colors['Sentinel2_TOA'], \n",
    "           markersize=20, markeredgewidth=1, label='_nolegend_')\n",
    "# Sentinel-2 SR\n",
    "ax.plot(sl_ests['datetime'].loc[sl_ests['dataset']=='Sentinel2_SR'], \n",
    "           sl_ests['snowline_elevs_median'].loc[sl_ests['dataset']=='Sentinel2_SR'], \n",
    "           '*', markeredgecolor=dataset_colors['Sentinel2_SR'], markerfacecolor='None', \n",
    "           markersize=20, markeredgewidth=1, label='_nolegend_')\n",
    "# Landsat\n",
    "ax.plot(sl_ests['datetime'].loc[sl_ests['dataset']=='Landsat'], \n",
    "           sl_ests['snowline_elevs_median'].loc[sl_ests['dataset']=='Landsat'], \n",
    "           '^', markeredgecolor='w', markerfacecolor=dataset_colors['Landsat'], \n",
    "           markersize=15, markeredgewidth=1, label='_nolegend_')                \n",
    "        \n",
    "# -----Dummy points for legend\n",
    "# observed\n",
    "ax.plot(np.datetime64('1970-01-01'), 0, 'xk', \n",
    "           markersize=15, markeredgewidth=3, label='observed')\n",
    "# USGS\n",
    "ax.plot(np.datetime64('1970-01-01'), 0, 's', markerfacecolor='None', markeredgecolor='r', \n",
    "               ms=10, markeredgewidth=2, label='USGS ELA')\n",
    "# Landsat\n",
    "ax.plot(np.datetime64('1970-01-01'), 0, '^', \n",
    "           markeredgecolor=dataset_colors['Landsat'], markerfacecolor=dataset_colors['Landsat'], \n",
    "           markersize=12, label='Landsat 8/9')\n",
    "# Sentinel-2 TOA\n",
    "ax.plot(np.datetime64('1970-01-01'), 0, '*',\n",
    "           markeredgecolor='w', markerfacecolor=dataset_colors['Sentinel2_TOA'], \n",
    "           markersize=18, label='Sentinel-2 TOA')\n",
    "# Sentinel-2 SR\n",
    "ax.plot(np.datetime64('1970-01-01'), 0, '*',\n",
    "           markeredgecolor=dataset_colors['Sentinel2_SR'], markerfacecolor='None', \n",
    "           markersize=18, label='Sentinel-2 SR')\n",
    "# PlanetScope\n",
    "ax.plot(np.datetime64('1970-01-01'), 0, '.', \n",
    "       markeredgecolor=dataset_colors['PlanetScope'], markerfacecolor=dataset_colors['PlanetScope'], \n",
    "       markersize=20, label='PlanetScope')\n",
    "ax.legend(loc='center', bbox_to_anchor=(0.5, 1.05), ncol=6)\n",
    "\n",
    "# -----Observed snow lines\n",
    "# define path to digitized snow lines\n",
    "sl_obs_path = base_path + '../snowline-package/' + site_name + '/snowlines/'\n",
    "sl_obs_fns = glob.glob(sl_obs_path + '*.shp')\n",
    "# load AOI as gpd.GeoDataFrame\n",
    "AOI_fn = base_path + '../study-sites/' + site_name + '/glacier_outlines/' + site_name + '_USGS_*.shp'\n",
    "AOI_fn = glob.glob(AOI_fn)[0]\n",
    "AOI = gpd.read_file(AOI_fn)\n",
    "# load DEM from GEE\n",
    "DEM, AOI_UTM = pf.query_GEE_for_DEM(AOI)\n",
    "# loop through observed snow lines\n",
    "for j, sl_obs_fn in enumerate(sl_obs_fns):\n",
    "    # load observed snow line\n",
    "    sl_obs = gpd.read_file(sl_obs_fn)\n",
    "    # extract date from filename\n",
    "    date = sl_obs_fn.split('/'+site_name+'_')[1][0:8]\n",
    "    datetime = np.datetime64(date[0:4] + '-' + date[4:6] + '-' + date[6:8]\n",
    "                             + 'T00:00:00')\n",
    "    # reproject snow line to UTM\n",
    "    sl_obs_UTM = sl_obs.to_crs(str(AOI_UTM.crs.to_epsg()))\n",
    "    # interpolate elevation at snow line points\n",
    "    if len(sl_obs_UTM) > 1:\n",
    "        sl_obs_elev = np.array([DEM.sel(time=DEM.time.data[0], x=x, y=y, method='nearest').elevation.data \n",
    "                            for x, y in list(zip(sl_obs_UTM.geometry[1].xy[0], \n",
    "                                                 sl_obs_UTM.geometry[1].xy[1]))])\n",
    "    else:\n",
    "        sl_obs_elev = np.array([DEM.sel(time=DEM.time.data[0], x=x, y=y, method='nearest').elevation.data \n",
    "                            for x, y in list(zip(sl_obs_UTM.geometry[0].xy[0], \n",
    "                                                 sl_obs_UTM.geometry[0].xy[1]))])\n",
    "    # calculate median snow line elevation\n",
    "    sl_obs_elev_median = np.nanmedian(sl_obs_elev)\n",
    "    # plot\n",
    "    ax.plot(datetime, sl_obs_elev_median, 'xk', markersize=10, markeredgewidth=2, label='_nolegend_')   \n",
    "            \n",
    "    # load USGS ELA estimates\n",
    "    usgs_fn = usgs_path + site_name+'/Output_'+site_name+'_Glacier_Wide_solutions_calibrated.csv'\n",
    "    usgs_file = pd.read_csv(usgs_fn)\n",
    "    ELA = usgs_file['ELA']\n",
    "    ELA_date = usgs_file['Ba_Date'].astype(np.datetime64)\n",
    "    ax.plot(ELA_date, ELA, 's', markerfacecolor='None', markeredgecolor='r', \n",
    "               ms=10, markeredgewidth=2, label='_nolegend_')\n",
    "\n",
    "# -----Adjust axes\n",
    "# axis limits\n",
    "xmin, xmax = np.datetime64('2017-05-01T00:00:00'), np.datetime64('2023-01-01T00:00:00')\n",
    "sl_elev_median_min = np.min(sl_ests['snowline_elevs_median'])\n",
    "sl_elev_median_max = np.max(sl_ests['snowline_elevs_median'])\n",
    "ymin = sl_elev_median_min - 0.1*(sl_elev_median_max - sl_elev_median_min)\n",
    "ymax = sl_elev_median_max + 0.1*(sl_elev_median_max - sl_elev_median_min)\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.grid()\n",
    "# x-labels\n",
    "ax.xaxis.set_minor_formatter(matplotlib.dates.DateFormatter('%b'))\n",
    "ax.xaxis.set_major_locator(fmt_month)\n",
    "ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter('%b'))\n",
    "sec_xaxis = ax.secondary_xaxis(-0.1)\n",
    "sec_xaxis.xaxis.set_major_locator(fmt_year)\n",
    "sec_xaxis.xaxis.set_major_formatter(matplotlib.dates.DateFormatter('%Y'))\n",
    "# Hide the second x-axis spines and ticks\n",
    "sec_xaxis.spines['bottom'].set_visible(False)\n",
    "sec_xaxis.tick_params(length=0, pad=10)\n",
    "# y-label\n",
    "ax.set_ylabel('Elevation [m]')\n",
    "plt.show()\n",
    "    \n",
    "# -----Save figure\n",
    "fig_fn = 'median_snowline_elevs_' + site_name + '.png'\n",
    "fig.savefig(out_path + fig_fn, facecolor='w', dpi=300)\n",
    "print('figure saved to file: '+out_path+fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdacd23-947f-426b-8cb0-6fc48d3c4dba",
   "metadata": {},
   "source": [
    "## Training data characteristics by dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5953ef42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Load dataset dictionary\n",
    "with open(base_path + 'inputs-outputs/datasets_characteristics.pkl', 'rb') as fn:\n",
    "    dataset_dict = pickle.load(fn)\n",
    "\n",
    "# -----Define band ranges\n",
    "L8_dict = dataset_dict['Landsat']\n",
    "L8_dict['bands'] = {'SR_B2': {'name': 'blue',\n",
    "                              'min_nm': 450,\n",
    "                              'max_nm': 510},\n",
    "                    'SR_B3': {'name': 'green',\n",
    "                              'min_nm': 530,\n",
    "                              'max_nm': 590},\n",
    "                    'SR_B4': {'name': 'red',\n",
    "                              'min_nm': 640,\n",
    "                              'max_nm': 670},\n",
    "                    'SR_B5': {'name': 'NIR',\n",
    "                              'min_nm': 850,\n",
    "                              'max_nm': 880},\n",
    "                    'SR_B6': {'name': 'SWIR1',\n",
    "                              'min_nm': 1570,\n",
    "                              'max_nm': 1650},\n",
    "                    'SR_B7': {'name': 'SWIR2',\n",
    "                              'min_nm': 2110,\n",
    "                              'max_nm': 2290}\n",
    "                   }\n",
    "\n",
    "S2_dict = dataset_dict['Sentinel-2']\n",
    "S2_dict['bands'] = {'B2': {'name': 'blue',\n",
    "                           'wavelength_min_nm': 459,\n",
    "                           'wavelength_max_nm': 525\n",
    "                          },\n",
    "                    'B3': {'name': 'green',\n",
    "                           'wavelength_min_nm': 541,\n",
    "                           'wavelength_max_nm': 577\n",
    "                          },\n",
    "                    'B4': {'name': 'red',\n",
    "                           'wavelength_min_nm': 649,\n",
    "                           'wavelength_max_nm': 680\n",
    "                          },\n",
    "                    'B8': {'name': 'NIR',\n",
    "                           'wavelength_min_nm': 780,\n",
    "                           'wavelength_max_nm': 886\n",
    "                          },\n",
    "                    'B11': {'name': 'SWIR1',\n",
    "                           'wavelength_min_nm': 1567,\n",
    "                           'wavelength_max_nm': 1658\n",
    "                            \n",
    "                          },\n",
    "                    'B12': {'name': 'SWIR2',\n",
    "                           'wavelength_min_nm': 2114,\n",
    "                           'wavelength_max_nm': 2289\n",
    "                          }\n",
    "                   }\n",
    "PS_dict = dataset_dict['PlanetScope']\n",
    "PS_dict['bands'] = {'B1': {'name': 'blue',\n",
    "                           'wavelength_min_nm':,\n",
    "                           'wavelength_max_nm': \n",
    "                          }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
