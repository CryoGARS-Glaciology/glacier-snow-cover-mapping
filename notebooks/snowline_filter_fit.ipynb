{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16794d14-1223-48e3-8dd8-a9c749a7b2d8",
   "metadata": {},
   "source": [
    "# Filter snow line time series\n",
    "\n",
    "Rainey Aberle\n",
    "\n",
    "2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e7d881-cb28-43f2-bb00-85bc6465a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import iqr\n",
    "from time import mktime\n",
    "import seaborn as sns\n",
    "from scipy.stats import median_abs_deviation as mad\n",
    "\n",
    "# path to snow-cover-mapping\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/'\n",
    "figures_out_path = base_path + 'figures/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1311957-512b-418b-b854-3f4b9e6cd0c4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Fit a linear trendline to snowline time series\n",
    "# site_names = ['Wolverine', 'Gulkana', 'LemonCreek', 'SouthCascade', 'Sperry']\n",
    "\n",
    "# # -----loop through sites\n",
    "# nmc = 100 # number of monte carlo simulations\n",
    "# pTrain = 0.8 # percentage of points to use for training\n",
    "# for site_name in site_names:\n",
    "    \n",
    "#     print(site_name)\n",
    "#     print('----------')\n",
    "    \n",
    "#     # -----Load snowlines             \n",
    "#     # PlanetScope\n",
    "#     PS_sl_est_path = glob.glob(base_path + '../study-sites/' + site_name + \n",
    "#                                '/imagery/PlanetScope/snowlines/*snowlines.pkl')[0]\n",
    "#     PS_sl_est = pd.read_pickle(PS_sl_est_path)\n",
    "#     PS_sl_est['dataset'] = 'PlanetScope'\n",
    "#     # Sentinel-2\n",
    "#     S2_sl_est_path = glob.glob(base_path + '../study-sites/' + site_name + \n",
    "#                                '/imagery/Sentinel-2/snowlines/*snowlines.pkl')[0]\n",
    "#     S2_sl_est = pd.read_pickle(S2_sl_est_path)\n",
    "#     S2_sl_est['dataset'] = 'Sentinel2'\n",
    "#     # Landsat\n",
    "#     L_sl_est_path = glob.glob(base_path + '../study-sites/' + site_name + \n",
    "#                               '/imagery/Landsat/snowlines/*snowlines.pkl')[0]\n",
    "#     L_sl_est = pd.read_pickle(L_sl_est_path)\n",
    "#     L_sl_est['dataset'] = 'Landsat'\n",
    "    \n",
    "#     # -----Concatenate snowlines dataframes\n",
    "#     sl_est_full = pd.concat([PS_sl_est, S2_sl_est, L_sl_est])\n",
    "#     # unify datetime datatypes\n",
    "#     sl_est_full['datetime'] = sl_est_full['datetime'].astype(np.datetime64)\n",
    "#     # add year column\n",
    "#     sl_est_full['year'] = [x.year for x in sl_est_full['datetime']]\n",
    "#     # sort df by datetime\n",
    "#     sl_est_full = sl_est_full.sort_values(by=['datetime'])\n",
    "#     # grab unique years\n",
    "#     years = np.unique(sl_est_full['year'])\n",
    "    \n",
    "#     # -----Set up figure\n",
    "#     # fig, ax = plt.subplots(1, len(years), figsize=(6*len(years), 6))\n",
    "#     fig, ax = plt.subplots(1, 1, figsize=(6*len(years), 6))\n",
    "#     ax.plot(sl_est_full['datetime'], sl_est_full['snowlines_elevs_median'], 'ok', markersize=10, label='data points')\n",
    "#     ax.set_ylabel('Snowline elevation [m]')\n",
    "#     ax.grid()\n",
    "#     plt.rcParams.update({'font.size':14})\n",
    "\n",
    "#     # -----Loop through years\n",
    "#     results_linear_model = pd.DataFrame()\n",
    "#     for i, year in enumerate(years):\n",
    "        \n",
    "#         # subset df\n",
    "#         sl_est_full_year = sl_est_full.loc[(sl_est_full['year']==year) \n",
    "#                                            & (sl_est_full['datetime'] < pd.to_datetime(str(year)+'-10-01'))]\n",
    "#                                            # & (sl_est_full['datetime'] > pd.to_datetime(str(year)+'-06-01'))]\n",
    "        \n",
    "#         # conduct Monte Carlo simulations to generate 100 linear fit models\n",
    "#         # grab X and Y data from snowline dates and median elevations\n",
    "#         datetimes = np.ravel(sl_est_full_year['datetime'])\n",
    "#         snowlines_elevs_median = np.array(np.ravel(sl_est_full_year['snowlines_elevs_median']), dtype=float)\n",
    "#         # remove NaNs\n",
    "#         X = datetimes[~np.isnan(snowlines_elevs_median)]\n",
    "#         if len(X) < 2:\n",
    "#             print('Not enough data points in ' + str(year) + ' for linear fit, skipping...')\n",
    "#             continue\n",
    "#         y = snowlines_elevs_median[~np.isnan(snowlines_elevs_median)]\n",
    "#         # convert dates to days after the first image date capture\n",
    "#         day1 = X[0] - np.timedelta64(1, 'D')\n",
    "#         X = np.array([np.timedelta64(day - day1, 'D') for day in X], dtype=float)\n",
    "#         # initialize coefficients data frame\n",
    "#         X_mod = np.linspace(X[0], X[-1], num=100) # points at which to evaluate the model\n",
    "#         y_mod = np.zeros((nmc, len(X_mod))) # array to hold modeled Y values\n",
    "#         m_mod = np.zeros(nmc) # linear fit coefficients\n",
    "#         b_mod = np.zeros(nmc) # linear fit intercepts\n",
    "#         y_mod_err = np.zeros(nmc) # array to hold error associated with each model\n",
    "#         # loop through Monte Carlo simulations\n",
    "#         # fig2, ax2 = plt.subplots(1, 1, figsize=(6,6))\n",
    "#         for j in np.arange(0,nmc):\n",
    "#             # split into training and testing data\n",
    "#             X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=pTrain, shuffle=True)\n",
    "#             # fit ordinary squares to training data\n",
    "#             fit = LinearRegression(positive=True).fit(X_train.reshape(-1, 1), y_train)\n",
    "#             # create weights for each value using the residuals of the ordinary least squares\n",
    "            \n",
    "#             # apply fourier model to testing data\n",
    "#             y_pred = fit.predict(X_test.reshape(-1, 1))\n",
    "#             # calculate mean error\n",
    "#             y_mod_err[j] = np.sum(np.abs(y_test - y_pred)) / len(y_test)\n",
    "#             # apply the model to the full X data\n",
    "#             y_mod[j,:] = fit.predict(X_mod.reshape(-1, 1))\n",
    "#             # store the coefficient and intercept\n",
    "#             m_mod[j] = fit.coef_[0]\n",
    "#             b_mod[j] = fit.intercept_\n",
    "\n",
    "#         # identify model with lowest error\n",
    "#         Ibest = np.argmin(y_mod_err)\n",
    "#         m_mod_best = m_mod[Ibest]\n",
    "#         b_mod_best = b_mod[Ibest]\n",
    "#         y_mod_best = [x*m_mod_best+b_mod_best for x in X_mod]\n",
    "#         # convert X back to dates\n",
    "#         X = [np.timedelta64(int(x), 'D')+day1 for x in X]\n",
    "#         X_mod = [np.timedelta64(int(x), 'D')+day1 for x in X_mod]\n",
    "        \n",
    "#         # save results in DataFrame\n",
    "#         result_linear_model = pd.DataFrame({'year': year,\n",
    "#                                'X': [X],\n",
    "#                                'y': [y],\n",
    "#                                'X_mod': [X_mod],\n",
    "#                                'y_mod_best': [y_mod_best]\n",
    "#                               })\n",
    "#         results_linear_model = pd.concat([results_linear_model, result_linear_model])\n",
    "        \n",
    "#         # calculate the IQR and median models\n",
    "#         # m_mod_iqr, m_mod_median = iqr(m_mod), np.nanmedian(b_mod)\n",
    "#         # m_mod_P25, m_mod_P75 = m_mod_median - m_mod_iqr/2, m_mod_median + m_mod_iqr/2\n",
    "#         # b_mod_iqr, b_mod_median = iqr(b_mod), np.nanmedian(b_mod)\n",
    "#         # b_mod_P25, b_mod_P75 = b_mod_median - b_mod_iqr/2, b_mod_median + b_mod_iqr/2   \n",
    "#         # y_mod_median = [m_mod_median*x + b_mod_median for x in X_mod]\n",
    "#         # y_mod_P25 = [m_mod_P25*x + b_mod_P25 for x in X_mod]\n",
    "#         # y_mod_P75 = [m_mod_P75*x + b_mod_P75 for x in X_mod]\n",
    "#         # y_mod_iqr = iqr(y_mod, axis=0)\n",
    "#         # y_mod_median = np.nanmedian(y_mod, axis=0)\n",
    "#         # y_mod_P25 = y_mod_median - y_mod_iqr/2\n",
    "#         # y_mod_P75 = y_mod_median + y_mod_iqr/2\n",
    "        \n",
    "#         # plot results\n",
    "#         # ax[i].fill_between(X_mod, y_mod_P25, y_mod_P75, facecolor='blue', alpha=0.5, label='model$_{IQR}$')\n",
    "#         # ax[i].plot(X_mod, y_mod_median, '.-b', linewidth=1, label='model$_{median}$')\n",
    "#         if i==1:\n",
    "#             ax.plot(X_mod, y_mod_best, '-b', linewidth=3, label='best linear model')\n",
    "#         else:\n",
    "#             ax.plot(X_mod, y_mod_best, '-b', linewidth=3, label='_nolegend_')\n",
    "#         ax.legend(loc='best')\n",
    "#     plt.show()\n",
    "    \n",
    "#     # save results in df\n",
    "#     # results_snowlines = sl_est_full\n",
    "#     # out_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/../study-sites/' + site_name + '/imagery/'\n",
    "#     # out_fn = (site_name + '_' + str(np.max(sl_est_full['datetime'])).replace('-','').replace(':','')[0:8]\n",
    "#     #           + '_' + str(np.max(sl_est_full['datetime'])).replace('-','')[0:8] + '_snowlines.pkl')\n",
    "#     # results_snowlines.to_pickle(out_path + out_fn)\n",
    "#     # print('compiled snowlines saved to file: ' + out_path + out_fn)\n",
    "#     # print(' ')\n",
    "#     # results_linear_model.to_pickle(out_path + out_fn[0:-4]+ '_linear_model.pkl')\n",
    "#     # print('snowlines linear model saved to file: ' + out_path + out_fn)\n",
    "#     # print(' ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f88aa60-f5cb-4775-8f61-9db70dc0b4c6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### Apply a weighted least squares fit\n",
    "\n",
    "# site_names = ['Wolverine', 'Gulkana', 'LemonCreek', 'SouthCascade', 'Sperry']\n",
    "\n",
    "# # -----loop through sites\n",
    "# # nmc = 100 # number of monte carlo simulations\n",
    "# # pTrain = 0.8 # percentage of points to use for training\n",
    "# # set up figure\n",
    "# fig, ax = plt.subplots(len(site_names), 1, figsize=(20, 24))\n",
    "# plt.rcParams.update({'font.size':14})\n",
    "# for i, site_name in enumerate(site_names):\n",
    "\n",
    "#     print(site_name)\n",
    "#     print('----------')\n",
    "#     ax[i].grid()\n",
    "#     ax[i].set_title(site_name + ' Glacier')\n",
    "    \n",
    "#     # -----Load snowlines             \n",
    "#     # PlanetScope\n",
    "#     PS_sl_est_path = glob.glob(base_path + '../study-sites/' + site_name + \n",
    "#                                '/imagery/PlanetScope/snowlines/*snowlines.pkl')[0]\n",
    "#     PS_sl_est = pd.read_pickle(PS_sl_est_path)\n",
    "#     PS_sl_est['dataset'] = 'PlanetScope'\n",
    "#     # Sentinel-2\n",
    "#     S2_sl_est_path = glob.glob(base_path + '../study-sites/' + site_name + \n",
    "#                                '/imagery/Sentinel-2/snowlines/*snowlines.pkl')[0]\n",
    "#     S2_sl_est = pd.read_pickle(S2_sl_est_path)\n",
    "#     S2_sl_est['dataset'] = 'Sentinel2'\n",
    "#     # Landsat\n",
    "#     L_sl_est_path = glob.glob(base_path + '../study-sites/' + site_name + \n",
    "#                               '/imagery/Landsat/snowlines/*snowlines.pkl')[0]\n",
    "#     L_sl_est = pd.read_pickle(L_sl_est_path)\n",
    "#     L_sl_est['dataset'] = 'Landsat'    \n",
    "    \n",
    "#     # -----Concatenate snowlines dataframes\n",
    "#     sl_est_full = pd.concat([PS_sl_est, S2_sl_est, L_sl_est])\n",
    "#     # unify datetime datatypes\n",
    "#     sl_est_full['datetime'] = sl_est_full['datetime'].astype(np.datetime64)\n",
    "#     # add year column\n",
    "#     sl_est_full['year'] = [x.year for x in sl_est_full['datetime']]\n",
    "#     # sort df by datetime\n",
    "#     sl_est_full = sl_est_full.sort_values(by=['datetime'])\n",
    "#     # grab unique years\n",
    "#     years = np.unique(sl_est_full['year'])\n",
    "\n",
    "#     # -----Loop through years\n",
    "#     results_linear_model = pd.DataFrame()\n",
    "#     for j, year in enumerate(years):\n",
    "        \n",
    "#         # subset df\n",
    "#         sl_est_full_year = sl_est_full.loc[(sl_est_full['year']==year) \n",
    "#                                            & (sl_est_full['datetime'] < pd.to_datetime(str(year)+'-10-01'))]\n",
    "#                                            # & (sl_est_full['datetime'] > pd.to_datetime(str(year)+'-06-01'))]\n",
    "        \n",
    "#         # conduct Monte Carlo simulations to generate 100 linear fit models\n",
    "#         # grab X and Y data from snowline dates and median elevations\n",
    "#         datetimes = np.ravel(sl_est_full_year['datetime'])\n",
    "#         snowlines_elevs_median = np.array(np.ravel(sl_est_full_year['snowlines_elevs_median']), dtype=float)\n",
    "#         # remove NaNs\n",
    "#         X = datetimes[~np.isnan(snowlines_elevs_median)]\n",
    "#         if len(X) < 2:\n",
    "#             print('Not enough data points in ' + str(year) + ' for linear fit, skipping...')\n",
    "#             continue\n",
    "#         y = snowlines_elevs_median[~np.isnan(snowlines_elevs_median)]\n",
    "#         # convert dates to days after the first image date capture\n",
    "#         day1 = X[0] - np.timedelta64(1, 'D')\n",
    "#         X = np.array([np.timedelta64(day - day1, 'D') for day in X], dtype=float)\n",
    "#         # initialize coefficients data frame\n",
    "#         X_mod = np.linspace(X[0], X[-1], num=100) # points at which to evaluate the model\n",
    "#         # y_mod = np.zeros((nmc, len(X_mod))) # array to hold modeled Y values\n",
    "#         # m_mod = np.zeros(nmc) # linear fit coefficients\n",
    "#         # b_mod = np.zeros(nmc) # linear fit intercepts\n",
    "#         # y_mod_err = np.zeros(nmc) # array to hold error associated with each model\n",
    "#         # loop through Monte Carlo simulations\n",
    "#         # for j in np.arange(0,nmc):\n",
    "#             # split into training and testing data\n",
    "#         # X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=pTrain, shuffle=True)\n",
    "#         # fit ordinary squares to training data\n",
    "#         # fit = LinearRegression(positive=True).fit(X_train.reshape(-1, 1), y_train)\n",
    "#         fit = LinearRegression(positive=True).fit(X.reshape(-1, 1), y)\n",
    "#         # apply linear model to testing data\n",
    "#         y_pred = fit.predict(X.reshape(-1, 1))\n",
    "#         # calculate weights using the inverse of the residuals\n",
    "#         weights = np.zeros(len(y))\n",
    "#         for k, yy in enumerate(y):\n",
    "#             residual = np.abs(y_pred[k] - yy)\n",
    "#             if residual>0:\n",
    "#                 weights[k] = 1/(residual**2)\n",
    "#             else:\n",
    "#                 weights[k] = 1\n",
    "#         # fit weighted least squares to data\n",
    "#         fit_weighted = LinearRegression(positive=True).fit(X.reshape(-1, 1), y, sample_weight=weights)\n",
    "#         y_pred_weighted = fit_weighted.predict(X_mod.reshape(-1,1))\n",
    "#         # convert X back to dates\n",
    "#         X = [np.timedelta64(int(x), 'D')+day1 for x in X]\n",
    "#         X_mod = [np.timedelta64(int(x), 'D')+day1 for x in X_mod]\n",
    "#         ax[i].set_ylabel('Snowline elevation [m]')\n",
    "#         if (i==0) & (j==2):\n",
    "#             ax[i].plot(X, y_pred, '-r', linewidth=3, label='ordinary least-squares')\n",
    "#             ax[i].plot(X_mod, y_pred_weighted, '-b', linewidth=3, label='weighted least-squares')\n",
    "#             ax[i].legend(loc='upper left')\n",
    "#         else:     \n",
    "#             ax[i].plot(sl_est_full['datetime'], sl_est_full['snowlines_elevs_median'], \n",
    "#                        'ok', markersize=5, label='_nolegend_')\n",
    "#             ax[i].plot(X, y_pred, '-r', linewidth=3, label='_nolegend_')\n",
    "#             ax[i].plot(X_mod, y_pred_weighted, '-b', linewidth=3, label='_nolegend_')\n",
    "\n",
    "#         # create weights for each value using the residuals of the ordinary least squares\n",
    "#         # weights = [(\n",
    "#         # calculate mean error\n",
    "#         # y_mod_err[j] = np.sum(np.abs(y_test - y_pred)) / len(y_test)\n",
    "#         # # apply the model to the full X data\n",
    "#         # y_mod[j,:] = fit.predict(X_mod.reshape(-1, 1))\n",
    "#         # # store the coefficient and intercept\n",
    "#         # m_mod[j] = fit.coef_[0]\n",
    "#         # b_mod[j] = fit.intercept_\n",
    "\n",
    "#         # identify model with lowest error\n",
    "# #         Ibest = np.argmin(y_mod_err)\n",
    "# #         m_mod_best = m_mod[Ibest]\n",
    "# #         b_mod_best = b_mod[Ibest]\n",
    "# #         y_mod_best = [x*m_mod_best+b_mod_best for x in X_mod]\n",
    "# #         # convert X back to dates\n",
    "# #         X = [np.timedelta64(int(x), 'D')+day1 for x in X]\n",
    "# #         X_mod = [np.timedelta64(int(x), 'D')+day1 for x in X_mod]\n",
    "        \n",
    "# #         # save results in DataFrame\n",
    "# #         result_linear_model = pd.DataFrame({'year': year,\n",
    "# #                                'X': [X],\n",
    "# #                                'y': [y],\n",
    "# #                                'X_mod': [X_mod],\n",
    "# #                                'y_mod_best': [y_mod_best]\n",
    "# #                               })\n",
    "# #         results_linear_model = pd.concat([results_linear_model, result_linear_model])\n",
    "        \n",
    "#         # calculate the IQR and median models\n",
    "#         # m_mod_iqr, m_mod_median = iqr(m_mod), np.nanmedian(b_mod)\n",
    "#         # m_mod_P25, m_mod_P75 = m_mod_median - m_mod_iqr/2, m_mod_median + m_mod_iqr/2\n",
    "#         # b_mod_iqr, b_mod_median = iqr(b_mod), np.nanmedian(b_mod)\n",
    "#         # b_mod_P25, b_mod_P75 = b_mod_median - b_mod_iqr/2, b_mod_median + b_mod_iqr/2   \n",
    "#         # y_mod_median = [m_mod_median*x + b_mod_median for x in X_mod]\n",
    "#         # y_mod_P25 = [m_mod_P25*x + b_mod_P25 for x in X_mod]\n",
    "#         # y_mod_P75 = [m_mod_P75*x + b_mod_P75 for x in X_mod]\n",
    "#         # y_mod_iqr = iqr(y_mod, axis=0)\n",
    "#         # y_mod_median = np.nanmedian(y_mod, axis=0)\n",
    "#         # y_mod_P25 = y_mod_median - y_mod_iqr/2\n",
    "#         # y_mod_P75 = y_mod_median + y_mod_iqr/2\n",
    "        \n",
    "#         # plot results\n",
    "#         # ax[i].fill_between(X_mod, y_mod_P25, y_mod_P75, facecolor='blue', alpha=0.5, label='model$_{IQR}$')\n",
    "#         # ax[i].plot(X_mod, y_mod_median, '.-b', linewidth=1, label='model$_{median}$')\n",
    "#         # if i==1:\n",
    "#         #     ax.plot(X_mod, y_mod_best, '-b', linewidth=3, label='best linear model')\n",
    "#         # else:\n",
    "#         #     ax.plot(X_mod, y_mod_best, '-b', linewidth=3, label='_nolegend_')\n",
    "#         # ax.legend(loc='best')\n",
    "# plt.show()\n",
    "    \n",
    "# # save figure\n",
    "# out_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/figures/'\n",
    "# fig_fn = 'snowline_fitting.png'\n",
    "# fig.savefig(out_path+fig_fn, facecolor='w', dpi=300)\n",
    "# print('figure saved to file: '+out_path+fig_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe542602-4f7e-4684-9584-3bfb88ee4278",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### Filter observations using monthly median values ###\n",
    "# site_names = ['Wolverine', 'Gulkana', 'LemonCreek', 'SouthCascade', 'Sperry']\n",
    "\n",
    "# # -----loop through sites\n",
    "# for site_name in site_names:\n",
    "    \n",
    "#     print(site_name)\n",
    "#     print('----------')\n",
    "    \n",
    "#     # -----Load snowlines    \n",
    "#     sl_est_path = base_path + '../study-sites/' + site_name + '/imagery/snowlines/'\n",
    "#     sl_est_fns = glob.glob(sl_est_path + '*snowline.pkl')\n",
    "#     # compile all snowline files into one DataFrame\n",
    "#     sl_est_full = pd.DataFrame()\n",
    "#     for fn in sl_est_fns:\n",
    "#         sl_est = pd.read_pickle(fn) # read file\n",
    "#         sl_est_full = pd.concat([sl_est_full, sl_est]) # concatenate to df\n",
    "#     sl_est_full = sl_est_full.reset_index(drop=True).sort_values(by=['datetime']) # renumber, sort by date\n",
    "    \n",
    "#     # -----Reformat snowlines dataframes\n",
    "#     # unify datetime datatypes\n",
    "#     sl_est_full['datetime'] = sl_est_full['datetime'].astype(np.datetime64)\n",
    "#     # add year column\n",
    "#     sl_est_full['year'] = [x.year for x in sl_est_full['datetime']]\n",
    "#     # grab unique years\n",
    "#     years = np.unique(sl_est_full['year'])\n",
    "#     # set datetime as index\n",
    "#     sl_est_full.index = sl_est_full['datetime']\n",
    "\n",
    "#     # -----Set up figure\n",
    "#     fig, ax = plt.subplots(3, 1, figsize=(12,16))\n",
    "#     plt.rcParams.update({'font.size':12, 'font.sans-serif':'Arial'})\n",
    "#     ax[0].plot(sl_est_full.datetime, sl_est_full.snowlines_elevs_median, '.b')\n",
    "#     xmin, xmax = np.datetime64('2015-05-01'), np.datetime64('2022-11-01')\n",
    "#     ymin, ymax = sl_est_full.snowlines_elevs_median.min()-25, sl_est_full.snowlines_elevs_median.max()+25\n",
    "#     ax[0].set_title('Data')\n",
    "#     ax[0].set_ylabel('Median snowline elevations [m]')\n",
    "#     ax[0].set_xlim(xmin, xmax)\n",
    "#     ax[0].set_ylim(ymin, ymax)\n",
    "#     ax[0].grid()\n",
    "#     ax[1].set_title('|Data - monthly median trend|')\n",
    "#     ax[1].set_ylabel('Noise [m]')\n",
    "#     ax[1].set_xlim(xmin, xmax)\n",
    "#     ax[1].grid()\n",
    "#     ax[2].set_title('Filtered data')\n",
    "#     ax[2].set_ylabel('Median snowline elevations [m]')\n",
    "#     ax[2].set_xlim(xmin, xmax)\n",
    "#     ax[2].set_ylim(ymin, ymax)\n",
    "#     ax[2].grid()\n",
    "    \n",
    "#     # -----Filter data points\n",
    "#     # loop through years\n",
    "#     sl_est_full_filt = pd.DataFrame()\n",
    "#     months = np.arange(0,12)\n",
    "#     # monthly_medians = np.zeros(len(years)*len(months))\n",
    "#     weeks = np.arange(0,52)\n",
    "#     i=0 # loop counter\n",
    "#     for year in years:\n",
    "#         # subset df to year\n",
    "#         df_year = sl_est_full[sl_est_full['datetime'].dt.year == year]\n",
    "#         # add dummy values for Jan - April\n",
    "#         df_year_dummy = pd.DataFrame()\n",
    "#         df_year_dummy['datetime'] = np.arange(np.datetime64(str(year)+'-01-01'), \n",
    "#                                               np.datetime64(str(year)+'-05-01'), dtype='datetime64[D]')\n",
    "#         df_year_dummy['snowlines_elevs_median'] = sl_est_full['snowlines_elevs_median'].min()\n",
    "#         df_year = pd.concat([df_year, df_year_dummy]).sort_values(by=['datetime']).reset_index(drop=True)\n",
    "#         # add day of year column\n",
    "#         df_year['doy'] = [x.timetuple().tm_yday for x in df_year['datetime']]\n",
    "#         # calculate monthly median snowline elevations\n",
    "#         monthly_medians = [np.nanmedian(df_year[df_year['datetime'].dt.month==month].snowlines_elevs_median)\n",
    "#                           for month in months]\n",
    "#         # create array of months in the year\n",
    "#         dates_interp = np.arange(np.datetime64(str(year)+'-01'), \n",
    "#                                  np.datetime64(str(year+1)+'-01'), \n",
    "#                                  np.timedelta64(1, 'M'))\n",
    "#         # plot median trend\n",
    "#         if year==years[0]:\n",
    "#             ax[2].plot(dates_interp, monthly_medians, '.-k', label='monthly median trend') \n",
    "#         else:\n",
    "#             ax[2].plot(dates_interp, monthly_medians, '.-k', label='_nolegend_') \n",
    "#         # interpolate monthly median at all observation dates\n",
    "#         monthly_medians_interp = np.interp(df_year['doy'].values, # evaluate at all observation dates\n",
    "#                                            [pd.Timestamp(x.astype('datetime64[D]') + np.timedelta64(14,'D')).timetuple().tm_yday\n",
    "#                                             for x in dates_interp], # the 15th of every month\n",
    "#                                            monthly_medians) # monthly median values\n",
    "#         # calculate noise, i.e. the difference between each observation and the monthly median trend\n",
    "#         noise = np.abs(df_year['snowlines_elevs_median'].values - monthly_medians_interp)\n",
    "        \n",
    "#         noise_thresh = (sl_est_full['snowlines_elevs_median'].max() \n",
    "#                         - sl_est_full['snowlines_elevs_median'].min())*0.4 # [m]\n",
    "#         # plot noise threshold, noise, and data points to be filtered\n",
    "#         ax[1].plot([xmin, xmax], [noise_thresh, noise_thresh], '-k', linewidth=2)\n",
    "#         ax[1].plot(df_year['datetime'], noise, '.r') # plot noise\n",
    "#         if year==years[0]:\n",
    "#             ax[2].plot(df_year.iloc[noise >= noise_thresh]['datetime'],\n",
    "#                        df_year.iloc[noise >= noise_thresh]['snowlines_elevs_median'], 'xr', label='removed points')\n",
    "#         else:\n",
    "#             ax[2].plot(df_year.iloc[noise >= noise_thresh]['datetime'],\n",
    "#                        df_year.iloc[noise >= noise_thresh]['snowlines_elevs_median'], 'xr', label='_nolegend_')            \n",
    "#         # filter data points using noise threshold\n",
    "#         df_year.iloc[noise >= noise_thresh, :] = np.nan\n",
    "#         df_year = df_year.dropna() # remove rows with NaN\n",
    "#         # plot filtered data points\n",
    "#         if year==years[0]:\n",
    "#             ax[2].plot(df_year['datetime'], df_year['snowlines_elevs_median'], '.b', label='filtered time series')\n",
    "#             ax[2].legend()\n",
    "#         else:\n",
    "#             ax[2].plot(df_year['datetime'], df_year['snowlines_elevs_median'], '.b', label='_nolegend_')\n",
    "#         # concatenate to filtered df\n",
    "#         sl_est_full_filt = pd.concat([sl_est_full_filt, df_year])\n",
    "        \n",
    "        \n",
    "#     plt.show()\n",
    "    \n",
    "#     # save figure\n",
    "#     fig_fn = figures_out_path + 'filtered_snowline_timeseries_' + site_name + '.png'\n",
    "#     fig.savefig(fig_fn, dpi=300, facecolor='w')\n",
    "#     print('figure saved to file: ' + fig_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5f214b-2263-41e3-97f5-27fdb35171fc",
   "metadata": {},
   "source": [
    "### Stack observations by year, filter points using the monthly distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e875b2-744d-4899-a2f9-0892aca09e68",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Identify site names (used in folder names)\n",
    "site_names = ['Wolverine', 'Gulkana', 'LemonCreek', 'SouthCascade', 'Sperry']\n",
    "\n",
    "# -----Loop through sites\n",
    "for i, site_name in enumerate(site_names):\n",
    "    \n",
    "    print(site_name)\n",
    "    \n",
    "    # -----Load snowlines    \n",
    "    \n",
    "    ###########################\n",
    "    ### MODIFY IF NECESSARY ###\n",
    "    # path to snowline pkl files\n",
    "    sl_est_path = base_path + '../study-sites/' + site_name + '/imagery/snowlines/' \n",
    "    # path where filtered snowlines will be saved\n",
    "    out_path = sl_est_path \n",
    "    # path to USGS mass balance data / ELA csvs (if no USGS files, set usgs_path=None)\n",
    "    usgs_path = '/Volumes/GoogleDrive/My Drive/Research/PhD/GIS_data/USGS/benchmarkGlacier_massBalance/'\n",
    "    ###########################\n",
    "    \n",
    "    sl_est_fns = glob.glob(sl_est_path + '*snowline.pkl')\n",
    "    # compile all snowline files into one DataFrame\n",
    "    sl_est_full = pd.DataFrame()\n",
    "    for fn in sl_est_fns:\n",
    "        sl_est = pd.read_pickle(fn) # read file\n",
    "        sl_est_full = pd.concat([sl_est_full, sl_est]) # concatenate to df\n",
    "    sl_est_full = sl_est_full.reset_index(drop=True).sort_values(by=['datetime']) # renumber, sort by date\n",
    "    \n",
    "    # -----Reformat snowlines dataframes\n",
    "    # unify datetime datatypes\n",
    "    sl_est_full['datetime'] = sl_est_full['datetime'].astype(np.datetime64)\n",
    "    # add month column\n",
    "    sl_est_full['month'] = [x.month for x in sl_est_full['datetime']]\n",
    "    # extract all unique months\n",
    "    months = np.unique(sl_est_full['month'])  \n",
    "    # set datetime as index\n",
    "    sl_est_full.index = sl_est_full['datetime']\n",
    " \n",
    "    # ----Set up figure\n",
    "    fig = plt.figure(figsize=(20, 6))\n",
    "    plt.rcParams.update({'font.size':18, 'font.sans-serif':'Arial'})\n",
    "    spec = fig.add_gridspec(ncols=2, nrows=1, width_ratios=[1, 2.5])\n",
    "    # ax0 = fig.add_subplot(spec[0, 0])\n",
    "    # ax0.axis('off')\n",
    "    ax1 = fig.add_subplot(spec[0, 0])\n",
    "    ax1.set_xlabel('Month')\n",
    "    ax1.set_ylabel('Median snowline elevations [m]')\n",
    "    ax1.grid()\n",
    "    ax2 = fig.add_subplot(spec[0, 1])\n",
    "    ax2.set_xlabel('Date')\n",
    "    ax2.grid()\n",
    "\n",
    "    # -----Filter points using median and IQR trend\n",
    "    med = np.array([np.nanmedian(sl_est_full.loc[sl_est_full['month']==month]['snowlines_elevs_median']) for month in months])\n",
    "    std = np.array([np.nanstd(sl_est_full.loc[sl_est_full['month']==month]['snowlines_elevs_median']) for month in months])\n",
    "    MAD = np.array([mad(sl_est_full.loc[sl_est_full['month']==month]['snowlines_elevs_median'], nan_policy='omit') for month in months])\n",
    "    sl_est_full_filt = sl_est_full.copy() # filtered dataframe\n",
    "    n_filt = 0 # count number of filtered points\n",
    "    for j, month in enumerate(months):\n",
    "        Ifilt = np.ravel(np.argwhere((sl_est_full.loc[sl_est_full['month']==month]['snowlines_elevs_median'] > med[j]+std[j]*2).values |\n",
    "                        ((sl_est_full.loc[sl_est_full['month']==month]['snowlines_elevs_median'] < med[j]-std[j]).values)))\n",
    "        n_filt = n_filt + len(Ifilt)\n",
    "        if len(Ifilt)>0:\n",
    "            sl_est_full_filt = sl_est_full_filt.mask((sl_est_full_filt['month']==month) & \n",
    "                                                     (sl_est_full_filt['snowlines_elevs_median'] > med[j]+std[j]*2) |\n",
    "                                                     (sl_est_full_filt['snowlines_elevs_median'] < med[j]-std[j]) \n",
    "                                                    )\n",
    "        # removed points\n",
    "        ax1.plot(sl_est_full.loc[sl_est_full['month']==month].iloc[Ifilt]['month'], \n",
    "                 sl_est_full.loc[sl_est_full['month']==month].iloc[Ifilt]['snowlines_elevs_median'], \n",
    "                 'x', markersize=5, color='#969696', label='_nolegend_') \n",
    "        ax2.plot(sl_est_full.loc[sl_est_full['month']==month].iloc[Ifilt]['datetime'], \n",
    "                 sl_est_full.loc[sl_est_full['month']==month].iloc[Ifilt]['snowlines_elevs_median'], \n",
    "                 'x', markersize=5, color='#969696')\n",
    "   \n",
    "    # -----Determine annual ELAs\n",
    "    # add years column\n",
    "    sl_est_full_filt['year'] = [x.year for x in sl_est_full_filt['datetime']]\n",
    "    # grab all unique years\n",
    "    years = np.unique(sl_est_full_filt['year'].dropna())\n",
    "    # initialize dataframe for ELAs\n",
    "    ELAs_df = pd.DataFrame()\n",
    "    # loop through years, save maximum median snowline elevation and date of observation\n",
    "    for year in years:\n",
    "        sl_est_year = sl_est_full_filt.loc[sl_est_full_filt['year']==year]\n",
    "        ELA, dt = sl_est_year.loc[sl_est_year['snowlines_elevs_median']==np.max(sl_est_year['snowlines_elevs_median'])][['snowlines_elevs_median', 'datetime']].values[0]\n",
    "        df = pd.DataFrame({'ELA': ELA, \n",
    "                           'datetime':dt}, \n",
    "                          index=[0])\n",
    "        ELAs_df = pd.concat([ELAs_df, df])\n",
    "    ELAs_df = ELAs_df.reset_index(drop=True)\n",
    "    \n",
    "    # -----Plot\n",
    "    # range of acceptable values\n",
    "    ax1.fill_between(months, med-std, med+std*2, color='#4eb3d3', label='Acceptable range') \n",
    "    # monthly median\n",
    "    ax1.plot(months, med, '-b', label='Monthly median') \n",
    "    # filtered time series\n",
    "    ax1.plot(sl_est_full_filt['month'], sl_est_full_filt['snowlines_elevs_median'], '.k', markersize=10, label='Filtered time series') \n",
    "    ax2.plot(sl_est_full_filt['datetime'], sl_est_full_filt['snowlines_elevs_median'], '.k', markersize=10)\n",
    "    # ELAs\n",
    "    ax2.plot(ELAs_df['datetime'], ELAs_df['ELA'], 's', color='b', markersize=10)\n",
    "    # dummy points for legend\n",
    "    ax1.plot(0, 0, 'x', markersize=5, color='#969696', label='Removed points')\n",
    "    ax1.plot(0, 0,  's', color='b', markersize=10, label='ELA')\n",
    "    # optional: plot USGS ELA estimates\n",
    "    if usgs_path:\n",
    "        usgs_fn = usgs_path + site_name+'/Output_'+site_name+'_Glacier_Wide_solutions_calibrated.csv'\n",
    "        usgs_file = pd.read_csv(usgs_fn)\n",
    "        ELA = usgs_file['ELA']\n",
    "        ELA_date = usgs_file['Ba_Date'].astype(np.datetime64)\n",
    "        ax1.plot(0,0, 's', markerfacecolor='None', markeredgecolor='orange', \n",
    "                 ms=10, markeredgewidth=2, label='USGS ELA')\n",
    "        ax2.plot(ELA_date, ELA, 's', markerfacecolor='None', markeredgecolor='orange', \n",
    "                 ms=10, markeredgewidth=2, label='_nolegend_')\n",
    "    # set axis limits\n",
    "    ax1.set_xlim(np.min(months)-0.5, np.max(months)+0.5)\n",
    "    ax2.set_xlim(np.datetime64('2016-01-01'), np.datetime64('2023-01-01'))\n",
    "    ymin, ymax = np.min(med - std)-10, np.max(med + 2*std)+10\n",
    "    ax1.set_ylim(ymin, ymax)\n",
    "    ax2.set_ylim(ymin, ymax)\n",
    "    # add legend to figure\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center', ncol=len(labels))    \n",
    "    \n",
    "    plt.show()\n",
    "    print('Number of removed points = '+str(n_filt))\n",
    "    \n",
    "    # -----Save figure\n",
    "    fig_fn = figures_out_path + 'filtered_snowline_timeseries_' + site_name + '.png'\n",
    "    fig.savefig(fig_fn, dpi=300, facecolor='w')\n",
    "    print('figure saved to file: ' + fig_fn)\n",
    "    \n",
    "    # -----Save filtered snowline time series\n",
    "    sl_est_full_filt = sl_est_full_filt.dropna().drop(['datetime', 'month'], axis=1)\n",
    "    sl_fn = (site_name + '_snowlines_filtered_' + str(np.min(sl_est_full['datetime']))[0:10].replace('-','') \n",
    "             + '_' + str(np.max(sl_est_full['datetime']))[0:10].replace('-','') + '.pkl')\n",
    "    sl_est_full_filt.to_pickle(out_path + sl_fn)\n",
    "    print('filtered snowlines saved to file: ' + out_path + sl_fn)\n",
    "    \n",
    "    # -----Save ELA times series\n",
    "    ELAs_fn = (site_name + '_ELAs_' + str(years[0]) + '_' + str(years[-1]) + '.pkl')\n",
    "    ELAs_df.to_pickle(out_path + ELAs_fn)\n",
    "    print('ELAs saved to file: ' + out_path + ELAs_fn)\n",
    "    print(' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cc6624-b147-4e39-bcd0-bed09feadc38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planet-snow",
   "language": "python",
   "name": "planet-snow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
