{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca431dca",
   "metadata": {},
   "source": [
    "# Classify snow-covered area (SCA) in PlanetScope imagery: full pipeline\n",
    "\n",
    "Rainey Aberle\n",
    "\n",
    "Department of Geosciences, Boise State University\n",
    "\n",
    "2022\n",
    "\n",
    "### Requirements:\n",
    "- Planet account with access to PlanetScope imagery through the NASA CSDA contract. Sign up __[here](https://www.planet.com/markets/nasa/)__.\n",
    "- Area of Interest (AOI) shapefile: where snow will be classified in each image. \n",
    "- PlanetScope 4-band image collection over the AOI. Download images using `planetAPI_image_download.ipynb` or through __[PlanetExplorer](https://www.planet.com/explorer/)__. \n",
    "- Google Earth Engine (GEE) account: used to pull DEM over the AOI. Sign up for a free account __[here](https://earthengine.google.com/new_signup/)__. \n",
    "\n",
    "\n",
    "### Outline:\n",
    "__0. Setup__ paths in directory, AOI file location - _modify this section!_\n",
    "\n",
    "__1. Mosaic images__ captured in the same hour\n",
    "\n",
    "__2. Adjust image radiometry__ using median surface reflectance at the top or bottom perentile of elevations\n",
    "\n",
    "__3. Classify SCA__ and use the snow elevations distribution to estimate the seasonal snowline\n",
    "\n",
    "__4. Estimate snow line__ and snow line elevations\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4498b1ad",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "#### Define paths in directory, image file extensions, and desired settings. \n",
    "Modify lines located within the following:\n",
    "\n",
    "`#### MODIFY HERE ####`  \n",
    "\n",
    "`#####################`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf7591",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MODIFY HERE #####\n",
    "\n",
    "# -----Paths in directory\n",
    "site_name = 'Wolverine'\n",
    "# path to snow-cover-mapping\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/'\n",
    "# path to images\n",
    "im_path = base_path + '../study-sites/' + site_name + '/imagery/PlanetScope/2016-2022/'\n",
    "# path to AOI including the name of the shapefile\n",
    "AOI_fn = im_path + '../../../glacier_outlines/' + site_name + '_USGS_*.shp'\n",
    "# path to DEM including the name of the tif file\n",
    "# Note: set DEM_fn=None if you want to use the ASTER GDEM on Google Earth Engine\n",
    "DEM_fn = im_path + '../../../DEMs/' + site_name + '*_DEM_filled.tif'\n",
    "# path for output images\n",
    "out_path = im_path + '../'\n",
    "# path for output figures\n",
    "figures_out_path = im_path + '../../../figures/'\n",
    "\n",
    "# -----Determine settings\n",
    "plot_results = True # = True to plot figures of results for each image where applicable\n",
    "skip_clipped = False # = True to skip images where bands appear \"clipped\", i.e. max blue SR < 0.8\n",
    "crop_to_AOI = True # = True to crop images to AOI before calculating SCA\n",
    "save_outputs = True # = True to save SCA images to file\n",
    "save_figures = True # = True to save SCA output figures to file\n",
    "\n",
    "#######################\n",
    "\n",
    "# -----Import packages\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import rasterio as rio\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import sys\n",
    "import time\n",
    "import ee\n",
    "import pickle\n",
    "from time import mktime\n",
    "from matplotlib import pyplot as plt, dates\n",
    "import matplotlib\n",
    "from PIL import Image as PIL_Image\n",
    "from IPython.display import Image as IPy_Image\n",
    "\n",
    "# -----Add path to functions\n",
    "sys.path.insert(1, base_path+'functions/')\n",
    "import pipeline_utils_PlanetScope as pf\n",
    "import pipeline_utils_LSM as lf\n",
    "\n",
    "# -----Set paths for output files\n",
    "im_masked_path = out_path + 'masked/'\n",
    "im_mosaics_path = out_path + 'mosaics/'\n",
    "im_adjusted_path = out_path + 'adjusted/'\n",
    "im_classified_path = out_path + 'classified/'\n",
    "snowlines_path = out_path + 'snowlines/'\n",
    "\n",
    "# -----Load AOI as gpd.GeoDataFrame\n",
    "AOI_fn = glob.glob(AOI_fn)[0]\n",
    "AOI = gpd.read_file(AOI_fn)\n",
    "    \n",
    "# -----Load DEM as Xarray DataSet\n",
    "if DEM_fn==None:\n",
    "    \n",
    "    # Authenticate and initialize Google Earth Engine\n",
    "    # Note: The first time you run this, you will be asked to authenticate your GEE account \n",
    "    # for use in this notebook. This will send you to an external web page, where you will \n",
    "    # walk through the GEE authentication workflow and copy an authentication code back \n",
    "    # in this notebook when prompted. \n",
    "    try:\n",
    "        ee.Initialize()\n",
    "    except: \n",
    "        ee.Authenticate()\n",
    "        ee.Initialize()\n",
    "    # query GEE for DEM\n",
    "    DEM, AOI_UTM = lf.query_GEE_for_DEM(AOI)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # reproject the AOI to WGS to solve for the optimal UTM zone\n",
    "    AOI_WGS = AOI.to_crs(4326)\n",
    "    AOI_WGS_centroid = [AOI_WGS.geometry[0].centroid.xy[0][0],\n",
    "                        AOI_WGS.geometry[0].centroid.xy[1][0]]\n",
    "    epsg_UTM = lf.convert_wgs_to_utm(AOI_WGS_centroid[0], AOI_WGS_centroid[1])\n",
    "    # reproject AOI to UTM\n",
    "    AOI_UTM = AOI.to_crs(str(epsg_UTM))\n",
    "    # load DEM as xarray DataSet\n",
    "    DEM_fn = glob.glob(DEM_fn)[0]\n",
    "    DEM = xr.open_dataset(DEM_fn)\n",
    "    DEM = DEM.rename({'band_data': 'elevation'})\n",
    "    # reproject the DEM to the optimal UTM zone\n",
    "    DEM = DEM.rio.reproject(str('EPSG:'+epsg_UTM))\n",
    "    \n",
    "# -----Load dataset dictionary\n",
    "with open(base_path + 'inputs-outputs/datasets_characteristics.pkl', 'rb') as fn:\n",
    "    dataset_dict = pickle.load(fn)\n",
    "dataset = 'PlanetScope'\n",
    "ds_dict = dataset_dict[dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd66462",
   "metadata": {},
   "source": [
    "## 1. Mask image pixels with clouds, shadows, and heavy haze using associated Usable Data Mask (`udm`) files.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcf7a34",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Read surface reflectance file names\n",
    "os.chdir(im_path)\n",
    "im_fns = glob.glob('*SR*.tif')\n",
    "im_fns = sorted(im_fns) # sort chronologically\n",
    "\n",
    "# ----Mask images\n",
    "for im_fn in im_fns:\n",
    "    \n",
    "    print(im_fn)\n",
    "    plot_results=True\n",
    "    pf.mask_im_pixels(im_path, im_fn, im_masked_path, save_outputs, plot_results)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac31ac8",
   "metadata": {},
   "source": [
    "## 2. Mosaic images by date\n",
    "\n",
    "Mosaic all images captured within the same hour to increase area coverage of each image over the AOI. Images captured in different hours are more likely to have drastic variations in illumination. Adapted from code developed by [Jukes Liu](https://github.com/julialiu18). \n",
    "\n",
    "Note that images with no data over the AOI are skipped in this step. Issues with illumination or radiometry will be further filtered and adjusted in the next step.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b561fbd2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Read masked image file names\n",
    "os.chdir(im_masked_path)\n",
    "im_masked_fns = glob.glob('*_mask.tif')\n",
    "im_masked_fns = sorted(im_masked_fns) # sort chronologically\n",
    "\n",
    "# ----Mosaic images by date\n",
    "pf.mosaic_ims_by_date(im_masked_path, im_masked_fns, im_mosaics_path, AOI_UTM, plot_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe43ce57",
   "metadata": {},
   "source": [
    "## 3. Adjust image radiometry\n",
    "\n",
    "Mitigate issues related to varying illumination and radiometry issues. \n",
    "- Create polygon(s) representing the upper 25th and lower 25th percentile of elevations.\n",
    "- Determine whether there is snow on the surface based on the difference between the median surface reflectance in each polygon and the brightness magnitude. This will determine whether to use the predicted surface reflectance of ICE or SNOW to stretch the image in the following step. \n",
    "- Stretch the image using the predicted surface reflectance of ICE or SNOW and the median value in each polygon, assuming the median surface reflectance value within the polygon is equal to that predicted for ICE or SNOW, and that the darkest point in the image has a surface reflectance of 0. Images with no real data values within the AOI or in the polygon(s) will be skipped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab9fabc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Read mosaicked image file names\n",
    "os.chdir(im_mosaics_path)\n",
    "im_mosaic_fns = glob.glob('*.tif')\n",
    "im_mosaic_fns = sorted(im_mosaic_fns)\n",
    "\n",
    "# -----Create a polygon(s) of the top 20th percentile elevations within the AOI\n",
    "plot_results=False \n",
    "polygon_top, polygon_bottom, im_mosaic_fn, im_mosaic = pf.create_AOI_elev_polys(AOI_UTM, im_mosaics_path, im_mosaic_fns, DEM)\n",
    "# plot\n",
    "if plot_results:\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    ax.imshow(np.dstack([im_mosaic.data[2], im_mosaic.data[1], im_mosaic.data[0]]), \n",
    "               extent=(np.min(im_mosaic.x), np.max(im_mosaic.x), \n",
    "                       np.min(im_mosaic.y), np.max(im_mosaic.y)))\n",
    "    AOI_UTM.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=2, label='AOI')\n",
    "    for count, geom in enumerate(polygon_top.geoms):\n",
    "        xs, ys = geom.exterior.xy\n",
    "        if count==0:\n",
    "            ax.plot([x for x in xs], [y for y in ys], color='c', label='top polygon(s)')\n",
    "        else:\n",
    "            ax.plot([x for x in xs], [y for y in ys], color='c', label='_nolegend_')\n",
    "    for count, geom in enumerate(polygon_bottom.geoms):\n",
    "        xs, ys = geom.exterior.xy\n",
    "        if count==0:\n",
    "            ax.plot([x for x in xs], [y for y in ys], color='orange', label='bottom polygon(s)')\n",
    "        else:\n",
    "            ax.plot([x for x in xs], [y for y in ys], color='orange', label='_nolegend_')\n",
    "    ax.set_xlabel('Easting [m]')\n",
    "    ax.set_ylabel('Northing [m]')\n",
    "    ax.set_title(im_mosaic_fn)\n",
    "    fig.legend(loc='upper right')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# -----Loop through images\n",
    "for im_mosaic_fn in im_mosaic_fns:\n",
    "    \n",
    "    # load image\n",
    "    print(im_mosaic_fn)\n",
    "    # adjust radiometry\n",
    "    im_adjusted_fn, im_adj_method = pf.adjust_image_radiometry(im_mosaic_fn, im_mosaics_path, polygon_top, polygon_bottom, \n",
    "                                                               AOI_UTM, ds_dict, dataset, site_name, im_adjusted_path, \n",
    "                                                               skip_clipped, plot_results)\n",
    "    print('image adjustment method = ' + im_adj_method)\n",
    "    print('----------')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373fd74b",
   "metadata": {},
   "source": [
    "## 4. Classify images\n",
    "\n",
    "All adjusted images will be classified using the pre-trained classifier into the following classes:\n",
    "- 1 = Snow\n",
    "- 2 = Shadowed snow\n",
    "- 3 = Ice\n",
    "- 4 = Bare ground\n",
    "- 5 = Water\n",
    "\n",
    "The resulting classified image collection cropped to the AOI if `crop_to_AOI = True` and will be saved to the `im_classified_path` folder in directory if `save_outputs = True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c6c72b-4b07-42d8-aa41-6440d2607473",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Load trained classifier and feature columns\n",
    "clf_fn = base_path+'inputs-outputs/PS_classifier_all_sites.sav'\n",
    "clf = pickle.load(open(clf_fn, 'rb'))\n",
    "feature_cols_fn = base_path+'inputs-outputs/PS_feature_cols.pkl'\n",
    "feature_cols = pickle.load(open(feature_cols_fn,'rb'))\n",
    "\n",
    "# -----Read masked images\n",
    "# im_adjusted_fns = glob.glob(im_adjusted_path + '*_adj.nc')\n",
    "im_adjusted_fns = glob.glob(im_adjusted_path + '*_adj.nc')\n",
    "\n",
    "im_adjusted_fns = sorted(im_adjusted_fns) # sort chronologically\n",
    "\n",
    "# -----Loop through masked image files\n",
    "for im_adjusted_fn in im_adjusted_fns:\n",
    "    # load file\n",
    "    im_adjusted = xr.open_dataset(im_adjusted_fn)\n",
    "    # classify images\n",
    "    plot_results=True\n",
    "    im_classified = lf.classify_image(im_adjusted, clf, feature_cols, \n",
    "                                      crop_to_AOI, AOI, ds_dict, dataset, \n",
    "                                      site_name, im_classified_path, plot_results, \n",
    "                                      figures_out_path)\n",
    "    if plot_results:\n",
    "        plt.show()\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d50dba",
   "metadata": {},
   "source": [
    "## 5. Delineate snowlines and estimate snowline elevations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3711ca59-e18d-44c0-bfc0-f96b93dac6a0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Read image file names\n",
    "# adjusted images\n",
    "im_adjusted_fns = glob.glob(im_adjusted_path + '*_adj.nc')\n",
    "im_adjusted_fns = sorted(im_adjusted_fns) # sort chronologically\n",
    "# classified images\n",
    "im_classified_fns = glob.glob(im_classified_path + '*_classified.nc')\n",
    "im_classified_fns = sorted(im_classified_fns) # sort chronologically\n",
    "\n",
    "# -----Initialize snowlines data frame\n",
    "snowlines_df = pd.DataFrame(columns=['study_site', 'datetime', 'snowlines_coords', 'snowlines_elevs', 'snowlines_elevs_median'])\n",
    "    \n",
    "# -----Loop through classified images\n",
    "for im_classified_fn in im_classified_fns:\n",
    "        \n",
    "    # load classified file\n",
    "    im_classified = xr.open_dataset(im_classified_fn)\n",
    "    im_dt = im_classified_fn.split(site_name+'_')[1][0:15]\n",
    "    print(im_dt)\n",
    "    \n",
    "    # check if snowline exists in directory already\n",
    "    snowline_fn = dataset + '_' + site_name + '_' + im_dt + '_snowline.pkl'\n",
    "    if os.path.exists(os.path.join(snowlines_path, snowline_fn)):\n",
    "        print('snowline already exist in file, loading...')\n",
    "        snowline_df = pickle.load(open(snowlines_path + snowline_fn,'rb'))\n",
    "    else:\n",
    "        # load masked image file\n",
    "        im_adjusted_fn = [x for x in im_adjusted_fns if (im_dt in x)][0] \n",
    "        im_adjusted = xr.open_dataset(im_adjusted_fn)\n",
    "        # delineate snowline\n",
    "        snowline_df = lf.delineate_im_snowline(im_adjusted, im_classified, site_name, AOI_UTM, DEM, ds_dict, \n",
    "                                               dataset, im_dt, snowlines_path, figures_out_path, plot_results)\n",
    "        # save snowline to file\n",
    "        snowline_df.to_pickle(snowlines_path + snowline_fn)\n",
    "        print('snowline saved to file:' + snowlines_path + snowline_fn)\n",
    "    # concatenate results to snowlines_df\n",
    "    snowlines_df = pd.concat([snowlines_df, snowline_df])\n",
    "    print(' ')\n",
    "    \n",
    "# -----Save snowlines_df to file\n",
    "date_start = im_classified_fns[0].split(site_name+'_')[1][0:8]\n",
    "date_end = im_classified_fns[-1].split(site_name+'_')[1][0:8]\n",
    "snowlines_fn = dataset + '_' + site_name + '_' + date_start + '_' + date_end + '_snowlines.pkl'\n",
    "snowlines_df = snowlines_df.reset_index(drop=True)\n",
    "snowlines_df.to_pickle(snowlines_path + snowlines_fn)\n",
    "print('snowlines saved to file:' + snowlines_path + snowlines_fn)\n",
    "\n",
    "# -----Plot median snow line elevations\n",
    "if plot_results:\n",
    "    fig2, ax2 = plt.subplots(figsize=(10,6))\n",
    "    plt.rcParams.update({'font.size':12, 'font.sans-serif':'Arial'})\n",
    "    # plot snowlines\n",
    "    ax2.plot(snowlines_df['datetime'].astype(np.datetime64),\n",
    "             snowlines_df['snowlines_elevs_median'], '.b', markersize=10)\n",
    "    ax2.set_ylabel('Median snow line elevation [m]')\n",
    "    ax2.grid()\n",
    "    # format x-axis\n",
    "    xmin, xmax = np.datetime64('2016-05-01T00:00:00'), np.datetime64('2022-11-01T00:00:00')\n",
    "    fmt_month = matplotlib.dates.MonthLocator(bymonth=(5, 11)) # minor ticks every month.\n",
    "    fmt_year = matplotlib.dates.YearLocator() # minor ticks every year.\n",
    "    ax2.xaxis.set_minor_formatter(matplotlib.dates.DateFormatter('%b'))\n",
    "    ax2.xaxis.set_major_locator(fmt_month)\n",
    "    ax2.xaxis.set_major_formatter(matplotlib.dates.DateFormatter('%b'))\n",
    "    # create a second x-axis beneath the first x-axis to show the year in YYYY format\n",
    "    sec_xaxis = ax2.secondary_xaxis(-0.1)\n",
    "    sec_xaxis.xaxis.set_major_locator(fmt_year)\n",
    "    sec_xaxis.xaxis.set_major_formatter(matplotlib.dates.DateFormatter('%Y'))\n",
    "    # Hide the second x-axis spines and ticks\n",
    "    sec_xaxis.spines['bottom'].set_visible(False)\n",
    "    sec_xaxis.tick_params(axis='x', length=0, pad=-10)\n",
    "    fig2.suptitle(site_name + ' Glacier median snow line elevations')\n",
    "    fig2.tight_layout()\n",
    "    plt.show()\n",
    "    # save figure\n",
    "    fig2_fn = figures_out_path + dataset + '_' + site_name + '_' + date_start.replace('-','') + '_' + date_end.replace('-','')+ '_snowline_median_elevs.png'\n",
    "    fig2.savefig(fig2_fn, dpi=300, facecolor='white', edgecolor='none')\n",
    "    print('figure saved to file:' + fig2_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a68b512",
   "metadata": {},
   "source": [
    "### _Optional:_ Compile individual figures into a .gif and delete individual figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd886f40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----SCA figures\n",
    "os.chdir(figures_out_path)\n",
    "fig_fns = glob.glob('PlanetScope_*_snowline.png') # load all output figure file names\n",
    "if fig_fns:\n",
    "    fig_fns = sorted(fig_fns) # sort chronologically\n",
    "    # grab figures date range for .gif file name\n",
    "    fig_start_date = fig_fns[0][3:-8] # first figure date\n",
    "    fig_end_date = fig_fns[-1][3:-8] # final figure date\n",
    "    frames = [PIL_Image.open(im) for im in fig_fns]\n",
    "    frame_one = frames[0]\n",
    "    gif_fn = ('PlanetScope_' + fig_start_date[0:8] + '_' + fig_end_date[0:8] + '_snowlines.gif' )\n",
    "    frame_one.save(figures_out_path + gif_fn, format=\"GIF\", append_images=frames, save_all=True, duration=2000, loop=0)\n",
    "    print('GIF saved to file:' + figures_out_path + gif_fn)\n",
    "    # clean up: delete individual figure files\n",
    "    for fig_fn in fig_fns:\n",
    "        os.remove(fig_fn)\n",
    "    print('Individual figure files deleted.')\n",
    "    \n",
    "# -----Snowline figures\n",
    "fig_fns = glob.glob('PlanetScope_*_snowline.png') # load all output figure file names\n",
    "if fig_fns:\n",
    "    fig_fns = sorted(fig_fns) # sort chronologically\n",
    "    # grab figures date range for .gif file name\n",
    "    fig_start_date = fig_fns[0][3:-8] # first figure date\n",
    "    fig_end_date = fig_fns[-1][3:-8] # final figure date\n",
    "    frames = [PIL_Image.open(im) for im in fig_fns]\n",
    "    frame_one = frames[0]\n",
    "    gif_fn = ('PlanetScope_' + fig_start_date[0:8] + '_' + fig_end_date[0:8] + '_snowlines.gif' )\n",
    "    frame_one.save(figures_out_path + gif_fn, format=\"GIF\", append_images=frames, save_all=True, duration=2000, loop=0)\n",
    "    print('GIF saved to file:' + figures_out_path + gif_fn)\n",
    "    # clean up: delete individual figure files\n",
    "    for fig_fn in fig_fns:\n",
    "        os.remove(fig_fn)\n",
    "    print('Individual figure files deleted.')\n",
    "# display .gif\n",
    "IPy_Image(filename = figures_out_path + gif_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9de2be5",
   "metadata": {},
   "source": [
    "## 6. Fit Fourier series model to snowline time series\n",
    "\n",
    "Adapted from code developed by [Jukes Liu](https://github.com/CryoGARS-Glaciology/Fourier-terminus-models)\n",
    "\n",
    "This code fits time series of median snow line elevations using Fourier Series with the optimal number of terms (approximately the number of years in the time series) chosen using Monte Carlo simulations. 500 Fourier Series models are generated for each time series. The model IQR is calculated and exported to a new csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed07ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----Set up X and Y data for model fitting\n",
    "# # read snowlines file name\n",
    "# snowlines_path = im_path + '../snowlines_old/'\n",
    "# os.chdir(snowlines_path)\n",
    "# snowlines_fn = glob.glob('*_snowlines*.pkl')[0]\n",
    "# snowlines = pd.read_pickle(snowlines_fn)\n",
    "\n",
    "# # grab X and Y data from snowline dates and median elevations\n",
    "# datetimes = np.ravel(snowlines['datetime'])\n",
    "# snowlines_elevs_median= np.array(np.ravel(snowlines['snowlines_elevs_median']), dtype=float)\n",
    "# # convert datatimes to floats\n",
    "# X = np.array([mktime(x.timetuple()) for x in datetimes], dtype=float)\n",
    "# # remove NaNs\n",
    "# X = datetimes[~np.isnan(snowlines_elevs_median)]\n",
    "# Y = snowlines_elevs_median[~np.isnan(snowlines_elevs_median)]\n",
    "# # convert dates to days after the first image date capture\n",
    "# day1 = np.datetime64('2016-05-01')\n",
    "# X = np.array([pd.Timedelta(day - day1, 'D').total_seconds() / 86400 for day in X])\n",
    "# # grab number of years from snowline datetimes\n",
    "# # used to create the range of model terms for testing\n",
    "# nyears = snowlines['datetime'].iloc[-1].year - snowlines['datetime'].iloc[0].year \n",
    "\n",
    "# # display data for model fitting\n",
    "# print('Number of years detected in dataset: ' + str(nyears))\n",
    "# fig = plt.figure(figsize=(12, 6))\n",
    "# plt.plot(X, Y, '.k')\n",
    "# plt.xlabel('Days since '+str(day1))\n",
    "# plt.ylabel('Snowline elevation median [m]')\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e46a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Generate optimized fourier model for snowline timeseries\n",
    "# X_mod, Y_mod, Y_mod_err = pf.optimized_fourier_model(X, Y, nyears, plot_results=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cf22af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_mod = X_mod = np.linspace(X[0], X[-1], num=100)\n",
    "# for i in np.arange(np.shape(Y_mod)[0]):\n",
    "#     plt.plot(X_mod, Y_mod[i,:])\n",
    "# plt.plot(X, Y, '.k')\n",
    "# # plt.ylim(np.nanmin(Y)-100, np.nanmax(Y)+100) \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93fff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nmc = 500 # number of monte carlo simulations\n",
    "# # initialize coefficients data frame\n",
    "# cols = [val[0] for val in fit_best.params.items()]\n",
    "# X_mod = np.linspace(X[0], X[-1], num=100) # points at which to evaluate the model\n",
    "# Y_mod = np.zeros((nmc, len(X_mod))) # array to hold modeled Y values\n",
    "# Y_mod_err = np.zeros(nmc) # array to hold error associated with each model\n",
    "# print('Conducting Monte Carlo simulations to generate 500 Fourier models...')\n",
    "# # loop through Monte Carlo simulations\n",
    "# for i in np.arange(0,nmc):\n",
    "\n",
    "#     # split into training and testing data\n",
    "#     X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=pTrain, shuffle=True)\n",
    "\n",
    "#     # fit fourier model to training data\n",
    "#     fit = Fit({y: fourier_series_symb(x, f=w, n=fourier_n)},\n",
    "#                 x=X_train, y=Y_train).execute()\n",
    "\n",
    "#        print(str(i)+ ' '+ str(len(fit.params)))\n",
    "\n",
    "#     # apply fourier model to testing data\n",
    "#     Y_pred = fit.model(x=X_test, **fit.params).y\n",
    "\n",
    "#     # calculate mean error\n",
    "#     Y_mod_err[i] = np.sum(np.abs(Y_test - Y_pred)) / len(Y_test)\n",
    "\n",
    "#     # apply the model to the full X data\n",
    "#     c = [c[1] for c in fit.params.items()] # coefficient values\n",
    "#     Y_mod[i,:] = fourier_model(c, X_mod)\n",
    "\n",
    "# # plot results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planet-snow",
   "language": "python",
   "name": "planet-snow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
