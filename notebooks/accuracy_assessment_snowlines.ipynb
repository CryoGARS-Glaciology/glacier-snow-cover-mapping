{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a738ee4d-62e5-4e1d-aee5-68dc7f494617",
   "metadata": {},
   "source": [
    "# Snowline performance assessment\n",
    "\n",
    "Rainey Aberle\n",
    "\n",
    "2022/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d809a61d-618f-44f7-a82c-7946104a000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "from joblib import dump, load\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import rioxarray as rxr\n",
    "import rasterio as rio\n",
    "from scipy import stats\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point, MultiLineString, LineString, shape, MultiPolygon, Polygon\n",
    "from shapely.ops import split, unary_union, polygonize, nearest_points\n",
    "import skimage.io\n",
    "from skimage import feature\n",
    "import sys\n",
    "import wxee as wx\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcc578f-bec3-4e6f-930d-633b083db83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to snow-cover-mapping/\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/'\n",
    "# path to study-sites/\n",
    "study_sites_path = '/Users/raineyaberle/Google Drive/My Drive/Research/CryoGARS-Glaciology/Advising/student-research/Alexandra-Friel/snow_cover_mapping_application/study-sites/'\n",
    "# path to snowline-package/\n",
    "snowlines_obs_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/snow_cover_mapping/snowline-package/' \n",
    "\n",
    "# names of study sites\n",
    "site_names = ['Wolverine', 'Gulkana', 'LemonCreek', 'SouthCascade', 'Sperry']\n",
    "# path for output figures\n",
    "figures_out_path = base_path+'figures/'\n",
    "\n",
    "# add path to functions\n",
    "sys.path.insert(1, base_path+'functions/')\n",
    "import pipeline_utils as f\n",
    "\n",
    "# load dataset dictionary\n",
    "dataset_dict = json.load(open(base_path + 'inputs-outputs/datasets_characteristics.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2870334a-841b-4aa2-8faa-1e1178f97ffc",
   "metadata": {},
   "source": [
    "## PlanetScope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9853ff3d-0006-46c9-8f89-3d2a7ae6734d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Load trained classifier and feature columns\n",
    "clf_fn = base_path+'inputs-outputs/PlanetScope_classifier_all_sites.joblib'\n",
    "clf = load(clf_fn)\n",
    "feature_cols_fn = base_path+'inputs-outputs/PlanetScope_feature_columns.json'\n",
    "feature_cols = json.load(open(feature_cols_fn))\n",
    "dataset = 'PlanetScope'\n",
    "\n",
    "# -----Loop through sites\n",
    "results_df = pd.DataFrame()\n",
    "for i, site_name in enumerate(site_names):    \n",
    "\n",
    "    print(site_name)\n",
    "    print('----------')\n",
    "    \n",
    "    # define path to raw images\n",
    "    im_path = snowlines_obs_path + site_name + '/images/'\n",
    "\n",
    "    # load observed snow line shapefile names\n",
    "    sl_obs_fns = glob.glob(snowlines_obs_path + site_name + '/snowlines/*.shp')\n",
    "    sl_obs_fns = sorted(sl_obs_fns) # sort chronologically\n",
    "    \n",
    "    # AOI\n",
    "    AOI_fn = glob.glob(study_sites_path + site_name + '/AOIs/' + site_name + '_USGS_*.shp')[0]\n",
    "    AOI = gpd.read_file(AOI_fn)\n",
    "    \n",
    "    # DEM\n",
    "    DEM_fn = glob.glob(study_sites_path + site_name + '/DEMs/' + site_name + '*_clip.tif')[0]\n",
    "    DEM = xr.open_dataset(DEM_fn)\n",
    "    DEM = DEM.rename({'band_data': 'elevation'})\n",
    "    DEM = DEM.rio.reproject('EPSG:'+str(AOI.crs.to_epsg()))\n",
    "    # remove unnecessary data (possible extra bands from ArcticDEM or other DEM)\n",
    "    if len(np.shape(DEM.elevation.data))>2:\n",
    "        DEM['elevation'] = DEM.elevation[0]\n",
    "    \n",
    "    # define output folders for classified images and snowline estimates\n",
    "    im_classified_path = snowlines_obs_path + site_name + '/classified/'\n",
    "    snowlines_est_path = snowlines_obs_path + site_name + '/snowlines_est/'\n",
    "\n",
    "    # initialize observed snowline elevations\n",
    "    sl_obs_elevs = np.zeros(len(sl_obs_fns)) \n",
    "\n",
    "    # loop through observed snow lines\n",
    "    for sl_obs_fn in sl_obs_fns:\n",
    "\n",
    "        # -----Load datasets\n",
    "        ### Observed\n",
    "        sl_obs = gpd.read_file(sl_obs_fn)\n",
    "        # drop None geometry columns\n",
    "        sl_obs = sl_obs.drop(columns=['id']).dropna().reset_index(drop=True)\n",
    "        # reproject observed snow line to UTM\n",
    "        sl_obs_UTM = sl_obs.to_crs(str(AOI.crs.to_epsg()))\n",
    "        # extract date from filename\n",
    "        date = sl_obs_fn.split('/'+site_name+'_')[1][0:11]\n",
    "        datetime = np.datetime64(date[0:4]+ '-' + date[4:6] + '-' + date[6:8] + ' ' + date[9:11] + ':00:00')\n",
    "        print(date)\n",
    "        \n",
    "        ### Estimated      \n",
    "        # open raw image of the same date\n",
    "        im_fn = glob.glob(im_path + date[0:8] + '*_adj.tif')[0] # define file name\n",
    "        im = rxr.open_rasterio(im_fn) # open image as xarray.DataArray\n",
    "        # create xarray.Dataset\n",
    "        im_adj = xr.Dataset(\n",
    "            data_vars=dict(\n",
    "                Blue=(['y', 'x'], im.data[0]),\n",
    "                Green=(['y', 'x'], im.data[1]),\n",
    "                Red=(['y', 'x'], im.data[2]),\n",
    "                NIR=(['y', 'x'], im.data[3])\n",
    "            ),\n",
    "            coords=im.coords,\n",
    "            attrs=dict(\n",
    "                no_data_values=np.nan,\n",
    "                image_scalar=1\n",
    "            )\n",
    "        )\n",
    "        im_adj = xr.where(im_adj != 0, im_adj/1e4, np.nan)\n",
    "        im_adj = im_adj.rio.write_crs('EPSG:' + str(im.rio.crs.to_epsg()))\n",
    "        # add NDSI band\n",
    "        im_adj['NDSI'] = ((im_adj[dataset_dict[dataset]['NDSI_bands'][0]] - im_adj[dataset_dict[dataset]['NDSI_bands'][1]])\n",
    "                          / (im_adj[dataset_dict[dataset]['NDSI_bands'][0]] + im_adj[dataset_dict[dataset]['NDSI_bands'][1]]))\n",
    "        # add time dimension\n",
    "        im_adj = im_adj.expand_dims({'time': [datetime]})\n",
    "        # classify image\n",
    "        im_classified_fn = site_name + '_' + date + '_PlanetScope_classified.nc'\n",
    "        if os.path.exists(im_classified_path + im_classified_fn):\n",
    "            print('Classified image already exists in file, loading...')\n",
    "            im_classified = xr.open_dataset(im_classified_path + im_classified_fn)\n",
    "            # remove no data values\n",
    "            im_classified = xr.where(im_classified==-9999, np.nan, im_classified)\n",
    "        else:  \n",
    "            im_classified = f.classify_image(im_adj, clf, feature_cols, True, AOI, DEM, dataset_dict, dataset, \n",
    "                                             im_classified_fn, im_classified_path)\n",
    "        # delineate snowline\n",
    "        snowline_fn = site_name + '_' + date + '_PlanetScope_snowline.csv'\n",
    "        sl_est = f.delineate_image_snowline(im_adj, im_classified, site_name, AOI, dataset_dict, dataset, date, \n",
    "                                            snowline_fn, snowlines_est_path, figures_out_path, plot_results=False)\n",
    "               \n",
    "        # check if snowlines were found\n",
    "        if len(sl_est['snowlines_coords_X'][0]):\n",
    "            # -----Sample elevations at observed snowline points\n",
    "            xsamp = sl_obs_UTM.geometry[0].coords.xy[0]\n",
    "            ysamp = sl_obs_UTM.geometry[0].coords.xy[1]\n",
    "            sl_obs_elev = [DEM.sel(x=x, y=y, method='nearest')['elevation'].data for x,y in list(zip(xsamp, ysamp))]\n",
    "\n",
    "            # -----Split line depending on distance between points\n",
    "            max_dist = 100 # m\n",
    "            line = sl_obs_UTM.geometry[0]\n",
    "            first_point = Point(line.coords.xy[0][0], line.coords.xy[1][0])\n",
    "            points = [Point(line.coords.xy[0][i], line.coords.xy[1][i]) for i in np.arange(0,len(line.coords.xy[0]))]\n",
    "            isplit = [0] # point indices where to split the line\n",
    "            for i, p in enumerate(points):\n",
    "                if i!=0:\n",
    "                    dist = p.distance(points[i-1])\n",
    "                    if dist > max_dist:\n",
    "                        isplit.append(i)\n",
    "            isplit.append(len(points)) # add ending point to complete the last line\n",
    "            line_split = [] # initialize split lines\n",
    "            # loop through split indices\n",
    "            if isplit:\n",
    "                for i, p in enumerate(isplit[:-1]):\n",
    "                    if isplit[i+1]-isplit[i] > 1: # must have at least two points to make a line\n",
    "                        line_split = line_split + [LineString(points[isplit[i]:isplit[i+1]])]\n",
    "            else:\n",
    "                line_split = line\n",
    "    \n",
    "            # -----Regrid the observed snowlines to equal spacing\n",
    "            dx = 30 # point spacing\n",
    "            points_regrid = []\n",
    "            for line in line_split:\n",
    "                distances = np.arange(0, line.length, dx)\n",
    "                line_points = [line.interpolate(distance) for distance in distances] + [first_point]\n",
    "                # filter points outside the AOI\n",
    "                IAOI = np.where(np.array([p.within(AOI.geometry[0]) for p in line_points], dtype=int) ==1)[0]\n",
    "                points_AOI = [line_points[i] for i in IAOI]\n",
    "                points_regrid = points_regrid + [p for p in points_AOI]\n",
    "\n",
    "            # -----Calculate distance between each observed snowline point and the closest estimated snowline point\n",
    "            distances = np.zeros(len(points_regrid))\n",
    "            for i, p in enumerate(points_regrid):\n",
    "                # find nearest point\n",
    "                nearest_point = nearest_points(sl_est['geometry'][0], p)[0]\n",
    "                # calculate distance between points\n",
    "                distances[i] = p.distance(nearest_point)\n",
    "            \n",
    "            # -----Display results\n",
    "            # plt.figure(figsize=(8, 8))\n",
    "            # plt.imshow(np.dstack([im_adj['Red'].data[0], im_adj['Green'].data[0], im_adj['Blue'].data[0]]), \n",
    "            #            extent=(np.min(im_adj.x.data), np.max(im_adj.x.data), np.min(im_adj.y.data), np.max(im_adj.y.data)))\n",
    "            # plt.plot([p.coords.xy[0][0] for p in points_regrid], \n",
    "            #          [p.coords.xy[1][0] for p in points_regrid], '.c', label='observed')\n",
    "            # plt.plot(sl_est['snowlines_coords_X'][0], sl_est['snowlines_coords_Y'][0], '.m', label='estimated')\n",
    "            # plt.legend(loc='upper right')\n",
    "            # plt.grid()\n",
    "            # plt.title(datetime)\n",
    "            # plt.show()\n",
    "\n",
    "            # compile results in df\n",
    "            result_df = pd.DataFrame({'study_site': site_name, \n",
    "                                      'datetime': datetime, \n",
    "                                      'snowline_obs': [points_regrid], \n",
    "                                      'snowline_obs_elev_median': np.nanmedian(sl_obs_elev),\n",
    "                                      'snowline_est': [sl_est['geometry'][0]], \n",
    "                                      'snowline_est_elev_median': sl_est['snowlines_elevs_median_m'],\n",
    "                                      'snowline_elev_median_differences': sl_est['snowlines_elevs_median_m'] - np.nanmedian(sl_obs_elev),\n",
    "                                      'snowline_distances': [distances],\n",
    "                                      'snowline_distance_median': np.nanmedian(distances)})\n",
    "\n",
    "            # concatenate to results_df\n",
    "            results_df = pd.concat([results_df, result_df])\n",
    "            \n",
    "    print(' ')\n",
    "            \n",
    "# -----Save to file\n",
    "results_fn = base_path + 'inputs-outputs/snowline_performance_PlanetScope.csv'\n",
    "results_df.to_csv(results_fn, index=False)\n",
    "print('Performance metrics saved to file: '+results_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2462620d-1e7d-44e0-9557-e851f872506a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10,6))\n",
    "ax[0].boxplot(results_df['snowline_distance_median'])\n",
    "ax[0].set_title('snowline_distance_median')\n",
    "ax[1].boxplot(results_df['snowline_elev_median_differences'])\n",
    "ax[1].set_title('Median snowline elevation differences')\n",
    "plt.show()\n",
    "\n",
    "# compile stats in dataframe\n",
    "results_stats_df = pd.DataFrame({'dataset':['PlanetScope'],\n",
    "                                 'ground distance median [m]': np.nanmedian(results_df['snowline_distance_median']),\n",
    "                                 'ground distance IQR [m]': stats.iqr(results_df['snowline_distance_median']),\n",
    "                                 'elevation difference median [m]': np.nanmedian(results_df['snowline_elev_median_differences']),\n",
    "                                 'elevation difference IQR [m]': stats.iqr(results_df['snowline_elev_median_differences']),\n",
    "                                 'N': len(results_df)\n",
    "                                })\n",
    "\n",
    "# print results\n",
    "print('PlanetScope snowline performance')\n",
    "print('----------')\n",
    "print('Ground distance = ' + str(np.round(results_stats_df['ground distance median [m]'][0],2)) \n",
    "      + ' +/- ' + str(np.round(results_stats_df['ground distance IQR [m]'][0],2)) + ' m')\n",
    "print('Median elevation difference = ' + str(np.round(results_stats_df['elevation difference median [m]'][0],2)) \n",
    "      + ' +/- ' + str(np.round(results_stats_df['elevation difference IQR [m]'][0],2)) + ' m')\n",
    "\n",
    "# save to file\n",
    "results_stats_fn = base_path + 'inputs-outputs/snowline_performance_stats_PlanetScope.csv'\n",
    "results_stats_df.to_csv(results_stats_fn, index=False)\n",
    "print('Performance metrics saved to file: '+results_stats_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe641225-4ca9-4c8d-a6c1-e1aa16a0793b",
   "metadata": {},
   "source": [
    "## Landsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b465145-6729-4af8-84ba-857c5f48a3b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Loop through sites\n",
    "results_df = pd.DataFrame()\n",
    "for i, site_name in enumerate(site_names):    \n",
    "\n",
    "    print(site_name)\n",
    "    print('----------')\n",
    "    \n",
    "    # load digitized snow lines file names\n",
    "    sl_obs_path = snowlines_obs_path + site_name + '/snowlines/'\n",
    "    sl_obs_fns = sorted(glob.glob(sl_obs_path + '*.shp'))\n",
    "\n",
    "    # define estimated snowlines dates\n",
    "    if site_name == 'Wolverine':\n",
    "        sl_est_dates = ['N/A', 'N/A', 'N/A', '20170919',\n",
    "                        'N/A', '20180828', 'N/A', '20180929',\n",
    "                        '20190628', 'N/A', '20190808', 'N/A',\n",
    "                        '20200614', 'N/A', '20200817', '20200911',\n",
    "                        'N/A', '20210719', '20210813', 'N/A']\n",
    "    elif site_name == 'Gulkana':\n",
    "        sl_est_dates = ['N/A', 'N/A', 'N/A', 'N/A','N/A',\n",
    "                        '20180704', 'N/A', '20180913',\n",
    "                        'N/A', '20190707', '20190808', 'N/A',\n",
    "                        'N/A', 'N/A', '20200819', 'N/A',\n",
    "                        'N/A', 'N/A', '20210712']\n",
    "    elif site_name == 'LemonCreek':\n",
    "        sl_est_dates = ['N/A',\n",
    "                        '20180603', '20180619', '20180705', 'N/A', '20180721',\n",
    "                        '20190521', '20190606', 'N/A', 'N/A', 'N/A',\n",
    "                        '20200905',\n",
    "                        '20210729', 'N/A', '20210823']\n",
    "    elif site_name == 'SouthCascade':\n",
    "        sl_est_dates = ['N/A', '20160913',\n",
    "                        'N/A', 'N/A', 'N/A', 'N/A', 'N/A',\n",
    "                        '20180726', 'N/A', 'N/A', '20180928',\n",
    "                        '20190720', 'N/A', 'N/A', \n",
    "                        'N/A', '20200816', '20200908', '20201003',\n",
    "                        '20210810', 'N/A']\n",
    "    elif site_name == 'Sperry':\n",
    "        sl_est_dates = ['N/A', 'N/A', \n",
    "                        '20170727', 'N/A', '20170828', 'N/A',\n",
    "                        '20180730', '20180815', 'N/A',\n",
    "                        'N/A', 'N/A', 'N/A', 'N/A',\n",
    "                        'N/A', 'N/A', 'N/A', '20201007',\n",
    "                        'N/A', '20210722']\n",
    "    \n",
    "    # load AOI\n",
    "    AOI_fn = glob.glob(study_sites_path + site_name + '/AOIs/' + site_name + '_USGS_*.shp')[0]\n",
    "    AOI = gpd.read_file(AOI_fn)\n",
    "    \n",
    "    # load DEM\n",
    "    DEM_fn = glob.glob(study_sites_path + site_name + '/DEMs/' + site_name + '*USGS_DEM*.tif')[0]\n",
    "    DEM = xr.open_dataset(DEM_fn)\n",
    "    DEM = DEM.rename({'band_data': 'elevation'})\n",
    "\n",
    "    # -----Loop through snowline pairs\n",
    "    for sl_obs_fn, sl_est_date in list(zip(sl_obs_fns, sl_est_dates)):\n",
    "        \n",
    "        # if observed image date exists for comparison\n",
    "        print(sl_obs_fn.split('/')[-1], sl_est_date)\n",
    "\n",
    "        if sl_est_date != 'N/A':\n",
    "            \n",
    "            # -----Load snowlines\n",
    "            ### Observed\n",
    "            sl_obs = gpd.read_file(sl_obs_fn)\n",
    "            # drop None geometry columns\n",
    "            sl_obs = sl_obs.drop(columns=['id']).dropna().reset_index(drop=True)\n",
    "            # reproject observed snow line to UTM\n",
    "            sl_obs_UTM = sl_obs.to_crs('EPSG:'+str(AOI.crs.to_epsg()))\n",
    "            ### Estimated\n",
    "            sl_est_fn = glob.glob(study_sites_path + site_name + '/imagery/snowlines/*'+sl_est_date+'*Landsat*.csv')[0]\n",
    "            sl_est = pd.read_csv(sl_est_fn)\n",
    "            # continue if there is a snowline estimate...\n",
    "            if sl_est['geometry'][0] != '[]':\n",
    "                \n",
    "                sl_est['geometry'] = gpd.GeoSeries.from_wkt(sl_est['geometry'])\n",
    "            \n",
    "                # -----Sample elevations at observed snowline points\n",
    "                xsamp = sl_obs_UTM.geometry[0].coords.xy[0]\n",
    "                ysamp = sl_obs_UTM.geometry[0].coords.xy[1]\n",
    "                sl_obs_elev = [DEM.sel(x=x, y=y, method='nearest')['elevation'].data[0] for x,y in list(zip(xsamp, ysamp))]\n",
    "                \n",
    "                # -----Split line depending on distance between points\n",
    "                max_dist = 100 # m\n",
    "                line = sl_obs_UTM.geometry[0]\n",
    "                first_point = Point(line.coords.xy[0][0], line.coords.xy[1][0])\n",
    "                points = [Point(line.coords.xy[0][i], line.coords.xy[1][i]) for i in np.arange(0,len(line.coords.xy[0]))]\n",
    "                isplit = [0] # point indices where to split the line\n",
    "                for i, p in enumerate(points):\n",
    "                    if i!=0:\n",
    "                        dist = p.distance(points[i-1])\n",
    "                        if dist > max_dist:\n",
    "                            isplit.append(i)\n",
    "                isplit.append(len(points)) # add ending point to complete the last line\n",
    "                line_split = [] # initialize split lines\n",
    "                # loop through split indices\n",
    "                if isplit:\n",
    "                    for i, p in enumerate(isplit[:-1]):\n",
    "                        if isplit[i+1]-isplit[i] > 1: # must have at least two points to make a line\n",
    "                            line_split = line_split + [LineString(points[isplit[i]:isplit[i+1]])]\n",
    "                else:\n",
    "                    line_split = line\n",
    "                    \n",
    "                # -----Regrid the observed snowlines to equal spacing\n",
    "                dx = 30 # point spacing\n",
    "                points_regrid = []\n",
    "                for line in line_split:\n",
    "                    distances = np.arange(0, line.length, dx)\n",
    "                    line_points = [line.interpolate(distance) for distance in distances] + [first_point]\n",
    "                    # filter points outside the AOI\n",
    "                    IAOI = np.where(np.array([p.within(AOI.geometry[0]) for p in line_points], dtype=int) ==1)[0]\n",
    "                    points_AOI = [line_points[i] for i in IAOI]\n",
    "                    points_regrid = points_regrid + [p for p in points_AOI]\n",
    "                \n",
    "                # -----Calculate distance between each observed snowline point and the closest estimated snowline point\n",
    "                distances = np.zeros(len(points_regrid))\n",
    "                for i, p in enumerate(points_regrid):\n",
    "                    # find nearest point\n",
    "                    nearest_point = nearest_points(sl_est['geometry'][0], p)[0]\n",
    "                    # calculate distance between points\n",
    "                    distances[i] = p.distance(nearest_point)\n",
    "            \n",
    "                # -----Display results\n",
    "                # plt.figure(figsize=(8, 8))\n",
    "                # plt.plot([p.coords.xy[0][0] for p in points_regrid], \n",
    "                #          [p.coords.xy[1][0] for p in points_regrid], '.c', label='observed')\n",
    "                # plt.plot(*sl_est['geometry'][0].coords.xy, '.m', label='estimated')\n",
    "                # plt.legend(loc='upper right')\n",
    "                # plt.grid()\n",
    "                # plt.title(sl_est_date)\n",
    "                # plt.show()\n",
    "\n",
    "                # compile results in df\n",
    "                result_df = pd.DataFrame({'study_site': site_name, \n",
    "                                          'snowline_obs_date': sl_obs_fn.split('/')[-1].split(site_name+'_')[-1][0:8], \n",
    "                                          'snowline_est_date': sl_est_date,\n",
    "                                          'snowline_obs': [points_regrid], \n",
    "                                          'snowline_obs_elev_median': np.nanmedian(sl_obs_elev),\n",
    "                                          'snowline_est': [sl_est['geometry']], \n",
    "                                          'snowline_est_elev_median': sl_est['snowline_elevs_median_m'],\n",
    "                                          'snowline_elev_median_differences': sl_est['snowline_elevs_median_m'] - np.nanmedian(sl_obs_elev),\n",
    "                                          'snowline_distances': [distances],\n",
    "                                          'snowline_distance_median': np.nanmedian(distances)})\n",
    "                # concatenate to results_df\n",
    "                results_df = pd.concat([results_df, result_df])\n",
    "            \n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a864f6e-0480-4ee8-883e-4ea21a0cc9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10,6))\n",
    "ax[0].boxplot(results_df['snowline_distance_median'])\n",
    "ax[0].set_title('snowline_distance_median')\n",
    "ax[1].boxplot(results_df.dropna()['snowline_elev_median_differences'])\n",
    "ax[1].set_title('snowline_elev_median_differences')\n",
    "plt.show()\n",
    "\n",
    "# compile stats in dataframe\n",
    "results_stats_df = pd.DataFrame({'dataset':['Landsat'],\n",
    "                                 'ground distance median [m]': np.nanmedian(results_df['snowline_distance_median']),\n",
    "                                 'ground distance IQR [m]': stats.iqr(results_df['snowline_distance_median'], nan_policy='omit'),\n",
    "                                 'elevation difference median [m]': np.nanmedian(results_df['snowline_elev_median_differences']),\n",
    "                                 'elevation difference IQR [m]': stats.iqr(results_df['snowline_elev_median_differences'], nan_policy='omit'),\n",
    "                                 'N': len(results_df)\n",
    "                                })\n",
    "\n",
    "# print results\n",
    "print('Landsat snowline performance')\n",
    "print('----------')\n",
    "print('Ground distance = ' + str(np.round(results_stats_df['ground distance median [m]'][0],2)) \n",
    "      + ' +/- ' + str(np.round(results_stats_df['ground distance IQR [m]'][0],2)) + ' m')\n",
    "print('Median elevation difference = ' + str(np.round(results_stats_df['elevation difference median [m]'][0],2)) \n",
    "      + ' +/- ' + str(np.round(results_stats_df['elevation difference IQR [m]'][0],2)) + ' m')\n",
    "\n",
    "# save to file\n",
    "results_stats_fn = base_path + 'inputs-outputs/snowline_performance_stats_Landsat.csv'\n",
    "results_stats_df.to_csv(results_stats_fn, index=False)\n",
    "print('Performance metrics saved to file: '+results_stats_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6746a297-3053-4595-89de-f6e60e6d8cf7",
   "metadata": {},
   "source": [
    "## Sentinel-2 SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a227f17-27e8-4c0b-a6a1-d9c6b5a92d7b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Loop through sites\n",
    "results_df = pd.DataFrame()\n",
    "for i, site_name in enumerate(site_names):    \n",
    "\n",
    "    print(site_name)\n",
    "    print('----------')\n",
    "    \n",
    "    # load digitized snow lines file names\n",
    "    sl_obs_path = snowlines_obs_path + site_name + '/snowlines/'\n",
    "    sl_obs_fns = sorted(glob.glob(sl_obs_path + '*.shp'))\n",
    "    # remove pre-2019 observations\n",
    "    sl_obs_fns = [fn for fn in sl_obs_fns if ('2016' not in fn) & ('2017' not in fn) & ('2018' not in fn)]\n",
    "\n",
    "    # define estimated snowlines dates\n",
    "    if site_name=='Wolverine':\n",
    "        sl_est_dates = ['20190626', '20190704', '20190808', '20190818',\n",
    "                        '20200613', '20200723', '20200817', '20200911',\n",
    "                        '20210613', '20210715', 'N/A', '20210911']\n",
    "    elif site_name=='Gulkana':\n",
    "        sl_est_dates = ['20190621', '20190709', '20190808', '20190830',\n",
    "                        '20200703', '20200804', '20200819', '20200906',\n",
    "                        '20210620', '20210705', '20210713']\n",
    "    elif site_name=='LemonCreek':\n",
    "        sl_est_dates = ['20190527', '20190606', '20190628', '20190706', '20190723',\n",
    "                        '20200905',\n",
    "                        '20210730', '20210730', '20210821']\n",
    "    elif site_name=='SouthCascade':\n",
    "        sl_est_dates = ['N/A', 'N/A', 'N/A', \n",
    "                        '20200731', '20200818', '20200909', '20201004', \n",
    "                        '20210813', '20210828']\n",
    "    elif site_name=='Sperry':\n",
    "        sl_est_dates = ['20190723', '20190731', '20190802', '20190807',\n",
    "                        '20200727', '20200821', '20200905', '20201008', \n",
    "                        '20210712', '20210725']\n",
    "    \n",
    "    # load AOI\n",
    "    AOI_fn = glob.glob(study_sites_path + site_name + '/AOIs/' + site_name + '_USGS_*.shp')[0]\n",
    "    AOI = gpd.read_file(AOI_fn)\n",
    "    \n",
    "    # load DEM\n",
    "    DEM_fn = glob.glob(study_sites_path + site_name + '/DEMs/' + site_name + '*USGS_DEM*.tif')[0]\n",
    "    DEM = xr.open_dataset(DEM_fn)\n",
    "    DEM = DEM.rename({'band_data': 'elevation'})\n",
    "\n",
    "    # -----Loop through snowline pairs\n",
    "    for sl_obs_fn, sl_est_date in list(zip(sl_obs_fns, sl_est_dates)):\n",
    "        \n",
    "        # if observed image date exists for comparison\n",
    "        if sl_est_date != 'N/A':\n",
    "            \n",
    "            print(sl_obs_fn.split('/')[-1], sl_est_date)\n",
    "\n",
    "            # -----Load snowlines\n",
    "            ### Observed\n",
    "            sl_obs = gpd.read_file(sl_obs_fn)\n",
    "            # drop None geometry columns\n",
    "            sl_obs = sl_obs.drop(columns=['id']).dropna().reset_index(drop=True)\n",
    "            # reproject observed snow line to UTM\n",
    "            sl_obs_UTM = sl_obs.to_crs('EPSG:'+str(AOI.crs.to_epsg()))\n",
    "            ### Estimated\n",
    "            sl_est_fn = glob.glob(study_sites_path + site_name + '/imagery/snowlines/*'+sl_est_date+'*Sentinel-2_SR*.csv')[0]\n",
    "            sl_est = pd.read_csv(sl_est_fn)\n",
    "            # continue if there is a snowline estimate...\n",
    "            if sl_est['geometry'][0] != '[]':\n",
    "                \n",
    "                sl_est['geometry'] = gpd.GeoSeries.from_wkt(sl_est['geometry'])\n",
    "            \n",
    "                # -----Sample elevations at observed snowline points\n",
    "                xsamp = sl_obs_UTM.geometry[0].coords.xy[0]\n",
    "                ysamp = sl_obs_UTM.geometry[0].coords.xy[1]\n",
    "                sl_obs_elev = [DEM.sel(x=x, y=y, method='nearest')['elevation'].data[0] for x,y in list(zip(xsamp, ysamp))]\n",
    "                \n",
    "                # -----Split line depending on distance between points\n",
    "                max_dist = 100 # m\n",
    "                line = sl_obs_UTM.geometry[0]\n",
    "                first_point = Point(line.coords.xy[0][0], line.coords.xy[1][0])\n",
    "                points = [Point(line.coords.xy[0][i], line.coords.xy[1][i]) for i in np.arange(0,len(line.coords.xy[0]))]\n",
    "                isplit = [0] # point indices where to split the line\n",
    "                for i, p in enumerate(points):\n",
    "                    if i!=0:\n",
    "                        dist = p.distance(points[i-1])\n",
    "                        if dist > max_dist:\n",
    "                            isplit.append(i)\n",
    "                isplit.append(len(points)) # add ending point to complete the last line\n",
    "                line_split = [] # initialize split lines\n",
    "                # loop through split indices\n",
    "                if isplit:\n",
    "                    for i, p in enumerate(isplit[:-1]):\n",
    "                        if isplit[i+1]-isplit[i] > 1: # must have at least two points to make a line\n",
    "                            line_split = line_split + [LineString(points[isplit[i]:isplit[i+1]])]\n",
    "                else:\n",
    "                    line_split = line\n",
    "                    \n",
    "                # -----Regrid the observed snowlines to equal spacing\n",
    "                dx = 30 # point spacing\n",
    "                points_regrid = []\n",
    "                for line in line_split:\n",
    "                    distances = np.arange(0, line.length, dx)\n",
    "                    line_points = [line.interpolate(distance) for distance in distances] + [first_point]\n",
    "                    # filter points outside the AOI\n",
    "                    IAOI = np.where(np.array([p.within(AOI.geometry[0]) for p in line_points], dtype=int) ==1)[0]\n",
    "                    points_AOI = [line_points[i] for i in IAOI]\n",
    "                    points_regrid = points_regrid + [p for p in points_AOI]\n",
    "                \n",
    "                # -----Calculate distance between each observed snowline point and the closest estimated snowline point\n",
    "                distances = np.zeros(len(points_regrid))\n",
    "                for i, p in enumerate(points_regrid):\n",
    "                    # find nearest point\n",
    "                    nearest_point = nearest_points(sl_est['geometry'][0], p)[0]\n",
    "                    # calculate distance between points\n",
    "                    distances[i] = p.distance(nearest_point)\n",
    "            \n",
    "                # -----Display results\n",
    "                # plt.figure(figsize=(8, 8))\n",
    "                # plt.plot([p.coords.xy[0][0] for p in points_regrid], \n",
    "                #          [p.coords.xy[1][0] for p in points_regrid], '.c', label='observed')\n",
    "                # plt.plot(*sl_est['geometry'][0].coords.xy, '.m', label='estimated')\n",
    "                # plt.legend(loc='upper right')\n",
    "                # plt.grid()\n",
    "                # plt.title(sl_est_date)\n",
    "                # plt.show()\n",
    "\n",
    "                # compile results in df\n",
    "                result_df = pd.DataFrame({'study_site': site_name, \n",
    "                                          'snowline_obs_date': sl_obs_fn.split('/')[-1].split(site_name+'_')[-1][0:8], \n",
    "                                          'snowline_est_date': sl_est_date,\n",
    "                                          'snowline_obs': [points_regrid], \n",
    "                                          'snowline_obs_elev_median': np.nanmedian(sl_obs_elev),\n",
    "                                          'snowline_est': [sl_est['geometry']], \n",
    "                                          'snowline_est_elev_median': sl_est['snowline_elevs_median_m'],\n",
    "                                          'snowline_elev_median_differences': sl_est['snowline_elevs_median_m'] - np.nanmedian(sl_obs_elev),\n",
    "                                          'snowline_distances': [distances],\n",
    "                                          'snowline_distance_median': np.nanmedian(distances)})\n",
    "                # concatenate to results_df\n",
    "                results_df = pd.concat([results_df, result_df])\n",
    "            \n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a598e62-39fb-47e5-a746-0bc1dcb88005",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10,6))\n",
    "ax[0].boxplot(results_df['snowline_distance_median'])\n",
    "ax[0].set_title('snowline_distance_median')\n",
    "ax[1].boxplot(results_df.dropna()['snowline_elev_median_differences'])\n",
    "ax[1].set_title('snowline_elev_median_differences')\n",
    "plt.show()\n",
    "\n",
    "# compile stats in dataframe\n",
    "results_stats_df = pd.DataFrame({'dataset':['Sentinel-2_SR'],\n",
    "                                 'ground distance median [m]': np.nanmedian(results_df['snowline_distance_median']),\n",
    "                                 'ground distance IQR [m]': stats.iqr(results_df['snowline_distance_median'], nan_policy='omit'),\n",
    "                                 'elevation difference median [m]': np.nanmedian(results_df['snowline_elev_median_differences']),\n",
    "                                 'elevation difference IQR [m]': stats.iqr(results_df['snowline_elev_median_differences'], nan_policy='omit'),\n",
    "                                 'N': len(results_df)\n",
    "                                })\n",
    "\n",
    "# print results\n",
    "print('Sentinel-2 SR snowline performance')\n",
    "print('----------')\n",
    "print('Ground distance = ' + str(np.round(results_stats_df['ground distance median [m]'][0],2)) \n",
    "      + ' +/- ' + str(np.round(results_stats_df['ground distance IQR [m]'][0],2)) + ' m')\n",
    "print('Median elevation difference = ' + str(np.round(results_stats_df['elevation difference median [m]'][0],2)) \n",
    "      + ' +/- ' + str(np.round(results_stats_df['elevation difference IQR [m]'][0],2)) + ' m')\n",
    "\n",
    "# save to file\n",
    "results_stats_fn = base_path + 'inputs-outputs/snowline_performance_stats_Sentinel-2_SR.csv'\n",
    "results_stats_df.to_csv(results_stats_fn, index=False)\n",
    "print('Performance metrics saved to file: '+results_stats_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab980c1-34ef-45f7-a064-1345a1517da4",
   "metadata": {},
   "source": [
    "## Sentinel-2 TOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb1b80-c11b-4f62-86b0-06393f206cd2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Loop through sites\n",
    "results_df = pd.DataFrame()\n",
    "for i, site_name in enumerate(site_names):    \n",
    "\n",
    "    print(site_name)\n",
    "    print('----------')\n",
    "    \n",
    "    # load digitized snow lines file names\n",
    "    sl_obs_path = snowlines_obs_path + site_name + '/snowlines/'\n",
    "    sl_obs_fns = sorted(glob.glob(sl_obs_path + '*.shp'))\n",
    "\n",
    "    # define estimated snowlines dates\n",
    "    if site_name=='Wolverine':\n",
    "        sl_est_dates = ['N/A', 'N/A', 'N/A', 'N/A',\n",
    "                        '20180731', '20180828', '20180902', '20180929',\n",
    "                        '20190626', '20190704', '20190805', '20190818',\n",
    "                        '20200613', '20200723', '20200817', 'N/A',\n",
    "                        '20210613', '20210715', 'N/A', '20210911']\n",
    "    elif site_name=='Gulkana':\n",
    "        sl_est_dates = ['20170621', 'N/A', '20170805', 'N/A',\n",
    "                        '20180704', '20180721', '20180909',\n",
    "                        '20190621', '20190709', '20190808', '20190830',\n",
    "                        '20200703', '20200804', '20200819', '20200906',\n",
    "                        '20210620', '20210705', '20210713']\n",
    "    elif site_name=='LemonCreek':\n",
    "        sl_est_dates = ['20170810', \n",
    "                        '20180603', '20180618', '20180703', '20180713', '20180723',\n",
    "                        '20190527', '20190606', '20190628', '20190706', '20190723',\n",
    "                        '20200905',\n",
    "                        '20210730', '20210730', '20210821']\n",
    "    elif site_name=='SouthCascade':\n",
    "        sl_est_dates = ['20160816', 'N/A',\n",
    "                        'N/A', '20170811', '20170826', '20170915', '20171005',\n",
    "                        '20180730', '20180809', '20180905', '20180928',\n",
    "                        '20190722', '20190730', '20190819', \n",
    "                        '20200731', '20200818', '20200909', '20201004',\n",
    "                        '20210813', '20210828']\n",
    "    elif site_name=='Sperry':\n",
    "        sl_est_dates = ['20160817', '20160830',\n",
    "                        '20170731', '20170807', '20170825', '20170901',\n",
    "                        '20180731', '20180815', 'N/A',\n",
    "                        '20190723', '20190731', '20190802', '20190807']\n",
    "    \n",
    "    # load AOI\n",
    "    AOI_fn = glob.glob(study_sites_path + site_name + '/AOIs/' + site_name + '_USGS_*.shp')[0]\n",
    "    AOI = gpd.read_file(AOI_fn)\n",
    "    \n",
    "    # load DEM\n",
    "    DEM_fn = glob.glob(study_sites_path + site_name + '/DEMs/' + site_name + '*USGS_DEM*.tif')[0]\n",
    "    DEM = xr.open_dataset(DEM_fn)\n",
    "    DEM = DEM.rename({'band_data': 'elevation'})\n",
    "\n",
    "    # -----Loop through snowline pairs\n",
    "    for sl_obs_fn, sl_est_date in list(zip(sl_obs_fns, sl_est_dates)):\n",
    "        \n",
    "        # if observed image date exists for comparison\n",
    "        print(sl_obs_fn.split('/')[-1], sl_est_date)\n",
    "        if sl_est_date != 'N/A':\n",
    "                        \n",
    "            # -----Load snowlines\n",
    "            ### Observed\n",
    "            sl_obs = gpd.read_file(sl_obs_fn)\n",
    "            # drop None geometry columns\n",
    "            sl_obs = sl_obs.drop(columns=['id']).dropna().reset_index(drop=True)\n",
    "            # reproject observed snow line to UTM\n",
    "            sl_obs_UTM = sl_obs.to_crs('EPSG:'+str(AOI.crs.to_epsg()))\n",
    "            ### Estimated\n",
    "            sl_est_fn = glob.glob(study_sites_path + site_name + '/imagery/snowlines/*'+sl_est_date+'*Sentinel-2_TOA*.csv')[0]\n",
    "            sl_est = pd.read_csv(sl_est_fn)\n",
    "            # continue if there is a snowline estimate...\n",
    "            if sl_est['geometry'][0] != '[]':\n",
    "                \n",
    "                sl_est['geometry'] = gpd.GeoSeries.from_wkt(sl_est['geometry'])\n",
    "            \n",
    "                # -----Sample elevations at observed snowline points\n",
    "                xsamp = sl_obs_UTM.geometry[0].coords.xy[0]\n",
    "                ysamp = sl_obs_UTM.geometry[0].coords.xy[1]\n",
    "                sl_obs_elev = [DEM.sel(x=x, y=y, method='nearest')['elevation'].data[0] for x,y in list(zip(xsamp, ysamp))]\n",
    "                \n",
    "                # -----Split line depending on distance between points\n",
    "                max_dist = 100 # m\n",
    "                line = sl_obs_UTM.geometry[0]\n",
    "                first_point = Point(line.coords.xy[0][0], line.coords.xy[1][0])\n",
    "                points = [Point(line.coords.xy[0][i], line.coords.xy[1][i]) for i in np.arange(0,len(line.coords.xy[0]))]\n",
    "                isplit = [0] # point indices where to split the line\n",
    "                for i, p in enumerate(points):\n",
    "                    if i!=0:\n",
    "                        dist = p.distance(points[i-1])\n",
    "                        if dist > max_dist:\n",
    "                            isplit.append(i)\n",
    "                isplit.append(len(points)) # add ending point to complete the last line\n",
    "                line_split = [] # initialize split lines\n",
    "                # loop through split indices\n",
    "                if isplit:\n",
    "                    for i, p in enumerate(isplit[:-1]):\n",
    "                        if isplit[i+1]-isplit[i] > 1: # must have at least two points to make a line\n",
    "                            line_split = line_split + [LineString(points[isplit[i]:isplit[i+1]])]\n",
    "                else:\n",
    "                    line_split = line\n",
    "                    \n",
    "                # -----Regrid the observed snowlines to equal spacing\n",
    "                dx = 30 # point spacing\n",
    "                points_regrid = []\n",
    "                for line in line_split:\n",
    "                    distances = np.arange(0, line.length, dx)\n",
    "                    line_points = [line.interpolate(distance) for distance in distances] + [first_point]\n",
    "                    # filter points outside the AOI\n",
    "                    IAOI = np.where(np.array([p.within(AOI.geometry[0]) for p in line_points], dtype=int) ==1)[0]\n",
    "                    points_AOI = [line_points[i] for i in IAOI]\n",
    "                    points_regrid = points_regrid + [p for p in points_AOI]\n",
    "                \n",
    "                # -----Calculate distance between each observed snowline point and the closest estimated snowline point\n",
    "                distances = np.zeros(len(points_regrid))\n",
    "                for i, p in enumerate(points_regrid):\n",
    "                    # find nearest point\n",
    "                    nearest_point = nearest_points(sl_est['geometry'][0], p)[0]\n",
    "                    # calculate distance between points\n",
    "                    distances[i] = p.distance(nearest_point)\n",
    "            \n",
    "                # -----Display results\n",
    "                # plt.figure(figsize=(8, 8))\n",
    "                # plt.plot([p.coords.xy[0][0] for p in points_regrid], \n",
    "                #          [p.coords.xy[1][0] for p in points_regrid], '.c', label='observed')\n",
    "                # plt.plot(*sl_est['geometry'][0].coords.xy, '.m', label='estimated')\n",
    "                # plt.legend(loc='upper right')\n",
    "                # plt.grid()\n",
    "                # plt.title(sl_est_date)\n",
    "                # plt.show()\n",
    "\n",
    "                # compile results in df\n",
    "                result_df = pd.DataFrame({'study_site': site_name, \n",
    "                                          'snowline_obs_date': sl_obs_fn.split('/')[-1].split(site_name+'_')[-1][0:8], \n",
    "                                          'snowline_est_date': sl_est_date,\n",
    "                                          'snowline_obs': [points_regrid], \n",
    "                                          'snowline_obs_elev_median': np.nanmedian(sl_obs_elev),\n",
    "                                          'snowline_est': [sl_est['geometry']], \n",
    "                                          'snowline_est_elev_median': sl_est['snowline_elevs_median_m'],\n",
    "                                          'snowline_elev_median_differences': sl_est['snowline_elevs_median_m'] - np.nanmedian(sl_obs_elev),\n",
    "                                          'snowline_distances': [distances],\n",
    "                                          'snowline_distance_median': np.nanmedian(distances)})\n",
    "                # concatenate to results_df\n",
    "                results_df = pd.concat([results_df, result_df])\n",
    "            \n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d084ea9-8b68-4637-9920-e810fcd77555",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10,6))\n",
    "ax[0].boxplot(results_df['snowline_distance_median'])\n",
    "ax[0].set_title('snowline_distance_median')\n",
    "ax[1].boxplot(results_df.dropna()['snowline_elev_median_differences'])\n",
    "ax[1].set_title('snowline_elev_median_differences')\n",
    "plt.show()\n",
    "\n",
    "# compile stats in dataframe\n",
    "results_stats_df = pd.DataFrame({'dataset':['Sentinel-2_TOA'],\n",
    "                                 'ground distance median [m]': np.nanmedian(results_df['snowline_distance_median']),\n",
    "                                 'ground distance IQR [m]': stats.iqr(results_df['snowline_distance_median'], nan_policy='omit'),\n",
    "                                 'elevation difference median [m]': np.nanmedian(results_df['snowline_elev_median_differences']),\n",
    "                                 'elevation difference IQR [m]': stats.iqr(results_df['snowline_elev_median_differences'], nan_policy='omit'),\n",
    "                                 'N': len(results_df)\n",
    "                                })\n",
    "\n",
    "# print results\n",
    "print('Sentinel-2 TOA snowline performance')\n",
    "print('----------')\n",
    "print('Ground distance = ' + str(np.round(results_stats_df['ground distance median [m]'][0],2)) \n",
    "      + ' +/- ' + str(np.round(results_stats_df['ground distance IQR [m]'][0],2)) + ' m')\n",
    "print('Median elevation difference = ' + str(np.round(results_stats_df['elevation difference median [m]'][0],2)) \n",
    "      + ' +/- ' + str(np.round(results_stats_df['elevation difference IQR [m]'][0],2)) + ' m')\n",
    "\n",
    "# save to file\n",
    "results_stats_fn = base_path + 'inputs-outputs/snowline_performance_stats_Sentinel-2_TOA.csv'\n",
    "results_stats_df.to_csv(results_stats_fn, index=False)\n",
    "print('Performance metrics saved to file: '+results_stats_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81575359-18cc-4320-8db2-23207ce5a431",
   "metadata": {},
   "source": [
    "## Compile all stats tables into one CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28022f71-313b-443f-83e5-1361ba8ad459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab stats file names\n",
    "fns = sorted(glob.glob(base_path + 'inputs-outputs/snowline_performance_stats_*.csv'))\n",
    "\n",
    "# initialize dataframe for all files\n",
    "results_full = pd.DataFrame()\n",
    "\n",
    "# loop through files\n",
    "for fn in fns:\n",
    "    # open file\n",
    "    results = pd.read_csv(fn)\n",
    "    # concatenate to full dataframe\n",
    "    results_full = pd.concat([results_full, results])\n",
    "    \n",
    "# add column for average metrics\n",
    "results = pd.DataFrame({'dataset': 'All datasets AVERAGE',\n",
    "                        'ground distance median [m]': np.nanmean(results_full['ground distance median [m]']),\n",
    "                        'ground distance IQR [m]': np.nanmean(results_full['ground distance IQR [m]']),\n",
    "                        'elevation difference median [m]': np.nanmean(results_full['elevation difference median [m]']),\n",
    "                        'elevation difference IQR [m]': np.nanmean(results_full['elevation difference IQR [m]']),\n",
    "                        'N': np.sum(results_full['N'])\n",
    "                       }, index=[5])\n",
    "results_full = pd.concat([results_full, results])\n",
    "                        \n",
    "    \n",
    "# save full dataframe to file\n",
    "results_full_fn = 'snowline_performance_stats.csv'\n",
    "results_full.to_csv(base_path + 'inputs-outputs/' + results_full_fn, index=False)\n",
    "print('stats for all datasets compiled and saved: ' + base_path + 'inputs-outputs/' + results_full_fn)\n",
    "\n",
    "# delete individual files\n",
    "for fn in fns:\n",
    "    os.remove(fn)\n",
    "    print('file deleted: '+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d5b4b4-7245-45b3-be61-e61988e0ed7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
