{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c53b019d",
   "metadata": {},
   "source": [
    "## Notebook to develop supervised classification algorithm for identifying snow in PlanetScope 4-band imagery\n",
    "Rainey Aberle\n",
    "\n",
    "Adapted from the [SciKit Learn Classifier comparison tutorial](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n",
    "\n",
    "### Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3837b29f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -----Import packages\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import ee\n",
    "import richdem as rd\n",
    "import scipy\n",
    "from shapely.geometry import Polygon\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import time\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ed447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Determine whether to save outputs to file\n",
    "save_outputs = False # = True to save output figures and best classifier \n",
    "\n",
    "# -----Define paths in directory\n",
    "# base directory (path to planet-snow/)\n",
    "base_path = '/Users/raineyaberle/Research/PhD/Planet_snow_cover/planet-snow/'\n",
    "# output folder for best classifier\n",
    "out_path = base_path+'inputs-outputs/'\n",
    "# path to classified points used to train and test classifiers\n",
    "data_pts_path = base_path+'../classified-points/'\n",
    "\n",
    "# -----Determine whether to use terrain parameters in classification\n",
    "terrain_predictors = True\n",
    "\n",
    "# -----Add path to functions\n",
    "sys.path.insert(1, base_path+'functions/')\n",
    "from classification_utils import crop_images_to_AOI, classify_image, calculate_SCA, query_GEE_for_DEM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eff3b2",
   "metadata": {},
   "source": [
    "### Authenticate and initialize Google Earth Engine (GEE)\n",
    "\n",
    "__Note:__ The first time you run the following cell, you will be asked to authenticate your GEE account for use in this notebook. This will send you to an external web page, where you will walk through the GEE authentication workflow and copy an authentication code back in this notebook when prompted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a360631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ee.Initialize()\n",
    "except: \n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb69ae3f",
   "metadata": {},
   "source": [
    "### Define supervised classification algorithms to test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b920160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Classifier names\n",
    "names = [\n",
    "#     \"Gaussian Process\", # keeps crashing kernel when classifying images!\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "    \"Logistic Regression\"\n",
    "]\n",
    "\n",
    "# -----Classifiers\n",
    "classifiers = [\n",
    "#     GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    LogisticRegression(random_state = 0)\n",
    "]\n",
    "\n",
    "# -----Classified points\n",
    "os.chdir(data_pts_path)\n",
    "data_pts_fns = glob.glob('*.shp')\n",
    "data_pts_fns.sort()\n",
    "data_pts_fns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3640589b",
   "metadata": {},
   "source": [
    "### Test site-specific classifiers\n",
    "\n",
    "Use classified points at each site to determine the best site-specific classifier, calculate accuracy\n",
    "\n",
    "Classified points split: 80% training, 20% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdb0425",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -----Define site names\n",
    "site_names = ['Wolverine', 'SitKusa', 'Mendenhall', 'Easton', 'Blue', 'Emmons']\n",
    "# site_names = ['Wolverine']#, 'Gulkana', 'SCascade', 'Sperry']\n",
    "\n",
    "# -----Initialize data_pts_full to save info for single classifier (next step)\n",
    "data_pts_full = gpd.GeoDataFrame()\n",
    "\n",
    "# -----Loop through sites\n",
    "for i in range(len(site_names)):\n",
    "    \n",
    "    print(site_names[i])\n",
    "\n",
    "    # -----Set up training data\n",
    "    # determine number of images used for classified points\n",
    "    num_images = len([s for s in data_pts_fns if (site_names[i] in s) and ('snow.shp' in s)])\n",
    "    # loop through each image\n",
    "    for j in range(0, num_images):\n",
    "        # load classified points\n",
    "        data_pts = pd.DataFrame() # dataframe to hold applicable data classes\n",
    "        # snow\n",
    "        if len([s for s in data_pts_fns if (site_names[i] in s) and ('snow.shp' in s)])>0: # check if class exists for site\n",
    "            data_pts_snow_fn = [s for s in data_pts_fns if (site_names[i] in s) and ('snow.shp' in s)][j]\n",
    "            data_pts_snow = gpd.read_file(data_pts_path + data_pts_snow_fn) # read file\n",
    "            data_pts_snow['class'] = 1 # determine class ID\n",
    "            data_pts = pd.concat([data_pts, data_pts_snow], ignore_index=True) # concatenate to full data points df\n",
    "            print(data_pts_snow_fn)\n",
    "        # shadowed snow\n",
    "        if len([s for s in data_pts_fns if (site_names[i] in s) and ('snow-shadowed.shp' in s)])>0: # check if class exists for site\n",
    "            data_pts_snow_sh_fn = [s for s in data_pts_fns if (site_names[i] in s) and ('snow-shadowed.shp' in s)][j]\n",
    "            data_pts_snow_sh = gpd.read_file(data_pts_path + data_pts_snow_sh_fn) # read file\n",
    "            data_pts_snow_sh['class'] = 2 # determine class ID\n",
    "            data_pts = pd.concat([data_pts, data_pts_snow_sh], ignore_index=True) # concatenate to full data points df\n",
    "            print(data_pts_snow_sh_fn)\n",
    "        # ice\n",
    "        if len([s for s in data_pts_fns if (site_names[i] in s) and ('ice.shp' in s)])>0: # check if class exists for site\n",
    "            data_pts_ice_fn = [s for s in data_pts_fns if (site_names[i] in s) and ('ice.shp' in s)][j]\n",
    "            data_pts_ice = gpd.read_file(data_pts_path + data_pts_ice_fn)  # read file\n",
    "            data_pts_ice['class'] = 3 # determine class ID\n",
    "            data_pts = pd.concat([data_pts, data_pts_ice], ignore_index=True) # concatenate to full data points df\n",
    "            print(data_pts_ice_fn)\n",
    "        # rock\n",
    "        if len([s for s in data_pts_fns if (site_names[i] in s) and ('rock.shp' in s)])>0: # check if class exists for site\n",
    "            data_pts_rock_fn = [s for s in data_pts_fns if (site_names[i] in s) and ('rock.shp' in s)][j]\n",
    "            data_pts_rock = gpd.read_file(data_pts_path + data_pts_rock_fn) # read file\n",
    "            data_pts_rock['class'] = 4 # determine class ID\n",
    "            data_pts = pd.concat([data_pts, data_pts_rock], ignore_index=True) # concatenate to full data points df\n",
    "            print(data_pts_rock_fn)\n",
    "        # water\n",
    "        if len([s for s in data_pts_fns if (site_names[i] in s) and ('water.shp' in s)])>0: # check if class exists for site\n",
    "            data_pts_water_fn = [s for s in data_pts_fns if (site_names[i] in s) and ('water.shp' in s)][j]\n",
    "            data_pts_water = gpd.read_file(data_pts_path + data_pts_water_fn) # read file\n",
    "            data_pts_water['class'] = 5 # determine class ID\n",
    "            data_pts = pd.concat([data_pts, data_pts_water], ignore_index=True) # concatenate to full data points df\n",
    "            print(data_pts_water_fn)\n",
    "    \n",
    "        # -----Load image\n",
    "        im_path = base_path+'../study-sites/'+site_names[i]+'/imagery/PlanetScope/adjusted-filtered/'\n",
    "        Idate = data_pts_snow_fn.index('_')+1\n",
    "        im_fn = data_pts_snow_fn[Idate:Idate+11]+'_adj.tif' # image file name\n",
    "        im_date = im_fn[0:4]+'-'+im_fn[4:6]+'-'+im_fn[6:8] # image capture date\n",
    "        moy = float(im_fn[4:6]) # month of year\n",
    "        im = rio.open(im_path+im_fn) # open image\n",
    "        epsg = int(str(im.crs)[5:]) # grab EPSG code\n",
    "        # read bands\n",
    "        b = im.read(1) \n",
    "        r = im.read(2) \n",
    "        g = im.read(3)\n",
    "        nir = im.read(4)\n",
    "        # divide by image scalar if max band values > 1000\n",
    "        apply_scalar = False\n",
    "#         if np.nanmax(b) > 1e3:\n",
    "#             apply_scalar = True\n",
    "#             im_scalar = 10000\n",
    "#             b = b / im_scalar\n",
    "#             g = g / im_scalar\n",
    "#             r = r / im_scalar\n",
    "#             nir = nir / im_scalar\n",
    "#         else:\n",
    "#             apply_scalar = False\n",
    "        # calculate NDSI\n",
    "        ndsi = (r - nir) / (r + nir)\n",
    "        # define coordinates grid\n",
    "        im_x = np.linspace(im.bounds.left, im.bounds.right, num=np.shape(b)[1])\n",
    "        im_y = np.linspace(im.bounds.top, im.bounds.bottom, num=np.shape(b)[0])\n",
    "    \n",
    "        # -----Reformat data points coordinates\n",
    "        # reproject to image epsg\n",
    "        data_pts = data_pts.to_crs(epsg) \n",
    "        # remove \"id\" column\n",
    "        data_pts = data_pts.drop(columns=['id'])\n",
    "        # remove rows containing NaN\n",
    "        data_pts = data_pts.dropna()\n",
    "        data_pts = data_pts.reset_index()\n",
    "        # add coords column\n",
    "        data_pts['coords'] = [(pt.bounds[0], pt.bounds[1]) for pt in data_pts['geometry']]\n",
    "        # add site_name column\n",
    "        data_pts['site_name'] = site_names[i]\n",
    "        # add month-of-year (moy) column\n",
    "        data_pts['moy'] = moy\n",
    "        # sample band values at points\n",
    "        data_pts['blue'] = [x[0] for x in im.sample(data_pts['coords'])]\n",
    "        data_pts['green'] = [x[1] for x in im.sample(data_pts['coords'])]\n",
    "        data_pts['red'] = [x[2] for x in im.sample(data_pts['coords'])]\n",
    "        data_pts['NIR'] = [x[3] for x in im.sample(data_pts['coords'])]\n",
    "        # divide values by im_scalar if applicable\n",
    "        if apply_scalar:\n",
    "            data_pts['blue'] = data_pts['blue'].div(im_scalar)\n",
    "            data_pts['green'] = data_pts['green'].div(im_scalar)\n",
    "            data_pts['red'] = data_pts['red'].div(im_scalar)\n",
    "            data_pts['NIR'] = data_pts['NIR'].div(im_scalar)\n",
    "        # add NDSI column\n",
    "        data_pts['NDSI'] = (data_pts['red'] - data_pts['NIR']) / (data_pts['red'] + data_pts['NIR'])\n",
    "        if terrain_predictors==True:\n",
    "            # -----Load DEM\n",
    "            x = im.bounds.left, im.bounds.right, im.bounds.right, im.bounds.left, im.bounds.left\n",
    "            y = im.bounds.bottom, im.bounds.bottom, im.bounds.top, im.bounds.top, im.bounds.bottom\n",
    "            coords = list(zip(x,y))\n",
    "            bb_gdf = gpd.GeoDataFrame({'geometry': [Polygon(coords)]}, crs=im.crs)\n",
    "            DEM, DEM_x, DEM_y, AOI_UTM = query_GEE_for_DEM(bb_gdf, im_path, im_fn)\n",
    "            # flatten DEM to 2D\n",
    "            DEM = DEM.reshape((DEM.shape[0], DEM.shape[1]))\n",
    "            DEM_rd = rd.rdarray(DEM, no_data=-9999) # rich DEM array of DEM\n",
    "            # calculate slope and aspect using DEM\n",
    "            slope = rd.TerrainAttribute(DEM_rd, attrib='slope_degrees')\n",
    "            aspect = rd.TerrainAttribute(DEM_rd, attrib='aspect')\n",
    "            # convert from rdarray to numpy array\n",
    "            slope, aspect = np.array(slope).astype(int), np.array(aspect).astype(int)\n",
    "            # interpolate elevation at coords\n",
    "            f_DEM = scipy.interpolate.interp2d(DEM_x, DEM_y, DEM)\n",
    "            data_pts['elevation'] = [f_DEM(x[0], x[1])[0] for x in data_pts['coords']]\n",
    "            # interpolate slope at coords\n",
    "            f_slope = scipy.interpolate.interp2d(DEM_x, DEM_y, slope) \n",
    "            data_pts['slope'] = [f_slope(x[0], x[1])[0] for x in data_pts['coords']] \n",
    "            # interpolate aspect at coords\n",
    "            f_aspect = scipy.interpolate.interp2d(DEM_x, DEM_y, aspect) \n",
    "            data_pts['aspect'] = [f_aspect(x[0], x[1])[0] for x in data_pts['coords']]\n",
    "            \n",
    "        # concatenate to full DataFrame\n",
    "        data_pts_full = pd.concat([data_pts_full, data_pts], ignore_index=True)\n",
    "    \n",
    "    # -----Plot RGB images, data point locations, and band histograms\n",
    "#     print('Training data:')\n",
    "#     fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(8,16), gridspec_kw={'height_ratios': [4, 1, 1]})\n",
    "#     plt.rcParams.update({'font.size': 14, 'font.sans-serif': 'Arial'})\n",
    "#     # Image 1\n",
    "#     ax1.imshow(np.dstack([r, g, b]), \n",
    "#                extent=(np.min(im_x), np.max(im_x), np.min(im_y), np.max(im_y)))\n",
    "#     data_pts.loc[data_pts['class']==1].plot(ax=ax1, markersize=15, color='cyan', label='snow')\n",
    "#     data_pts.loc[data_pts['class']==2].plot(ax=ax1, markersize=15, color='blue', label='ice')\n",
    "#     data_pts.loc[data_pts['class']==3].plot(ax=ax1, markersize=15, color='orange', label='rock')\n",
    "#     data_pts.loc[data_pts['class']==4].plot(ax=ax1, markersize=15, color='grey', label='snow')\n",
    "#     ax1.legend(loc='lower right')\n",
    "#     ax1.set_xlabel('Easting [m]')\n",
    "#     ax1.set_ylabel('Northing [m]')\n",
    "#     ax1.set_title(im_date)\n",
    "#     ax2.hist(b[b>0].flatten(), color='blue', histtype='step', linewidth=2, bins=100, label='blue')\n",
    "#     ax2.hist(g[g>0].flatten(), color='green', histtype='step', linewidth=2, bins=100, label='green')\n",
    "#     ax2.hist(r[r>0].flatten(), color='red', histtype='step', linewidth=2, bins=100, label='red')\n",
    "#     ax2.hist(nir[nir>0].flatten(), color='brown', histtype='step', linewidth=2, bins=100, label='NIR')\n",
    "#     ax2.set_xlabel('Surface reflectance')\n",
    "#     ax2.set_ylabel('Pixel counts')\n",
    "#     ax2.grid()\n",
    "#     ax3.legend(loc='right')\n",
    "#     ax3.hist(ndsi.flatten(), bins=100)\n",
    "#     ax3.set_xlabel('NDSI')\n",
    "#     ax3.set_ylabel('Pixel counts')\n",
    "#     ax3.grid()\n",
    "#     plt.show()\n",
    "        \n",
    "    # -----Test supervised classification algorithms\n",
    "    # Split data points into features (band values / terrain parameters) and target variable (class)\n",
    "    if terrain_predictors==True:\n",
    "        feature_cols = ['blue', 'green', 'red', 'NIR', 'NDSI', 'moy', 'elevation', 'slope', 'aspect']\n",
    "    else:\n",
    "        feature_cols = ['blue', 'green', 'red', 'NIR', 'NDSI', 'moy']\n",
    "    X = data_pts[feature_cols] # features\n",
    "    y = data_pts['class'] # target variable\n",
    "\n",
    "    # Split data points into testing and training\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Iterate over classifiers\n",
    "    j = 0 # loop counter\n",
    "    accuracy = [] # classifier overall accuracy\n",
    "    K = [] # classifier kappa score\n",
    "    TP, TN, FP, FN = [], [], [], [] # confusion matrix components\n",
    "    for name, clf in zip(names, classifiers):\n",
    "            \n",
    "        # train classifier\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Calculate statistics\n",
    "        # overall accuracy\n",
    "        y_pred = clf.predict(X_test)\n",
    "        accuracy = accuracy + [metrics.accuracy_score(y_test, y_pred)]\n",
    "        # Kappa score\n",
    "        K = K + [metrics.cohen_kappa_score(y_test, y_pred)]\n",
    "        # confusion matrix\n",
    "        TP = TP + [np.where((y_test==1) & (y_pred==1))[0].size]\n",
    "        TN = TN + [np.where((y_test!=1) & (y_pred!=1))[0].size]\n",
    "        FP = FP + [np.where((y_test!=1) & (y_pred==1))[0].size]\n",
    "        FN = FN + [np.where((y_test==1) & (y_pred!=1))[0].size] \n",
    "    \n",
    "        j+=1\n",
    "\n",
    "    # Determine best classifier based on accuracy\n",
    "    results = pd.DataFrame()\n",
    "    results['Classifier'], results['Accuracy'], results['Kappa_score'] = names, accuracy, K\n",
    "    results['TP'], results['TN'], results['FP'], results['FN'] = TP, TN, FP, FN\n",
    "    clf_best_name = names[np.where(accuracy==np.max(accuracy))[0][0]]\n",
    "    clf_best = classifiers[np.where(accuracy==np.max(accuracy))[0][0]]\n",
    "    print(results)\n",
    "    print('')\n",
    "    print('Best accuracy classifier: ' + clf_best_name)\n",
    "\n",
    "    # -----Save most accurate classifier\n",
    "    if save_outputs==True:\n",
    "        clf_fn = out_path + site_names[i] + '_classifier.sav'\n",
    "        pickle.dump(clf_best, open(clf_fn, 'wb'))\n",
    "        print('Most accurate classifier saved to file: ',clf_fn)\n",
    "        feature_cols_fn = out_path + site_names[i] + '_classifier_feature_cols.pkl'\n",
    "        pickle.dump(feature_cols, open(feature_cols_fn, 'wb'))\n",
    "        print('Feature columns saved to file: ', feature_cols_fn)\n",
    "    \n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdb3fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Plot spectral components of each class\n",
    "xticks = np.array([1, 2, 3, 4, 5], dtype=float)\n",
    "xtick_labels = ['Snow', 'Shadowed snow', 'Ice', 'Rock', 'Water']\n",
    "marker_size = 5\n",
    "fig1, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "plt.rcParams.update({'font.size': 14, 'font.sans-serif': 'Arial'})\n",
    "# blue \n",
    "data_b = [data_pts_full.loc[data_pts_full['class']==1]['blue'], \n",
    "          data_pts_full.loc[data_pts_full['class']==2]['blue'],\n",
    "          data_pts_full.loc[data_pts_full['class']==3]['blue'],\n",
    "          data_pts_full.loc[data_pts_full['class']==4]['blue'], \n",
    "          data_pts_full.loc[data_pts_full['class']==5]['blue']]\n",
    "ax1.boxplot(data_b)\n",
    "ax1.set_xticks(xticks)\n",
    "ax1.set_xticklabels(xtick_labels)\n",
    "ax1.set_title('Blue')\n",
    "# green\n",
    "data_g = [data_pts_full.loc[data_pts_full['class']==1]['green'], \n",
    "          data_pts_full.loc[data_pts_full['class']==2]['green'],\n",
    "          data_pts_full.loc[data_pts_full['class']==3]['green'],\n",
    "          data_pts_full.loc[data_pts_full['class']==4]['green'], \n",
    "          data_pts_full.loc[data_pts_full['class']==5]['green']]\n",
    "ax2.boxplot(data_g)\n",
    "ax2.set_xticks(xticks)\n",
    "ax2.set_xticklabels(xtick_labels)\n",
    "ax2.set_title('Green')\n",
    "# red\n",
    "data_r = [data_pts_full.loc[data_pts_full['class']==1]['red'], \n",
    "          data_pts_full.loc[data_pts_full['class']==2]['red'],\n",
    "          data_pts_full.loc[data_pts_full['class']==3]['red'],\n",
    "          data_pts_full.loc[data_pts_full['class']==4]['red'],\n",
    "          data_pts_full.loc[data_pts_full['class']==5]['red']]\n",
    "ax3.boxplot(data_r)\n",
    "ax3.set_xticks(xticks)\n",
    "ax3.set_xticklabels(xtick_labels)\n",
    "ax3.set_title('Red')\n",
    "# NIR\n",
    "data_nir = [data_pts_full.loc[data_pts_full['class']==1]['NIR'], \n",
    "            data_pts_full.loc[data_pts_full['class']==2]['NIR'],\n",
    "            data_pts_full.loc[data_pts_full['class']==3]['NIR'],\n",
    "            data_pts_full.loc[data_pts_full['class']==4]['NIR'],\n",
    "            data_pts_full.loc[data_pts_full['class']==5]['NIR']]\n",
    "ax4.boxplot(data_nir)\n",
    "ax4.set_xticks(xticks)\n",
    "ax4.set_xticklabels(xtick_labels)\n",
    "ax4.set_title('NIR')\n",
    "\n",
    "fig2, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "# NDSI\n",
    "data_ndsi = [data_pts_full.loc[data_pts_full['class']==1]['NDSI'], \n",
    "             data_pts_full.loc[data_pts_full['class']==2]['NDSI'],\n",
    "             data_pts_full.loc[data_pts_full['class']==3]['NDSI'],\n",
    "             data_pts_full.loc[data_pts_full['class']==4]['NDSI'], \n",
    "             data_pts_full.loc[data_pts_full['class']==5]['NDSI']]\n",
    "ax.boxplot(data_ndsi)\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_xticklabels(xtick_labels)\n",
    "ax.set_ylim(-1, 1)\n",
    "ax.set_title('NDSI')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf504e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Plot terrain parameters of each class\n",
    "xticks = np.array([1, 2, 3, 4, 5], dtype=float)\n",
    "xtick_labels = ['Snow', 'Shadowed snow', 'Ice', 'Rock', 'Water']\n",
    "marker_size = 5\n",
    "fig1, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 8))\n",
    "plt.rcParams.update({'font.size': 14, 'font.sans-serif': 'Arial'})\n",
    "# elevation \n",
    "data_elev = [data_pts_full.loc[data_pts_full['class']==1]['elevation'], \n",
    "          data_pts_full.loc[data_pts_full['class']==2]['elevation'],\n",
    "          data_pts_full.loc[data_pts_full['class']==3]['elevation'],\n",
    "          data_pts_full.loc[data_pts_full['class']==4]['elevation'], \n",
    "          data_pts_full.loc[data_pts_full['class']==5]['elevation']]\n",
    "ax1.boxplot(data_elev)\n",
    "ax1.set_xticks(xticks)\n",
    "ax1.set_xticklabels(xtick_labels)\n",
    "ax1.set_title('Elevation')\n",
    "# slope\n",
    "data_slope = [data_pts_full.loc[data_pts_full['class']==1]['slope'], \n",
    "          data_pts_full.loc[data_pts_full['class']==2]['slope'],\n",
    "          data_pts_full.loc[data_pts_full['class']==3]['slope'],\n",
    "          data_pts_full.loc[data_pts_full['class']==4]['slope'], \n",
    "          data_pts_full.loc[data_pts_full['class']==5]['slope']]\n",
    "ax2.boxplot(data_slope)\n",
    "ax2.set_xticks(xticks)\n",
    "ax2.set_xticklabels(xtick_labels)\n",
    "ax2.set_title('slope')\n",
    "# aspect\n",
    "data_aspect = [data_pts_full.loc[data_pts_full['class']==1]['aspect'], \n",
    "          data_pts_full.loc[data_pts_full['class']==2]['aspect'],\n",
    "          data_pts_full.loc[data_pts_full['class']==3]['aspect'],\n",
    "          data_pts_full.loc[data_pts_full['class']==4]['aspect'],\n",
    "          data_pts_full.loc[data_pts_full['class']==5]['aspect']]\n",
    "ax3.boxplot(data_aspect)\n",
    "ax3.set_xticks(xticks)\n",
    "ax3.set_xticklabels(xtick_labels)\n",
    "ax3.set_title('aspect')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e177566d",
   "metadata": {},
   "source": [
    "### Test one classifier for all sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa7204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Test supervised classification algorithms\n",
    "# Split data points into features (band values) and target variable (snow)\n",
    "# if terrain_predictors==True:\n",
    "feature_cols = ['blue', 'green', 'red', 'NIR', 'NDSI', 'moy'] \n",
    "# else:\n",
    "# feature_cols = ['blue', 'green', 'red', 'NIR', 'NDSI', 'moy']\n",
    "X = data_pts_full[feature_cols] # features\n",
    "y = data_pts_full['class'] # target variable\n",
    "\n",
    "# Split data points into testing and training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Iterate over classifiers\n",
    "j = 0 # loop counter\n",
    "accuracy = [] # classifier overall accuracy\n",
    "K = [] # classifier kappa score\n",
    "TP, TN, FP, FN = [], [], [], [] # confusion matrix components\n",
    "for name, clf in zip(names, classifiers):\n",
    "\n",
    "    # train classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate statistics\n",
    "    # overall accuracy\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy + [metrics.accuracy_score(y_test, y_pred)]\n",
    "    # Kappa score\n",
    "    K = K + [metrics.cohen_kappa_score(y_test, y_pred)]\n",
    "    # confusion matrix\n",
    "    TP = TP + [np.where((y_test==1) & (y_pred==1))[0].size]\n",
    "    TN = TN + [np.where((y_test!=1) & (y_pred!=1))[0].size]\n",
    "    FP = FP + [np.where((y_test!=1) & (y_pred==1))[0].size]\n",
    "    FN = FN + [np.where((y_test==1) & (y_pred!=1))[0].size] \n",
    "\n",
    "    j+=1\n",
    "\n",
    "# Determine best classifier based on accuracy\n",
    "results = pd.DataFrame()\n",
    "results['Classifier'], results['Accuracy'], results['Kappa_score'] = names, accuracy, K\n",
    "results['TP'], results['TN'], results['FP'], results['FN'] = TP, TN, FP, FN\n",
    "clf_best_name = names[np.where(accuracy==np.max(accuracy))[0][0]]\n",
    "clf_best = classifiers[np.where(accuracy==np.max(accuracy))[0][0]]\n",
    "print(results)\n",
    "print('')\n",
    "print('Best accuracy classifier: ' + clf_best_name)\n",
    "\n",
    "# -----Save most accurate classifier\n",
    "if save_outputs==True:\n",
    "    clf_fn = out_path+'all_sites_classifier.sav'\n",
    "    pickle.dump(clf_best, open(clf_fn, 'wb'))\n",
    "    print('Most accurate classifier saved to file: ',clf_fn)\n",
    "    feature_cols_fn = out_path+'all_sites_classifier_feature_cols.pkl'\n",
    "    pickle.dump(feature_cols, open(feature_cols_fn, 'wb'))\n",
    "    print('Feature columns saved to file: ',feature_cols_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10187df6",
   "metadata": {},
   "source": [
    "### Test unsupervised classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e6c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----KMeans\n",
    "# image 1\n",
    "# I1_real = ~np.isnan(b1)\n",
    "# im1_x_mesh, im1_y_mesh = np.meshgrid(im1_x, im1_y)\n",
    "# im1_x_real = im1_x_mesh[I1_real]\n",
    "# im1_y_real = im1_y_mesh[I1_real]\n",
    "# b1_real = b1[I1_real].flatten()\n",
    "# g1_real = g1[I1_real].flatten()\n",
    "# r1_real = r1[I1_real].flatten()\n",
    "# nir1_real = nir1[I1_real].flatten()\n",
    "# ndsi1_real = ((r1_real - nir1_real)/(r1_real + nir1_real))\n",
    "# X1 = np.column_stack((b1_real, g1_real, r1_real, nir1_real, ndsi1_real))\n",
    "\n",
    "# # image 2\n",
    "# I2_real = ~np.isnan(b2)\n",
    "# im2_x_mesh, im2_y_mesh = np.meshgrid(im2_x, im2_y)\n",
    "# im2_x_real = im2_x_mesh[I2_real]\n",
    "# im2_y_real = im2_y_mesh[I2_real]\n",
    "# b2_real = b2[I2_real].flatten()\n",
    "# g2_real = g2[I2_real].flatten()\n",
    "# r2_real = r2[I2_real].flatten()\n",
    "# nir2_real = nir2[I2_real].flatten()\n",
    "# ndsi2_real = ((r2_real - nir2_real)/(r2_real + nir2_real))\n",
    "# X2 = np.column_stack((b2_real, g2_real, r2_real, nir2_real, ndsi2_real))\n",
    "\n",
    "# # generate classifier and classify images\n",
    "# n = 3 # number of clusters\n",
    "# Y1 = KMeans(n_clusters=n).fit(X1)\n",
    "# labels1 = Y1.labels_\n",
    "# Y2 = KMeans(n_clusters=n).fit(X2)\n",
    "# labels2 = Y2.labels_\n",
    "\n",
    "# # reshape from flat array to original shape\n",
    "# clusters1 = np.zeros((np.shape(b1)[0], np.shape(b1)[1]))\n",
    "# clusters1[:] = np.nan\n",
    "# clusters1[I1_real] = labels1\n",
    "# clusters2 = np.zeros((np.shape(b2)[0], np.shape(b2)[1]))\n",
    "# clusters2[:] = np.nan\n",
    "# clusters2[I2_real] = labels2\n",
    "\n",
    "# fig, ax = plt.subplots(2, 2, figsize=(12,12))\n",
    "# ax[0,0].imshow(np.dstack([r1, g1, b1]))\n",
    "# ax[0,1].imshow(clusters1)\n",
    "# ax[1,0].imshow(np.dstack([r2, g2, b2]))\n",
    "# ax[1,1].imshow(clusters2)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planet-snow",
   "language": "python",
   "name": "planet-snow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
