{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c53b019d",
   "metadata": {},
   "source": [
    "## Notebook to develop supervised classification algorithm for identifying snow in PlanetScope 4-band imagery\n",
    "Rainey Aberle\n",
    "\n",
    "Adapted from the [SciKit Learn Classifier comparison tutorial](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n",
    "\n",
    "### Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3837b29f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -----Import packages\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import time\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ed447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Determine whether to save outputs to file\n",
    "save_outputs = False # = True to save output figures and best classifier \n",
    "\n",
    "# -----Define paths in directory\n",
    "# base directory (path to planet-snow/)\n",
    "base_path = '/Users/raineyaberle/Research/PhD/planet-snow/'\n",
    "# output folder for best classifier\n",
    "out_path = base_path+'inputs-outputs/'\n",
    "\n",
    "# -----Add path to functions\n",
    "sys.path.insert(1, base_path+'functions/')\n",
    "from classification_utils import crop_images_to_AOI, classify_image, calculate_SCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb69ae3f",
   "metadata": {},
   "source": [
    "### Define supervised classification algorithms to test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b920160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Classifier names\n",
    "names = [\n",
    "#     \"Gaussian Process\", # keeps crashing kernel when classifying images!\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "    \"Logistic Regression\"\n",
    "]\n",
    "\n",
    "# -----Classifiers\n",
    "classifiers = [\n",
    "#     GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    LogisticRegression(random_state = 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3640589b",
   "metadata": {},
   "source": [
    "### Test site-specific classifiers\n",
    "\n",
    "Use classified points at each site to determine the best site-specific classifier, calculate accuracy\n",
    "\n",
    "Classified points split: 80% training, 20% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdb0425",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -----Define site IDs\n",
    "# Wolverine (WO), SitKusa (SK), Mendenhall (ME), Easton (EA), Blue (BL), Emmons (EM)\n",
    "site_IDs = ['WO', 'SK', 'ME', 'EA', 'BL', 'EM']\n",
    "site_names = ['Wolverine', 'SitKusa', 'Mendenhall', 'Easton', 'Blue', 'Emmons']\n",
    "\n",
    "# -----Define images\n",
    "im_fns = [('20190626_21_adj.tif', '20190819_21_adj.tif'), # Wolverine\n",
    "         ('20190805_20_adj.tif'), # Sit Kusa\n",
    "         ('20210823_20_adj.tif'), # Mendenhall\n",
    "         ('20181020_17_adj.tif', '20200829_19_adj.tif'), # Easton\n",
    "         ('20210703_18_adj.tif', '20210924_18_adj.tif'), #Blue\n",
    "         ('20210604_18_adj.tif', '20210828_19_adj.tif')] # Emmons\n",
    "\n",
    "# -----Initialize data_pts for single classifier (next step)\n",
    "data_pts_full = gpd.GeoDataFrame()\n",
    "\n",
    "# -----Loop through sites\n",
    "for i in range(len(site_names)):\n",
    "    \n",
    "    print('----------')\n",
    "    print(site_names[i])\n",
    "    \n",
    "    # path to images\n",
    "    im_path = base_path+'../study-sites/'+site_names[i]+'/imagery/Planet/adjusted-filtered/'\n",
    "\n",
    "    if len(im_fns[i])==2:\n",
    "        \n",
    "        # Image 1\n",
    "        im1_fn = im_fns[i][0] # image 1 file name\n",
    "        im1_date = im1_fn[0:4]+'-'+im1_fn[4:6]+'-'+im1_fn[6:8] # image 1 capture date\n",
    "        im1 = rio.open(im_path+im1_fn) # open image 1\n",
    "        epsg = str(im1.crs)[5:] # grab EPSG code\n",
    "        # read bands\n",
    "        b1 = im1.read(1).astype(float) \n",
    "        r1 = im1.read(2).astype(float) \n",
    "        g1 = im1.read(3).astype(float) \n",
    "        nir1 = im1.read(4).astype(float) \n",
    "        # divide by image scalar if max band values > 1000\n",
    "        if np.nanmax(b1) > 1e3:\n",
    "            apply_scalar = True\n",
    "            im_scalar = 10000\n",
    "            b1 = b1 / im_scalar\n",
    "            g1 = g1 / im_scalar\n",
    "            r1 = r1 / im_scalar\n",
    "            nir1 = nir1 / im_scalar\n",
    "        else:\n",
    "            apply_scalar = False\n",
    "        # calculate NDSI\n",
    "        ndsi1 = (r1 - nir1) / (r1 + nir1)\n",
    "        # define coordinates grid\n",
    "        im1_x = np.linspace(im1.bounds.left, im1.bounds.right, num=np.shape(b1)[1])\n",
    "        im1_y = np.linspace(im1.bounds.top, im1.bounds.bottom, num=np.shape(b1)[0])\n",
    "        # load snow training points\n",
    "        data_snow_pts1_fn = base_path+'../study-sites/'+site_names[i]+'/classified-points/'+site_IDs[i]+'_snow_points_'+im1_fn[:-8]+'.shp'\n",
    "        data_snow_pts1 = gpd.read_file(data_snow_pts1_fn) \n",
    "        data_snow_pts1 = data_snow_pts1.to_crs(epsg) # reproject to image CRS\n",
    "        # load non-snow points\n",
    "        data_non_snow_pts1_fn = base_path+'../study-sites/'+site_names[i]+'/classified-points/'+site_IDs[i]+'_non_snow_points_'+im1_fn[:-8]+'.shp'\n",
    "        data_non_snow_pts1 = gpd.read_file(data_non_snow_pts1_fn)\n",
    "        data_non_snow_pts1 = data_non_snow_pts1.to_crs(epsg) # reproject to image CRS\n",
    "        \n",
    "        # Image 2\n",
    "        im2_fn = im_fns[i][1] # image 2 file name\n",
    "        im2_date = im2_fn[0:4]+'-'+im2_fn[4:6]+'-'+im2_fn[6:8] # image 1 capture date\n",
    "        im2 = rio.open(im_path+im2_fn) # open image 1\n",
    "        # read bands\n",
    "        b2 = im2.read(1).astype(float)\n",
    "        r2 = im2.read(2).astype(float) \n",
    "        g2 = im2.read(3).astype(float) \n",
    "        nir2 = im2.read(4).astype(float) \n",
    "        if apply_scalar==True:\n",
    "            b2 = b2 / im_scalar\n",
    "            g2 = g2 / im_scalar\n",
    "            r2 = r2 / im_scalar\n",
    "            nir2 = nir2 / im_scalar\n",
    "        # calculate NDSI\n",
    "        ndsi2 = (r2 - nir2) / (r2 + nir2)\n",
    "        # define coordinates grid\n",
    "        im2_x = np.linspace(im2.bounds.left, im2.bounds.right, num=np.shape(b2)[1])\n",
    "        im2_y = np.linspace(im2.bounds.top, im2.bounds.bottom, num=np.shape(b2)[0])\n",
    "        # load snow training points\n",
    "        data_snow_pts2_fn = base_path+'../study-sites/'+site_names[i]+'/classified-points/'+site_IDs[i]+'_snow_points_'+im2_fn[:-8]+'.shp'\n",
    "        data_snow_pts2 = gpd.read_file(data_snow_pts2_fn)\n",
    "        # reproject to defined CRS\n",
    "        data_snow_pts2 = data_snow_pts2.to_crs(epsg)\n",
    "        # load non-snow points\n",
    "        data_non_snow_pts2_fn = base_path+'../study-sites/'+site_names[i]+'/classified-points/'+site_IDs[i]+'_non_snow_points_'+im2_fn[:-8]+'.shp'\n",
    "        data_non_snow_pts2 = gpd.read_file(data_non_snow_pts2_fn)\n",
    "        # Reproject to defined CRS\n",
    "        data_non_snow_pts2 = data_non_snow_pts2.to_crs(epsg)        \n",
    "        \n",
    "        # Plot RGB images, data point locations, and band histograms\n",
    "        print('Training data:')\n",
    "        fig, ((ax1,ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(16,16), gridspec_kw={'height_ratios': [4, 1, 1]})\n",
    "        plt.rcParams.update({'font.size': 14, 'font.sans-serif': 'Arial'})\n",
    "        # Image 1\n",
    "        ax1.imshow(np.dstack([r1, g1, b1]), \n",
    "                   extent=(np.min(im1_x), np.max(im1_x), np.min(im1_y), np.max(im1_y)))\n",
    "        data_snow_pts1.plot(ax=ax1, markersize=15, color='cyan', label='snow')\n",
    "        data_non_snow_pts1.plot(ax=ax1, markersize=15, color='orange', label='non-snow')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.set_xlabel('Easting [m]')\n",
    "        ax1.set_ylabel('Northing [m]')\n",
    "        ax1.set_title(im1_date)\n",
    "        ax3.hist(b1[b1>0].flatten(), color='blue', histtype='step', linewidth=2, bins=100, label='blue')\n",
    "        ax3.hist(g1[g1>0].flatten(), color='green', histtype='step', linewidth=2, bins=100, label='green')\n",
    "        ax3.hist(r1[r1>0].flatten(), color='red', histtype='step', linewidth=2, bins=100, label='red')\n",
    "        ax3.hist(nir1[nir1>0].flatten(), color='brown', histtype='step', linewidth=2, bins=100, label='NIR')\n",
    "        ax3.set_xlabel('Surface reflectance')\n",
    "        ax3.set_ylabel('Pixel counts')\n",
    "        ax3.grid()\n",
    "        ax3.legend(loc='right')\n",
    "        ax5.hist(ndsi1.flatten(), bins=100)\n",
    "        ax5.set_xlabel('NDSI')\n",
    "        ax5.set_ylabel('Pixel counts')\n",
    "        ax5.grid()\n",
    "        # Image 2\n",
    "        ax2.imshow(np.dstack([r2, g2, b2]), \n",
    "                   extent=(np.min(im2_x), np.max(im2_x), np.min(im2_y), np.max(im2_y)))\n",
    "        data_snow_pts2.plot(ax=ax2, markersize=15, color='cyan', label='snow')\n",
    "        data_non_snow_pts2.plot(ax=ax2, markersize=15, color='orange', label='non-snow')\n",
    "        ax2.legend(loc='lower right')\n",
    "        ax2.set_xlabel('Easting [m]')\n",
    "        ax2.set_title(im2_date)\n",
    "        ax4.hist(b2[b2>0].flatten(), color='blue', histtype='step', linewidth=2, bins=100, label='blue')\n",
    "        ax4.hist(g2[g2>0].flatten(), color='green', histtype='step', linewidth=2, bins=100, label='green')\n",
    "        ax4.hist(r2[r2>0].flatten(), color='red', histtype='step', linewidth=2, bins=100, label='red')\n",
    "        ax4.hist(nir2[nir2>0].flatten(), color='brown', histtype='step', linewidth=2, bins=100, label='NIR')\n",
    "        ax4.set_xlabel('Surface reflectance')\n",
    "        ax4.grid()\n",
    "        ax6.hist(ndsi2.flatten(), bins=100)\n",
    "        ax6.set_xlabel('NDSI')\n",
    "        ax6.grid()\n",
    "        plt.show()\n",
    "        \n",
    "        # Set up training data\n",
    "        # Add date and snow classification column to data points\n",
    "        data_snow_pts1['date'] = im1_date\n",
    "        data_snow_pts1['snow'] = 1\n",
    "        data_non_snow_pts1['date'] = im1_date\n",
    "        data_non_snow_pts1['snow'] = 0\n",
    "        data_snow_pts2['date'] = im2_date\n",
    "        data_snow_pts2['snow'] = 1\n",
    "        data_non_snow_pts2['date'] = im2_date\n",
    "        data_non_snow_pts2['snow'] = 0\n",
    "        # Merge snow and non-snow points\n",
    "        data_pts = pd.concat([data_snow_pts1, data_non_snow_pts1, data_snow_pts2, data_non_snow_pts2], ignore_index=True)\n",
    "        # Add coords column\n",
    "        data_pts['coords'] = [(pt.bounds[0], pt.bounds[1]) for pt in data_pts['geometry']]\n",
    "        # remove \"id\" and \"geometry\" columns\n",
    "        data_pts = data_pts.drop(columns=['id', 'geometry'])\n",
    "        # Sample band values at points\n",
    "        data_pts['blue'] = ' '\n",
    "        data_pts['green'] = ' '\n",
    "        data_pts['red'] = ' '\n",
    "        data_pts['NIR'] = ' '\n",
    "        data_pts['blue'].loc[data_pts['date']==im1_date] = [x[0] for x in im1.sample(data_pts['coords'].loc[data_pts['date']==im1_date])]\n",
    "        data_pts['green'].loc[data_pts['date']==im1_date] = [x[1] for x in im1.sample(data_pts['coords'].loc[data_pts['date']==im1_date])]\n",
    "        data_pts['red'].loc[data_pts['date']==im1_date] = [x[2] for x in im1.sample(data_pts['coords'].loc[data_pts['date']==im1_date])]\n",
    "        data_pts['NIR'].loc[data_pts['date']==im1_date] = [x[3] for x in im1.sample(data_pts['coords'].loc[data_pts['date']==im1_date])]\n",
    "        data_pts['blue'].loc[data_pts['date']==im2_date] = [x[0] for x in im2.sample(data_pts['coords'].loc[data_pts['date']==im2_date])]\n",
    "        data_pts['green'].loc[data_pts['date']==im2_date] = [x[1] for x in im2.sample(data_pts['coords'].loc[data_pts['date']==im2_date])]\n",
    "        data_pts['red'].loc[data_pts['date']==im2_date] = [x[2] for x in im2.sample(data_pts['coords'].loc[data_pts['date']==im2_date])]\n",
    "        data_pts['NIR'].loc[data_pts['date']==im2_date] = [x[3] for x in im2.sample(data_pts['coords'].loc[data_pts['date']==im2_date])]\n",
    "        # Divide values by im_scalar if applicable\n",
    "        if apply_scalar:\n",
    "            data_pts['blue'] = data_pts['blue'].div(im_scalar)\n",
    "            data_pts['green'] = data_pts['green'].div(im_scalar)\n",
    "            data_pts['red'] = data_pts['red'].div(im_scalar)\n",
    "            data_pts['NIR'] = data_pts['NIR'].div(im_scalar)\n",
    "        # Add NDSI column\n",
    "        data_pts['NDSI'] = (data_pts['red'] - data_pts['NIR']) / (data_pts['red'] + data_pts['NIR'])\n",
    "        # Remove rows containing NaN\n",
    "        data_pts = data_pts.dropna()\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # Image 1\n",
    "        im1_fn = im_fns[i] # file name\n",
    "        im1_date = im1_fn[0:4]+'-'+im1_fn[4:6]+'-'+im1_fn[6:8] # image capture date\n",
    "        im1 = rio.open(im_path+im1_fn) # open image 1\n",
    "        epsg = str(im1.crs)[5:] # grab EPSG code\n",
    "        # read bands\n",
    "        b1 = im1.read(1).astype(float) \n",
    "        r1 = im1.read(2).astype(float) \n",
    "        g1 = im1.read(3).astype(float) \n",
    "        nir1 = im1.read(4).astype(float) \n",
    "        # divide by image scalar if max band values > 1000\n",
    "        if np.nanmax(b1) > 1e3:\n",
    "            apply_scalar = True\n",
    "            im_scalar = 10000\n",
    "            b1 = b1 / im_scalar\n",
    "            g1 = g1 / im_scalar\n",
    "            r1 = r1 / im_scalar\n",
    "            nir1 = nir1 / im_scalar\n",
    "        else:\n",
    "            apply_scalar = False\n",
    "        # calculate NDSI\n",
    "        ndsi1 = (r1 - nir1) / (r1 + nir1)\n",
    "        # define coordinates grid\n",
    "        im1_x = np.linspace(im1.bounds.left, im1.bounds.right, num=np.shape(b1)[1])\n",
    "        im1_y = np.linspace(im1.bounds.top, im1.bounds.bottom, num=np.shape(b1)[0])\n",
    "        # load snow training points\n",
    "        data_snow_pts1_fn = base_path+'../study-sites/'+site_names[i]+'/classified-points/'+site_IDs[i]+'_snow_points_'+im1_fn[:-8]+'.shp'\n",
    "        data_snow_pts1 = gpd.read_file(data_snow_pts1_fn) \n",
    "        data_snow_pts1 = data_snow_pts1.to_crs(epsg) # reproject to image CRS\n",
    "        # load non-snow points\n",
    "        data_non_snow_pts1_fn = base_path+'../study-sites/'+site_names[i]+'/classified-points/'+site_IDs[i]+'_non_snow_points_'+im1_fn[:-8]+'.shp'\n",
    "        data_non_snow_pts1 = gpd.read_file(data_non_snow_pts1_fn)\n",
    "        data_non_snow_pts1 = data_non_snow_pts1.to_crs(epsg) # reproject to image CRS     \n",
    "        \n",
    "        # Plot RGB images, data point locations, and band histograms\n",
    "        print('Training data:')\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10,16), gridspec_kw={'height_ratios': [4, 1, 1]})\n",
    "        plt.rcParams.update({'font.size': 14, 'font.sans-serif': 'Arial'})\n",
    "        # Image 1\n",
    "        ax1.imshow(np.dstack([r1, g1, b1]), \n",
    "                   extent=(np.min(im1_x), np.max(im1_x), np.min(im1_y), np.max(im1_y)))\n",
    "        data_snow_pts1.plot(ax=ax1, markersize=15, color='cyan', label='snow')\n",
    "        data_non_snow_pts1.plot(ax=ax1, markersize=15, color='orange', label='non-snow')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.set_xlabel('Easting [m]')\n",
    "        ax1.set_ylabel('Northing [m]')\n",
    "        ax1.set_title(im1_date)\n",
    "        ax2.hist(b1[b1>0].flatten(), color='blue', histtype='step', linewidth=2, bins=100, label='blue')\n",
    "        ax2.hist(g1[g1>0].flatten(), color='green', histtype='step', linewidth=2, bins=100, label='green')\n",
    "        ax2.hist(r1[r1>0].flatten(), color='red', histtype='step', linewidth=2, bins=100, label='red')\n",
    "        ax2.hist(nir1[nir1>0].flatten(), color='brown', histtype='step', linewidth=2, bins=100, label='NIR')\n",
    "        ax2.set_xlabel('Surface reflectance')\n",
    "        ax2.set_ylabel('Pixel counts')\n",
    "        ax2.grid()\n",
    "        ax2.legend(loc='right')\n",
    "        ax3.hist(ndsi1.flatten(), bins=100)\n",
    "        ax3.set_xlabel('NDSI')\n",
    "        ax3.set_ylabel('Pixel counts')\n",
    "        ax3.grid()\n",
    "        plt.show()\n",
    "        \n",
    "        # -----Set up training data\n",
    "        # Add date and snow classification column to data points\n",
    "        data_snow_pts1['date'] = im1_date\n",
    "        data_snow_pts1['snow'] = 1\n",
    "        data_non_snow_pts1['date'] = im1_date\n",
    "        data_non_snow_pts1['snow'] = 0\n",
    "        # Merge snow and non-snow points\n",
    "        data_pts = pd.concat([data_snow_pts1, data_non_snow_pts1], ignore_index=True)\n",
    "        # Add coords column\n",
    "        data_pts['coords'] = [(pt.bounds[0], pt.bounds[1]) for pt in data_pts['geometry']]\n",
    "        # remove \"id\" and \"geometry\" columns\n",
    "        data_pts = data_pts.drop(columns=['id', 'geometry'])\n",
    "        # Sample band values at points\n",
    "        data_pts['blue'] = ' '\n",
    "        data_pts['green'] = ' '\n",
    "        data_pts['red'] = ' '\n",
    "        data_pts['NIR'] = ' '\n",
    "        data_pts['blue'] = [x[0] for x in im1.sample(data_pts['coords'])]\n",
    "        data_pts['green'] = [x[1] for x in im1.sample(data_pts['coords'])]\n",
    "        data_pts['red'] = [x[2] for x in im1.sample(data_pts['coords'])]\n",
    "        data_pts['NIR'] = [x[3] for x in im1.sample(data_pts['coords'])]\n",
    "        # Divide values by im_scalar if applicable\n",
    "        if apply_scalar:\n",
    "            data_pts['blue'] = data_pts['blue'].div(im_scalar)\n",
    "            data_pts['green'] = data_pts['green'].div(im_scalar)\n",
    "            data_pts['red'] = data_pts['red'].div(im_scalar)\n",
    "            data_pts['NIR'] = data_pts['NIR'].div(im_scalar)\n",
    "        # Add NDSI column\n",
    "        data_pts['NDSI'] = (data_pts['red'] - data_pts['NIR']) / (data_pts['red'] + data_pts['NIR'])\n",
    "        # Remove rows containing NaN\n",
    "        data_pts = data_pts.dropna()\n",
    "        \n",
    "    # -----Add data points to full data points DF\n",
    "    data_pts_full = pd.concat([data_pts_full, data_pts], ignore_index=True)\n",
    "\n",
    "    # -----Test supervised classification algorithms\n",
    "    # Split data points into features (band values) and target variable (snow)\n",
    "    feature_cols = ['blue', 'green', 'red', 'NIR', 'NDSI']\n",
    "    X = data_pts[feature_cols] # features\n",
    "    y = data_pts['snow'] # target variable\n",
    "\n",
    "    # Split data points into testing and training\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Iterate over classifiers\n",
    "    j = 0 # loop counter\n",
    "    accuracy = [] # classifier overall accuracy\n",
    "    K = [] # classifier kappa score\n",
    "    TP, TN, FP, FN = [], [], [], [] # confusion matrix components\n",
    "    for name, clf in zip(names, classifiers):\n",
    "            \n",
    "        # train classifier\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Calculate statistics\n",
    "        # overall accuracy\n",
    "        y_pred = clf.predict(X_test)\n",
    "        accuracy = accuracy + [metrics.accuracy_score(y_test, y_pred)]\n",
    "        # Kappa score\n",
    "        K = K + [metrics.cohen_kappa_score(y_test, y_pred)]\n",
    "        # confusion matrix\n",
    "        TP = TP + [np.where((y_test==1) & (y_pred==1))[0].size]\n",
    "        TN = TN + [np.where((y_test==0) & (y_pred==0))[0].size]\n",
    "        FP = FP + [np.where((y_test==0) & (y_pred==1))[0].size]\n",
    "        FN = FN + [np.where((y_test==1) & (y_pred==0))[0].size] \n",
    "    \n",
    "        j+=1\n",
    "\n",
    "    # Determine best classifier based on accuracy\n",
    "    results = pd.DataFrame()\n",
    "    results['Classifier'], results['Accuracy'], results['Kappa_score'] = names, accuracy, K\n",
    "    results['TP'], results['TN'], results['FP'], results['FN'] = TP, TN, FP, FN\n",
    "    clf_best_name = names[np.where(accuracy==np.max(accuracy))[0][0]]\n",
    "    clf_best = classifiers[np.where(accuracy==np.max(accuracy))[0][0]]\n",
    "    print(results)\n",
    "    print('')\n",
    "    print('Best accuracy classifier: ' + clf_best_name)\n",
    "\n",
    "    # -----Save most accurate classifier\n",
    "    if save_outputs==True:\n",
    "        clf_fn = out_path+site_IDs[i]+'_classifier.sav'\n",
    "        pickle.dump(clf_best, open(clf_fn, 'wb'))\n",
    "        print('Most accurate classifier saved to file: ',clf_fn)\n",
    "        feature_cols_fn = out_path+site_IDs[i]+'_classifier_feature_cols.pkl'\n",
    "        pickle.dump(feature_cols, open(feature_cols_fn, 'wb'))\n",
    "        print('Feature columns saved to file: ',feature_cols_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e177566d",
   "metadata": {},
   "source": [
    "### Test one classifier for all sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa7204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Test supervised classification algorithms\n",
    "# Split data points into features (band values) and target variable (snow)\n",
    "feature_cols = ['blue', 'green', 'red', 'NIR', 'NDSI']\n",
    "X = data_pts_full[feature_cols] # features\n",
    "y = data_pts_full['snow'] # target variable\n",
    "\n",
    "# Split data points into testing and training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Iterate over classifiers\n",
    "j = 0 # loop counter\n",
    "accuracy = [] # classifier overall accuracy\n",
    "K = [] # classifier kappa score\n",
    "TP, TN, FP, FN = [], [], [], [] # confusion matrix components\n",
    "for name, clf in zip(names, classifiers):\n",
    "\n",
    "    # train classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate statistics\n",
    "    # overall accuracy\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy + [metrics.accuracy_score(y_test, y_pred)]\n",
    "    # Kappa score\n",
    "    K = K + [metrics.cohen_kappa_score(y_test, y_pred)]\n",
    "    # confusion matrix\n",
    "    TP = TP + [np.where((y_test==1) & (y_pred==1))[0].size]\n",
    "    TN = TN + [np.where((y_test==0) & (y_pred==0))[0].size]\n",
    "    FP = FP + [np.where((y_test==0) & (y_pred==1))[0].size]\n",
    "    FN = FN + [np.where((y_test==1) & (y_pred==0))[0].size] \n",
    "\n",
    "    j+=1\n",
    "\n",
    "# Determine best classifier based on accuracy\n",
    "results = pd.DataFrame()\n",
    "results['Classifier'], results['Accuracy'], results['Kappa_score'] = names, accuracy, K\n",
    "results['TP'], results['TN'], results['FP'], results['FN'] = TP, TN, FP, FN\n",
    "clf_best_name = names[np.where(accuracy==np.max(accuracy))[0][0]]\n",
    "clf_best = classifiers[np.where(accuracy==np.max(accuracy))[0][0]]\n",
    "print(results)\n",
    "print('')\n",
    "print('Best accuracy classifier: ' + clf_best_name)\n",
    "\n",
    "# -----Save most accurate classifier\n",
    "if save_outputs==True:\n",
    "    clf_fn = out_path+'all_sites_classifier.sav'\n",
    "    pickle.dump(clf_best, open(clf_fn, 'wb'))\n",
    "    print('Most accurate classifier saved to file: ',clf_fn)\n",
    "    feature_cols_fn = out_path+'all_sites_classifier_feature_cols.pkl'\n",
    "    pickle.dump(feature_cols, open(feature_cols_fn, 'wb'))\n",
    "    print('Feature columns saved to file: ',feature_cols_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10187df6",
   "metadata": {},
   "source": [
    "### Test unsupervised classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e6c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----KMeans\n",
    "# image 1\n",
    "# I1_real = ~np.isnan(b1)\n",
    "# im1_x_mesh, im1_y_mesh = np.meshgrid(im1_x, im1_y)\n",
    "# im1_x_real = im1_x_mesh[I1_real]\n",
    "# im1_y_real = im1_y_mesh[I1_real]\n",
    "# b1_real = b1[I1_real].flatten()\n",
    "# g1_real = g1[I1_real].flatten()\n",
    "# r1_real = r1[I1_real].flatten()\n",
    "# nir1_real = nir1[I1_real].flatten()\n",
    "# ndsi1_real = ((r1_real - nir1_real)/(r1_real + nir1_real))\n",
    "# X1 = np.column_stack((b1_real, g1_real, r1_real, nir1_real, ndsi1_real))\n",
    "\n",
    "# # image 2\n",
    "# I2_real = ~np.isnan(b2)\n",
    "# im2_x_mesh, im2_y_mesh = np.meshgrid(im2_x, im2_y)\n",
    "# im2_x_real = im2_x_mesh[I2_real]\n",
    "# im2_y_real = im2_y_mesh[I2_real]\n",
    "# b2_real = b2[I2_real].flatten()\n",
    "# g2_real = g2[I2_real].flatten()\n",
    "# r2_real = r2[I2_real].flatten()\n",
    "# nir2_real = nir2[I2_real].flatten()\n",
    "# ndsi2_real = ((r2_real - nir2_real)/(r2_real + nir2_real))\n",
    "# X2 = np.column_stack((b2_real, g2_real, r2_real, nir2_real, ndsi2_real))\n",
    "\n",
    "# # generate classifier and classify images\n",
    "# n = 3 # number of clusters\n",
    "# Y1 = KMeans(n_clusters=n).fit(X1)\n",
    "# labels1 = Y1.labels_\n",
    "# Y2 = KMeans(n_clusters=n).fit(X2)\n",
    "# labels2 = Y2.labels_\n",
    "\n",
    "# # reshape from flat array to original shape\n",
    "# clusters1 = np.zeros((np.shape(b1)[0], np.shape(b1)[1]))\n",
    "# clusters1[:] = np.nan\n",
    "# clusters1[I1_real] = labels1\n",
    "# clusters2 = np.zeros((np.shape(b2)[0], np.shape(b2)[1]))\n",
    "# clusters2[:] = np.nan\n",
    "# clusters2[I2_real] = labels2\n",
    "\n",
    "# fig, ax = plt.subplots(2, 2, figsize=(12,12))\n",
    "# ax[0,0].imshow(np.dstack([r1, g1, b1]))\n",
    "# ax[0,1].imshow(clusters1)\n",
    "# ax[1,0].imshow(np.dstack([r2, g2, b2]))\n",
    "# ax[1,1].imshow(clusters2)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
