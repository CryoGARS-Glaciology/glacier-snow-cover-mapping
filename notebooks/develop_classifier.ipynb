{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c53b019d",
   "metadata": {},
   "source": [
    "## Notebook to develop supervised classification algorithm for identifying snow in PlanetScope 4-band imagery\n",
    "Rainey Aberle\n",
    "\n",
    "Adapted from the [SciKit Learn Classifier comparison tutorial](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n",
    "\n",
    "### Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3837b29f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -----Import packages\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import time\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791e0eb1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -----Determine whether to save outputs to file\n",
    "save_outputs = True # = True to save output figures and best classifier \n",
    "\n",
    "# -----Define site ID used to identify classifier output\n",
    "# Wolverine (WO), Sit Kusa (SK), Mendenhall (ME), Easton (EA), Blue (BL), Emmons (EM)\n",
    "site_ID = 'WO'\n",
    "site_name = 'Wolverine'\n",
    "\n",
    "# -----Define paths in directory\n",
    "# base directory (path to planet-snow/)\n",
    "base_path = '/Users/raineyaberle/Research/PhD/planet-snow/'\n",
    "# image directory\n",
    "im_path = base_path+'../study-sites/'+site_name+'/imagery/Planet/adjusted-filtered/'\n",
    "# output folder for best classifier\n",
    "out_path = base_path+'inputs-outputs/'\n",
    "# output folder for figures\n",
    "figures_out_path = base_path+'figures/classifiers/'\n",
    "\n",
    "# -----Add path to functions\n",
    "sys.path.insert(1, base_path+'functions/')\n",
    "from classification_utils import crop_images_to_AOI, classify_image, calculate_SCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feef04c4",
   "metadata": {},
   "source": [
    "### Load images and snow/non-snow classified points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105961c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -----Define EPSG code\n",
    "epsg = 32606\n",
    "\n",
    "# -----Image 1\n",
    "# load image\n",
    "im1_fn = '20190626_21_adj.tif'\n",
    "im1 = rio.open(im_path+im1_fn)\n",
    "# read bands\n",
    "b1 = im1.read(1).astype(float) \n",
    "r1 = im1.read(2).astype(float) \n",
    "g1 = im1.read(3).astype(float) \n",
    "nir1 = im1.read(4).astype(float) \n",
    "if np.nanmean(b1) > 1e3:\n",
    "    im_scalar = 10000\n",
    "    b1 = b1 / im_scalar\n",
    "    g1 = g1 / im_scalar\n",
    "    r1 = r1 / im_scalar\n",
    "    nir1 = nir1 / im_scalar\n",
    "ndsi1 = (r1 - nir1) / (r1 + nir1)\n",
    "# define coordinates grid\n",
    "im1_x = np.linspace(im1.bounds.left, im1.bounds.right, num=np.shape(b1)[1])\n",
    "im1_y = np.linspace(im1.bounds.top, im1.bounds.bottom, num=np.shape(b1)[0])\n",
    "print('Image 1 CRS:',im1.crs)\n",
    "# load snow training points\n",
    "data_snow_pts1_fn = base_path+'../study-sites/'+site_name+'/classified-points/snow_pts_20190626_21.shp'\n",
    "data_snow_pts1 = gpd.read_file(data_snow_pts1_fn)\n",
    "# reproject to defined CRS\n",
    "data_snow_pts1 = data_snow_pts1.to_crs(epsg)\n",
    "print('Snow points 1 CRS:', data_snow_pts1.crs)\n",
    "# load non-snow points\n",
    "data_non_snow_pts1_fn = base_path+'../study-sites/'+site_name+'/classified-points/non_snow_pts_20190626_21.shp'\n",
    "data_non_snow_pts1 = gpd.read_file(data_non_snow_pts1_fn)\n",
    "# Reproject to defined CRS\n",
    "data_non_snow_pts1 = data_non_snow_pts1.to_crs(epsg)\n",
    "print('Non-snow points 1 CRS:', data_non_snow_pts1.crs)\n",
    "\n",
    "# -----Image 2\n",
    "im2_fn = '20190819_21_adj.tif'\n",
    "im2 = rio.open(im_path+im2_fn)\n",
    "# read bands\n",
    "b2 = im2.read(1).astype(float)\n",
    "r2 = im2.read(2).astype(float) \n",
    "g2 = im2.read(3).astype(float) \n",
    "nir2 = im2.read(4).astype(float) \n",
    "if np.nanmean(b2) > 1e3:\n",
    "    im_scalar = 10000\n",
    "    b2 = b2 / im_scalar\n",
    "    g2 = g2 / im_scalar\n",
    "    r2 = r2 / im_scalar\n",
    "    nir2 = nir2 / im_scalar\n",
    "ndsi2 = (r2 - nir2) / (r2 + nir2)\n",
    "# define coordinates grid\n",
    "im2_x = np.linspace(im2.bounds.left, im2.bounds.right, num=np.shape(b2)[1])\n",
    "im2_y = np.linspace(im2.bounds.top, im2.bounds.bottom, num=np.shape(b2)[0])\n",
    "print('Image 2 CRS:',im2.crs)\n",
    "# load snow training points\n",
    "data_snow_pts2_fn = base_path+'../study-sites/'+site_name+'/classified-points/snow_pts_20190819_21.shp'\n",
    "data_snow_pts2 = gpd.read_file(data_snow_pts2_fn)\n",
    "# reproject to defined CRS\n",
    "data_snow_pts2 = data_snow_pts2.to_crs(epsg)\n",
    "print('Snow points 2 CRS:', data_snow_pts2.crs)\n",
    "# load non-snow points\n",
    "data_non_snow_pts2_fn = base_path+'../study-sites/'+site_name+'/classified-points/non_snow_pts_20190819_21.shp'\n",
    "data_non_snow_pts2 = gpd.read_file(data_non_snow_pts2_fn)\n",
    "# Reproject to defined CRS\n",
    "data_non_snow_pts2 = data_non_snow_pts2.to_crs(epsg)\n",
    "print('Non-snow points 2 CRS:', data_non_snow_pts2.crs)\n",
    "\n",
    "# -----Plot RGB images, data point locations, and band histograms\n",
    "fig, ((ax1,ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(16,16), gridspec_kw={'height_ratios': [4, 1, 1]})\n",
    "plt.rcParams.update({'font.size': 14, 'font.sans-serif': 'Arial'})\n",
    "# Image 1\n",
    "ax1.imshow(np.dstack([r1, g1, b1]), \n",
    "           extent=(np.min(im1_x), np.max(im1_x), np.min(im1_y), np.max(im1_y)))\n",
    "data_snow_pts1.plot(ax=ax1, markersize=15, color='cyan', label='snow')\n",
    "data_non_snow_pts1.plot(ax=ax1, markersize=15, color='orange', label='non-snow')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.set_xlabel('Easting [m]')\n",
    "ax1.set_ylabel('Northing [m]')\n",
    "ax1.set_title('2021-04-21')\n",
    "ax3.hist(b1[b1>0].flatten(), color='blue', histtype='step', linewidth=2, bins=100, label='blue')\n",
    "ax3.hist(g1[g1>0].flatten(), color='green', histtype='step', linewidth=2, bins=100, label='green')\n",
    "ax3.hist(r1[r1>0].flatten(), color='red', histtype='step', linewidth=2, bins=100, label='red')\n",
    "ax3.hist(nir1[nir1>0].flatten(), color='brown', histtype='step', linewidth=2, bins=100, label='NIR')\n",
    "ax3.set_xlabel('Surface reflectance')\n",
    "ax3.set_ylabel('Pixel counts')\n",
    "ax3.grid()\n",
    "ax3.legend(loc='right')\n",
    "ax5.hist(ndsi1.flatten(), bins=100)\n",
    "ax5.set_xlabel('NDSI')\n",
    "ax5.set_ylabel('Pixel counts')\n",
    "ax5.grid()\n",
    "# Image 2\n",
    "ax2.imshow(np.dstack([r2, g2, b2]), \n",
    "           extent=(np.min(im2_x), np.max(im2_x), np.min(im2_y), np.max(im2_y)))\n",
    "data_snow_pts2.plot(ax=ax2, markersize=15, color='cyan', label='snow')\n",
    "data_non_snow_pts2.plot(ax=ax2, markersize=15, color='orange', label='non-snow')\n",
    "ax2.legend(loc='lower right')\n",
    "ax2.set_xlabel('Easting [m]')\n",
    "ax2.set_title('2021-08-15')\n",
    "ax4.hist(b2[b2>0].flatten(), color='blue', histtype='step', linewidth=2, bins=100, label='blue')\n",
    "ax4.hist(g2[g2>0].flatten(), color='green', histtype='step', linewidth=2, bins=100, label='green')\n",
    "ax4.hist(r2[r2>0].flatten(), color='red', histtype='step', linewidth=2, bins=100, label='red')\n",
    "ax4.hist(nir2[nir2>0].flatten(), color='brown', histtype='step', linewidth=2, bins=100, label='NIR')\n",
    "ax4.set_xlabel('Surface reflectance')\n",
    "ax4.grid()\n",
    "ax6.hist(ndsi2.flatten(), bins=100)\n",
    "ax6.set_xlabel('NDSI')\n",
    "ax6.grid()\n",
    "plt.show()\n",
    "\n",
    "# -----Save figure\n",
    "if save_outputs==True:\n",
    "    fig.savefig(figures_out_path+'training_data.png', dpi=200, facecolor='white', edgecolor='none')\n",
    "    print('figure saved to file')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e87403",
   "metadata": {},
   "source": [
    "### Set up training data\n",
    "Add 'snow' classification column, merge snow and non-snow points, sample band values at points, and add NDSI column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b929942",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -----Add date and snow classification column to data points\n",
    "im1_date = im1_fn[0:4]+'-'+im1_fn[4:6]+'-'+im1_fn[6:8]\n",
    "data_snow_pts1['date'] = im1_date\n",
    "data_snow_pts1['snow'] = 1\n",
    "data_non_snow_pts1['date'] = im1_date\n",
    "data_non_snow_pts1['snow'] = 0\n",
    "im2_date = im2_fn[0:4]+'-'+im2_fn[4:6]+'-'+im2_fn[6:8]\n",
    "data_snow_pts2['date'] = im2_date\n",
    "data_snow_pts2['snow'] = 1\n",
    "data_non_snow_pts2['date'] = im2_date\n",
    "data_non_snow_pts2['snow'] = 0\n",
    "\n",
    "# -----Merge snow and non-snow points\n",
    "data_pts = data_snow_pts1.append(data_non_snow_pts1, ignore_index=True).append(data_snow_pts2, ignore_index=True).append(data_non_snow_pts2, ignore_index=True)\n",
    "# Add coords column\n",
    "data_pts['coords'] = [(pt.bounds[0], pt.bounds[1]) for pt in data_pts['geometry']]\n",
    "# remove \"id\" and \"geometry\" columns\n",
    "data_pts = data_pts.drop(columns=['id', 'geometry'])\n",
    "\n",
    "# -----Sample band values at points\n",
    "data_pts['blue'] = ' '\n",
    "data_pts['green'] = ' '\n",
    "data_pts['red'] = ' '\n",
    "data_pts['NIR'] = ' '\n",
    "data_pts['blue'].loc[data_pts['date']==im1_date] = [x[0] for x in im1.sample(data_pts['coords'].loc[data_pts['date']==im1_date])]\n",
    "data_pts['green'].loc[data_pts['date']==im1_date] = [x[1] for x in im1.sample(data_pts['coords'].loc[data_pts['date']==im1_date])]\n",
    "data_pts['red'].loc[data_pts['date']==im1_date] = [x[2] for x in im1.sample(data_pts['coords'].loc[data_pts['date']==im1_date])]\n",
    "data_pts['NIR'].loc[data_pts['date']==im1_date] = [x[3] for x in im1.sample(data_pts['coords'].loc[data_pts['date']==im1_date])]\n",
    "data_pts['blue'].loc[data_pts['date']==im2_date] = [x[0] for x in im2.sample(data_pts['coords'].loc[data_pts['date']==im2_date])]\n",
    "data_pts['green'].loc[data_pts['date']==im2_date] = [x[1] for x in im2.sample(data_pts['coords'].loc[data_pts['date']==im2_date])]\n",
    "data_pts['red'].loc[data_pts['date']==im2_date] = [x[2] for x in im2.sample(data_pts['coords'].loc[data_pts['date']==im2_date])]\n",
    "data_pts['NIR'].loc[data_pts['date']==im2_date] = [x[3] for x in im2.sample(data_pts['coords'].loc[data_pts['date']==im2_date])]\n",
    "\n",
    "# -----Add NDSI column\n",
    "data_pts['NDSI'] = (data_pts['red'] - data_pts['NIR']) / (data_pts['red'] + data_pts['NIR'])\n",
    "\n",
    "print(data_pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3cc6b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Test supervised classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2940d4f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -----Start timer\n",
    "t1 = time.time() \n",
    "\n",
    "# -----Split data points into features (band values) and target variable (snow)\n",
    "feature_cols = ['blue', 'green', 'red', 'NIR', 'NDSI']\n",
    "X = data_pts[feature_cols] # features\n",
    "y = data_pts['snow'] # target variable\n",
    "\n",
    "# -----Split data points into testing and training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# -----Classifier names\n",
    "names = [\n",
    "#     \"Gaussian Process\",\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Neural Net\",\n",
    "#     \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "    \"Logistic Regression\"\n",
    "]\n",
    "\n",
    "# -----Classifiers\n",
    "classifiers = [\n",
    "#     GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "#     AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    LogisticRegression(random_state = 0)\n",
    "]\n",
    "    \n",
    "# -----Iterate over classifiers\n",
    "i = 0 # loop counter\n",
    "accuracy = [] # classifier accuracy\n",
    "for name, clf in zip(names, classifiers):\n",
    "    \n",
    "    # train classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy + [metrics.accuracy_score(y_test, y_pred)]\n",
    "    \n",
    "    # -----Predict snow classification for the full images\n",
    "    plot_output = False\n",
    "    snow1 = classify_image(im1, clf, feature_cols, plot_output)[2]\n",
    "    snow2 = classify_image(im2, clf, feature_cols, plot_output)[2]\n",
    "    # plot results\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 12))\n",
    "    plt.rcParams.update({'font.size': 14, 'font.sans-serif': 'Arial'})\n",
    "    # Image 1\n",
    "    ax1.imshow(np.dstack([r1, g1, b1]), \n",
    "               extent=(np.min(im1_x)/1000, np.max(im1_x)/1000, np.min(im1_y)/1000, np.max(im1_y)/1000))\n",
    "    ax1.set_title(im1_date+' RGB Image')\n",
    "    ax1.set_ylabel('Northing [km]')\n",
    "    ax2.imshow(np.where(snow1==1, 1, np.nan), cmap='Blues', clim=(0,1.2),\n",
    "               extent=(np.min(im1_x)/1000, np.max(im1_x)/1000, np.min(im1_y)/1000, np.max(im1_y)/1000))\n",
    "    ax2.imshow(np.where(snow1==0, 0, np.nan), cmap='Oranges', clim=(-1, 2),\n",
    "               extent=(np.min(im1_x)/1000, np.max(im1_x)/1000, np.min(im1_y)/1000, np.max(im1_y)/1000))\n",
    "    ax2.set_title('Predicted snow')\n",
    "    # Image 2\n",
    "    ax3.imshow(np.dstack([r2, g2, b2]), \n",
    "               extent=(np.min(im2_x)/1000, np.max(im2_x)/1000, np.min(im2_y)/1000, np.max(im2_y)/1000))\n",
    "    ax3.set_title(im2_date+' RGB Image')\n",
    "    ax3.set_ylabel('Northing [km]')\n",
    "    ax3.set_xlabel('Easting [km]')\n",
    "    ax4.imshow(np.where(snow2==1, 1, np.nan), cmap='Blues', clim=(0,1.2),\n",
    "           extent=(np.min(im2_x)/1000, np.max(im2_x)/1000, np.min(im2_y)/1000, np.max(im2_y)/1000))\n",
    "    ax4.imshow(np.where(snow2==0, 0, np.nan), cmap='Oranges', clim=(-1, 2),\n",
    "               extent=(np.min(im2_x)/1000, np.max(im2_x)/1000, np.min(im2_y)/1000, np.max(im2_y)/1000))\n",
    "    ax4.set_title('Predicted snow')\n",
    "    ax4.set_xlabel('Easting [km]')\n",
    "    fig.suptitle(name + ' | Accuracy: ' + str(np.round(accuracy[i]*100,2)))\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    # save figure\n",
    "    if save_outputs:\n",
    "        fig.savefig(figures_out_path + name.replace(' ','') + '_results.png', \n",
    "                    dpi=200, facecolor='white', edgecolor='none')\n",
    "        print('figure saved to file')\n",
    "    \n",
    "    # display time elapsed\n",
    "    print('Time elapsed: ',str(np.round((time.time()-t1)/60, 2)),' minutes')\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "# -----Determine best classifier based on accuracy\n",
    "results = pd.DataFrame()\n",
    "results['Classifier'], results['Accuracy'] = names, accuracy\n",
    "clf_best_name = names[np.where(accuracy==np.max(accuracy))[0][0]]\n",
    "clf_best = classifiers[np.where(accuracy==np.max(accuracy))[0][0]]\n",
    "print(results)\n",
    "print('')\n",
    "print('Best accuracy classifier: ' + clf_best_name)\n",
    "    \n",
    "# -----Display time elapsed\n",
    "print('')\n",
    "print('Time elapsed: ',str(np.round((time.time()-t1)/60, 2)),' minutes')\n",
    "\n",
    "# -----Save most accurate classifier\n",
    "if save_outputs==True:\n",
    "    clf_fn = out_path+site_ID+'_best_classifier.sav'\n",
    "    pickle.dump(clf_best, open(clf_fn, 'wb'))\n",
    "    print('Most accurate classifier saved to file: ',clf_fn)\n",
    "    feature_cols_fn = out_path+site_ID+'_best_classifier_feature_cols.pkl'\n",
    "    pickle.dump(feature_cols, open(feature_cols_fn, 'wb'))\n",
    "    print('Feature columns saved to file: ',feature_cols_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10187df6",
   "metadata": {},
   "source": [
    "### Test unsupervised classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e6c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----KMeans\n",
    "# image 1\n",
    "I1_real = ~np.isnan(b1)\n",
    "im1_x_mesh, im1_y_mesh = np.meshgrid(im1_x, im1_y)\n",
    "im1_x_real = im1_x_mesh[I1_real]\n",
    "im1_y_real = im1_y_mesh[I1_real]\n",
    "b1_real = b1[I1_real].flatten()\n",
    "g1_real = g1[I1_real].flatten()\n",
    "r1_real = r1[I1_real].flatten()\n",
    "nir1_real = nir1[I1_real].flatten()\n",
    "ndsi1_real = ((r1_real - nir1_real)/(r1_real + nir1_real))\n",
    "X1 = np.column_stack((b1_real, g1_real, r1_real, nir1_real, ndsi1_real))\n",
    "\n",
    "# image 2\n",
    "I2_real = ~np.isnan(b2)\n",
    "im2_x_mesh, im2_y_mesh = np.meshgrid(im2_x, im2_y)\n",
    "im2_x_real = im2_x_mesh[I2_real]\n",
    "im2_y_real = im2_y_mesh[I2_real]\n",
    "b2_real = b2[I2_real].flatten()\n",
    "g2_real = g2[I2_real].flatten()\n",
    "r2_real = r2[I2_real].flatten()\n",
    "nir2_real = nir2[I2_real].flatten()\n",
    "ndsi2_real = ((r2_real - nir2_real)/(r2_real + nir2_real))\n",
    "X2 = np.column_stack((b2_real, g2_real, r2_real, nir2_real, ndsi2_real))\n",
    "\n",
    "# generate classifier and classify images\n",
    "n = 3 # number of clusters\n",
    "Y1 = KMeans(n_clusters=n).fit(X1)\n",
    "labels1 = Y1.labels_\n",
    "Y2 = KMeans(n_clusters=n).fit(X2)\n",
    "labels2 = Y2.labels_\n",
    "\n",
    "# reshape from flat array to original shape\n",
    "clusters1 = np.zeros((np.shape(b1)[0], np.shape(b1)[1]))\n",
    "clusters1[:] = np.nan\n",
    "clusters1[I1_real] = labels1\n",
    "clusters2 = np.zeros((np.shape(b2)[0], np.shape(b2)[1]))\n",
    "clusters2[:] = np.nan\n",
    "clusters2[I2_real] = labels2\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12,12))\n",
    "ax[0,0].imshow(np.dstack([r1, g1, b1]))\n",
    "ax[0,1].imshow(clusters1)\n",
    "ax[1,0].imshow(np.dstack([r2, g2, b2]))\n",
    "ax[1,1].imshow(clusters2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0973a4",
   "metadata": {},
   "source": [
    "### Test thresholding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab793867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Define possible thresholds\n",
    "# r_min_thresholds = np.linspace(0.4, 0.6, num=10)\n",
    "# ndsi_min_thresholds = np.linspace(-0.1, 0.1, num=10)\n",
    "# ndsi_max_thresholds = np.linspace(0.15, 0.6, num=10)\n",
    "\n",
    "# # -----Loop through possible thresholds\n",
    "# accuracy = np.zeros((len(r_min_thresholds), len(ndsi_min_thresholds), len(ndsi_max_thresholds))) # initialize accuracy\n",
    "# i, j, k = 0, 0, 0 # loop counters\n",
    "# for r_min_thresh in r_min_thresholds:\n",
    "#     j=0\n",
    "#     for ndsi_min_thresh in ndsi_min_thresholds:\n",
    "#         k=0\n",
    "#         for ndsi_max_thresh in ndsi_max_thresholds:\n",
    "#             # apply threshold to images\n",
    "#             snow1 = np.where((r1 > r_min_thresh) & (ndsi1 > ndsi_min_thresh) & (ndsi1 < ndsi_max_thresh), 1, 0)\n",
    "#             snow2 = np.where((r2 > r_min_thresh) & (ndsi2 > ndsi_min_thresh) & (ndsi2 < ndsi_max_thresh), 1, 0)\n",
    "\n",
    "#             # calculate accuracy using data points\n",
    "#             data_pts['snow_pred'] = 0\n",
    "#             data_pts['snow_pred'].loc[(data_pts['red'] > r_min_thresh) \n",
    "#                                        & (data_pts['NDSI'] > ndsi_min_thresh) & (data_pts['NDSI'] < ndsi_max_thresh)] = 1\n",
    "#             accuracy[i][j][k] = len(np.where(data_pts['snow_pred']==data_pts['snow'])[0]) / len(data_pts['snow']) \n",
    "#             k+=1\n",
    "#         j+=1\n",
    "#     i+=1\n",
    "            \n",
    "# r_min_thresh_best = r_min_thresholds[np.where(accuracy==np.max(accuracy))[0][0]]\n",
    "# ndsi_min_thresh_best = ndsi_min_thresholds[np.where(accuracy==np.max(accuracy))[0][1]]\n",
    "# ndsi_max_thresh_best = ndsi_max_thresholds[np.where(accuracy==np.max(accuracy))[0][2]]\n",
    "# print('Best red minimum threshold:',r_min_thresh_best)\n",
    "# print('Best NDSI minimum threshold:',ndsi_min_thresh_best)\n",
    "# print('Best NDSI maximum threshold:',ndsi_max_thresh_best)\n",
    "\n",
    "# # r_min_thresh_best = 0.4\n",
    "# # ndsi_min_thresh_best = 0.0\n",
    "# # ndsi_max_thresh_best = 0.15\n",
    "\n",
    "# # plot results\n",
    "# fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(24, 8))\n",
    "# plt.rcParams.update({'font.size': 14, 'font.sans-serif': 'Arial'})\n",
    "# # 2021-04-21\n",
    "# ax1.imshow(np.dstack([r1, g1, b1]), \n",
    "#            extent=(np.min(im1_x)/1000, np.max(im1_x)/1000, np.min(im1_y)/1000, np.max(im1_y)/1000))\n",
    "# ax1.set_title('2021-04-21: RGB Image')\n",
    "# ax1.set_ylabel('Northing [km]')\n",
    "# ax1.set_xlabel('Easting [km]')\n",
    "# ax2.imshow(np.where((r1 > r_min_thresh_best) & (ndsi1 > ndsi_min_thresh_best) & (ndsi1 < ndsi_max_thresh_best), 1, np.nan),\n",
    "#            cmap='Blues', clim=(0,1),\n",
    "#            extent=(np.min(im1_x)/1000, np.max(im1_x)/1000, np.min(im1_y)/1000, np.max(im1_y)/1000))\n",
    "# ax2.set_title('Predicted snow')\n",
    "# ax2.set_xlabel('Easting [km]')\n",
    "# # 2021-08-15\n",
    "# ax3.imshow(np.dstack([r2, g2, b2]), \n",
    "#            extent=(np.min(im2_x)/1000, np.max(im2_x)/1000, np.min(im2_y)/1000, np.max(im2_y)/1000))\n",
    "# ax3.set_title('2021-08-15: RGB Image')\n",
    "# ax3.set_ylabel('Northing [km]')\n",
    "# ax3.set_xlabel('Easting [km]')\n",
    "# snow2_plot = ax4.imshow(np.where((r2 > r_min_thresh_best) & (ndsi2 > ndsi_min_thresh_best) & (ndsi2 < ndsi_max_thresh_best), 1, np.nan), \n",
    "#                         cmap='Blues', clim=(0,1),\n",
    "#                         extent=(np.min(im2_x)/1000, np.max(im2_x)/1000, np.min(im2_y)/1000, np.max(im2_y)/1000))\n",
    "# ax4.set_title('Predicted snow')\n",
    "# ax4.set_xlabel('Easting [km]')\n",
    "# fig.colorbar(snow2_plot, ax=ax4, shrink=0.5)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29752be0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
