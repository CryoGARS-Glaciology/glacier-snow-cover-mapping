{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c53b019d",
   "metadata": {},
   "source": [
    "## Notebook to develop supervised classification algorithm for identifying snow in PlanetScope 4-band imagery\n",
    "Rainey Aberle\n",
    "\n",
    "Adapted from the [SciKit Learn Classifier comparison tutorial](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n",
    "\n",
    "### Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3837b29f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -----Import packages\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import time\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ed447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Determine whether to save outputs to file\n",
    "save_outputs = True # = True to save output figures and best classifier \n",
    "\n",
    "# -----Define paths in directory\n",
    "# base directory (path to planet-snow/)\n",
    "base_path = '/Users/raineyaberle/Research/PhD/Planet_snow_cover/planet-snow/'\n",
    "# output folder for best classifier\n",
    "out_path = base_path+'inputs-outputs/'\n",
    "# path to classified points used to train and test classifiers\n",
    "data_pts_path = base_path+'../classified-points/'\n",
    "\n",
    "# -----Add path to functions\n",
    "sys.path.insert(1, base_path+'functions/')\n",
    "from classification_utils import crop_images_to_AOI, classify_image, calculate_SCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb69ae3f",
   "metadata": {},
   "source": [
    "### Define supervised classification algorithms to test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b920160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Classifier names\n",
    "names = [\n",
    "#     \"Gaussian Process\", # keeps crashing kernel when classifying images!\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "    \"Logistic Regression\"\n",
    "]\n",
    "\n",
    "# -----Classifiers\n",
    "classifiers = [\n",
    "#     GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    LogisticRegression(random_state = 0)\n",
    "]\n",
    "\n",
    "# -----Classified points\n",
    "os.chdir(data_pts_path)\n",
    "data_pts_fns = glob.glob('*.shp')\n",
    "data_pts_fns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3640589b",
   "metadata": {},
   "source": [
    "### Test site-specific classifiers\n",
    "\n",
    "Use classified points at each site to determine the best site-specific classifier, calculate accuracy\n",
    "\n",
    "Classified points split: 80% training, 20% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdb0425",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -----Define site IDs\n",
    "# Wolverine (WO), SitKusa (SK), Mendenhall (ME), Easton (EA), Blue (BL), Emmons (EM)\n",
    "site_IDs = ['WO', 'SK', 'ME', 'EA']#, 'BL', 'EM']\n",
    "site_names = ['Wolverine', 'SitKusa', 'Mendenhall', 'Easton']#, 'Blue', 'Emmons']\n",
    "\n",
    "# -----Initialize data_pts for single classifier (next step)\n",
    "data_pts_full = gpd.GeoDataFrame()\n",
    "\n",
    "# -----Loop through sites\n",
    "for i in range(len(site_names)):\n",
    "    \n",
    "    print('----------')\n",
    "    print(site_names[i])\n",
    "\n",
    "    # -----Set up training data\n",
    "    # load classified points\n",
    "    # snow\n",
    "    data_pts_snow_fn = [x for x in data_pts_fns if (site_IDs[i] in x) and ('snow.shp' in x)][0]\n",
    "    data_pts_snow = gpd.read_file(data_pts_path + data_pts_snow_fn) \n",
    "    data_pts_snow['class'] = 1\n",
    "    # ice\n",
    "    data_pts_ice_fn = [x for x in data_pts_fns if (site_IDs[i] in x) and ('ice.shp' in x)][0]\n",
    "    data_pts_ice = gpd.read_file(data_pts_path + data_pts_ice_fn)\n",
    "    data_pts_ice['class'] = 2\n",
    "    # rock\n",
    "    data_pts_rock_fn = [x for x in data_pts_fns if (site_IDs[i] in x) and ('rock.shp' in x)][0]\n",
    "    data_pts_rock = gpd.read_file(data_pts_path + data_pts_rock_fn)   \n",
    "    data_pts_rock['class'] = 3\n",
    "    # shadowed snow\n",
    "    if site_names[i]!='Easton':\n",
    "        data_pts_snow_sh_fn = [x for x in data_pts_fns if (site_IDs[i] in x) and ('snow-shadowed.shp' in x)][0]\n",
    "        data_pts_snow_sh = gpd.read_file(data_pts_path + data_pts_snow_sh_fn)\n",
    "        data_pts_snow_sh['class'] = 4\n",
    "        # merge data points\n",
    "        data_pts = pd.concat([data_pts_snow, data_pts_ice, data_pts_rock, data_pts_snow_sh], ignore_index=True)\n",
    "    else:\n",
    "        # merge data points\n",
    "        data_pts = pd.concat([data_pts_snow, data_pts_ice, data_pts_rock], ignore_index=True)\n",
    "    \n",
    "    # -----Load image\n",
    "    im_path = base_path+'../study-sites/'+site_names[i]+'/imagery/Planet/adjusted-filtered/'\n",
    "    im_fn = data_pts_snow_fn[3:14]+'_adj.tif' # image file name\n",
    "    im_date = im_fn[0:4]+'-'+im_fn[4:6]+'-'+im_fn[6:8] # image capture date\n",
    "    im = rio.open(im_path+im_fn) # open image\n",
    "    epsg = str(im.crs)[5:] # grab EPSG code\n",
    "    # read bands\n",
    "    b = im.read(1).astype(float) \n",
    "    r = im.read(2).astype(float) \n",
    "    g = im.read(3).astype(float) \n",
    "    nir = im.read(4).astype(float) \n",
    "    # divide by image scalar if max band values > 1000\n",
    "    if np.nanmax(b) > 1e3:\n",
    "        apply_scalar = True\n",
    "        im_scalar = 10000\n",
    "        b = b / im_scalar\n",
    "        g = g / im_scalar\n",
    "        r = r / im_scalar\n",
    "        nir = nir / im_scalar\n",
    "    else:\n",
    "        apply_scalar = False\n",
    "    # calculate NDSI\n",
    "    ndsi = (r - nir) / (r + nir)\n",
    "    # define coordinates grid\n",
    "    im_x = np.linspace(im.bounds.left, im.bounds.right, num=np.shape(b)[1])\n",
    "    im_y = np.linspace(im.bounds.top, im.bounds.bottom, num=np.shape(b)[0])\n",
    "    \n",
    "    # -----Reformat data points coordinates\n",
    "    # reproject to image epsg\n",
    "    data_pts = data_pts.to_crs(epsg) \n",
    "    # remove \"id\" column\n",
    "    data_pts = data_pts.drop(columns=['id'])\n",
    "    # remove rows containing NaN\n",
    "    data_pts = data_pts.dropna()\n",
    "    # add coords column\n",
    "    data_pts['coords'] = [(pt.bounds[0], pt.bounds[1]) for pt in data_pts['geometry']]\n",
    "    # sample band values at points\n",
    "    data_pts['blue'] = [x[0] for x in im.sample(data_pts['coords'])]\n",
    "    data_pts['green'] = [x[1] for x in im.sample(data_pts['coords'])]\n",
    "    data_pts['red'] = [x[2] for x in im.sample(data_pts['coords'])]\n",
    "    data_pts['NIR'] = [x[3] for x in im.sample(data_pts['coords'])]\n",
    "    # divide values by im_scalar if applicable\n",
    "    if apply_scalar:\n",
    "        data_pts['blue'] = data_pts['blue'].div(im_scalar)\n",
    "        data_pts['green'] = data_pts['green'].div(im_scalar)\n",
    "        data_pts['red'] = data_pts['red'].div(im_scalar)\n",
    "        data_pts['NIR'] = data_pts['NIR'].div(im_scalar)\n",
    "    # add NDSI column\n",
    "    data_pts['NDSI'] = (data_pts['red'] - data_pts['NIR']) / (data_pts['red'] + data_pts['NIR'])\n",
    "    # add data points to full data points DF\n",
    "    data_pts_full = pd.concat([data_pts_full, data_pts], ignore_index=True)\n",
    "    \n",
    "    # -----Plot RGB images, data point locations, and band histograms\n",
    "#     print('Training data:')\n",
    "#     fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(8,16), gridspec_kw={'height_ratios': [4, 1, 1]})\n",
    "#     plt.rcParams.update({'font.size': 14, 'font.sans-serif': 'Arial'})\n",
    "#     # Image 1\n",
    "#     ax1.imshow(np.dstack([r, g, b]), \n",
    "#                extent=(np.min(im_x), np.max(im_x), np.min(im_y), np.max(im_y)))\n",
    "#     data_pts.loc[data_pts['class']==1].plot(ax=ax1, markersize=15, color='cyan', label='snow')\n",
    "#     data_pts.loc[data_pts['class']==2].plot(ax=ax1, markersize=15, color='blue', label='ice')\n",
    "#     data_pts.loc[data_pts['class']==3].plot(ax=ax1, markersize=15, color='orange', label='rock')\n",
    "#     data_pts.loc[data_pts['class']==4].plot(ax=ax1, markersize=15, color='grey', label='snow')\n",
    "#     ax1.legend(loc='lower right')\n",
    "#     ax1.set_xlabel('Easting [m]')\n",
    "#     ax1.set_ylabel('Northing [m]')\n",
    "#     ax1.set_title(im_date)\n",
    "#     ax2.hist(b[b>0].flatten(), color='blue', histtype='step', linewidth=2, bins=100, label='blue')\n",
    "#     ax2.hist(g[g>0].flatten(), color='green', histtype='step', linewidth=2, bins=100, label='green')\n",
    "#     ax2.hist(r[r>0].flatten(), color='red', histtype='step', linewidth=2, bins=100, label='red')\n",
    "#     ax2.hist(nir[nir>0].flatten(), color='brown', histtype='step', linewidth=2, bins=100, label='NIR')\n",
    "#     ax2.set_xlabel('Surface reflectance')\n",
    "#     ax2.set_ylabel('Pixel counts')\n",
    "#     ax2.grid()\n",
    "#     ax3.legend(loc='right')\n",
    "#     ax3.hist(ndsi.flatten(), bins=100)\n",
    "#     ax3.set_xlabel('NDSI')\n",
    "#     ax3.set_ylabel('Pixel counts')\n",
    "#     ax3.grid()\n",
    "#     plt.show()\n",
    "        \n",
    "    # -----Test supervised classification algorithms\n",
    "    # Split data points into features (band values) and target variable (snow)\n",
    "    feature_cols = ['blue', 'green', 'red', 'NIR', 'NDSI']\n",
    "    X = data_pts[feature_cols] # features\n",
    "    y = data_pts['class'] # target variable\n",
    "\n",
    "    # Split data points into testing and training\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Iterate over classifiers\n",
    "    j = 0 # loop counter\n",
    "    accuracy = [] # classifier overall accuracy\n",
    "    K = [] # classifier kappa score\n",
    "    TP, TN, FP, FN = [], [], [], [] # confusion matrix components\n",
    "    for name, clf in zip(names, classifiers):\n",
    "            \n",
    "        # train classifier\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Calculate statistics\n",
    "        # overall accuracy\n",
    "        y_pred = clf.predict(X_test)\n",
    "        accuracy = accuracy + [metrics.accuracy_score(y_test, y_pred)]\n",
    "        # Kappa score\n",
    "        K = K + [metrics.cohen_kappa_score(y_test, y_pred)]\n",
    "        # confusion matrix\n",
    "        TP = TP + [np.where((y_test==1) & (y_pred==1))[0].size]\n",
    "        TN = TN + [np.where((y_test!=1) & (y_pred!=1))[0].size]\n",
    "        FP = FP + [np.where((y_test!=1) & (y_pred==1))[0].size]\n",
    "        FN = FN + [np.where((y_test==1) & (y_pred!=1))[0].size] \n",
    "    \n",
    "        j+=1\n",
    "\n",
    "    # Determine best classifier based on accuracy\n",
    "    results = pd.DataFrame()\n",
    "    results['Classifier'], results['Accuracy'], results['Kappa_score'] = names, accuracy, K\n",
    "    results['TP'], results['TN'], results['FP'], results['FN'] = TP, TN, FP, FN\n",
    "    clf_best_name = names[np.where(accuracy==np.max(accuracy))[0][0]]\n",
    "    clf_best = classifiers[np.where(accuracy==np.max(accuracy))[0][0]]\n",
    "    print(results)\n",
    "    print('')\n",
    "    print('Best accuracy classifier: ' + clf_best_name)\n",
    "\n",
    "    # -----Save most accurate classifier\n",
    "    if save_outputs==True:\n",
    "        clf_fn = out_path+site_IDs[i]+'_classifier.sav'\n",
    "        pickle.dump(clf_best, open(clf_fn, 'wb'))\n",
    "        print('Most accurate classifier saved to file: ',clf_fn)\n",
    "        feature_cols_fn = out_path+site_IDs[i]+'_classifier_feature_cols.pkl'\n",
    "        pickle.dump(feature_cols, open(feature_cols_fn, 'wb'))\n",
    "        print('Feature columns saved to file: ',feature_cols_fn)\n",
    "\n",
    "# -----Plot spectral information of classes\n",
    "fig1, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "plt.rcParams.update({'font.size': 14, 'font.sans-serif': 'Arial'})\n",
    "ax1.scatter(1*np.ones(len(data_pts_full.loc[data_pts_full['class']==1]['blue'])), data_pts_full.loc[data_pts_full['class']==1]['blue'],\n",
    "         s=10, color='cyan', label='snow')\n",
    "ax1.scatter(2*np.ones(len(data_pts_full.loc[data_pts_full['class']==2]['blue'])), data_pts_full.loc[data_pts_full['class']==2]['blue'], \n",
    "         s=10, color='blue', label='ice')\n",
    "ax1.scatter(3*np.ones(len(data_pts_full.loc[data_pts_full['class']==3]['blue'])), data_pts_full.loc[data_pts_full['class']==3]['blue'],  \n",
    "         s=10, color='orange', label='rock')\n",
    "ax1.scatter(4*np.ones(len(data_pts_full.loc[data_pts_full['class']==4]['blue'])), data_pts_full.loc[data_pts_full['class']==4]['blue'], \n",
    "         s=10, color='grey', label='shadowed snow')\n",
    "ax1.grid()\n",
    "ax1.set_title('Blue')\n",
    "ax2.scatter(1*np.ones(len(data_pts_full.loc[data_pts_full['class']==1]['green'])), data_pts_full.loc[data_pts_full['class']==1]['green'],\n",
    "         s=10, color='cyan', label='snow')\n",
    "ax2.scatter(2*np.ones(len(data_pts_full.loc[data_pts_full['class']==2]['green'])), data_pts_full.loc[data_pts_full['class']==2]['green'], \n",
    "         s=10, color='blue', label='ice')\n",
    "ax2.scatter(3*np.ones(len(data_pts_full.loc[data_pts_full['class']==3]['green'])), data_pts_full.loc[data_pts_full['class']==3]['green'],  \n",
    "         s=10, color='orange', label='rock')\n",
    "ax2.scatter(4*np.ones(len(data_pts_full.loc[data_pts_full['class']==4]['green'])), data_pts_full.loc[data_pts_full['class']==4]['green'], \n",
    "         s=10, color='grey', label='shadowed snow')\n",
    "ax2.grid()\n",
    "ax2.set_title('Green')\n",
    "ax3.scatter(1*np.ones(len(data_pts_full.loc[data_pts_full['class']==1]['red'])), data_pts_full.loc[data_pts_full['class']==1]['red'],\n",
    "         s=10, color='cyan', label='snow')\n",
    "ax3.scatter(2*np.ones(len(data_pts_full.loc[data_pts_full['class']==2]['red'])), data_pts_full.loc[data_pts_full['class']==2]['red'], \n",
    "         s=10, color='blue', label='ice')\n",
    "ax3.scatter(3*np.ones(len(data_pts_full.loc[data_pts_full['class']==3]['red'])), data_pts_full.loc[data_pts_full['class']==3]['red'],  \n",
    "         s=10, color='orange', label='rock')\n",
    "ax3.scatter(4*np.ones(len(data_pts_full.loc[data_pts_full['class']==4]['red'])), data_pts_full.loc[data_pts_full['class']==4]['red'], \n",
    "         s=10, color='grey', label='shadowed snow')\n",
    "ax3.grid()\n",
    "ax3.set_title('Red')\n",
    "ax4.scatter(1*np.ones(len(data_pts_full.loc[data_pts_full['class']==1]['NIR'])), data_pts_full.loc[data_pts_full['class']==1]['NIR'],\n",
    "         s=10, color='cyan', label='snow')\n",
    "ax4.scatter(2*np.ones(len(data_pts_full.loc[data_pts_full['class']==2]['NIR'])), data_pts_full.loc[data_pts_full['class']==2]['NIR'], \n",
    "         s=10, color='blue', label='ice')\n",
    "ax4.scatter(3*np.ones(len(data_pts_full.loc[data_pts_full['class']==3]['NIR'])), data_pts_full.loc[data_pts_full['class']==3]['NIR'],  \n",
    "         s=10, color='orange', label='rock')\n",
    "ax4.scatter(4*np.ones(len(data_pts_full.loc[data_pts_full['class']==4]['NIR'])), data_pts_full.loc[data_pts_full['class']==4]['NIR'], \n",
    "         s=10, color='grey', label='shadowed snow')\n",
    "ax4.grid()\n",
    "ax4.set_title('NIR')\n",
    "\n",
    "fig2, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "ax.scatter(1*np.ones(len(data_pts_full.loc[data_pts_full['class']==1]['NDSI'])), data_pts_full.loc[data_pts_full['class']==1]['NDSI'],\n",
    "         s=10, color='cyan', label='snow')\n",
    "ax.scatter(2*np.ones(len(data_pts_full.loc[data_pts_full['class']==2]['NDSI'])), data_pts_full.loc[data_pts_full['class']==2]['NDSI'], \n",
    "         s=10, color='blue', label='ice')\n",
    "ax.scatter(3*np.ones(len(data_pts_full.loc[data_pts_full['class']==3]['NDSI'])), data_pts_full.loc[data_pts_full['class']==3]['NDSI'],  \n",
    "         s=10, color='orange', label='rock')\n",
    "ax.scatter(4*np.ones(len(data_pts_full.loc[data_pts_full['class']==4]['NDSI'])), data_pts_full.loc[data_pts_full['class']==4]['NDSI'], \n",
    "         s=10, color='grey', label='shadowed snow')\n",
    "ax.grid()\n",
    "ax.set_title('NDSI')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e177566d",
   "metadata": {},
   "source": [
    "### Test one classifier for all sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa7204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Test supervised classification algorithms\n",
    "# Split data points into features (band values) and target variable (snow)\n",
    "feature_cols = ['blue', 'green', 'red', 'NIR', 'NDSI']\n",
    "X = data_pts_full[feature_cols] # features\n",
    "y = data_pts_full['class'] # target variable\n",
    "\n",
    "# Split data points into testing and training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Iterate over classifiers\n",
    "j = 0 # loop counter\n",
    "accuracy = [] # classifier overall accuracy\n",
    "K = [] # classifier kappa score\n",
    "TP, TN, FP, FN = [], [], [], [] # confusion matrix components\n",
    "for name, clf in zip(names, classifiers):\n",
    "\n",
    "    # train classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate statistics\n",
    "    # overall accuracy\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy + [metrics.accuracy_score(y_test, y_pred)]\n",
    "    # Kappa score\n",
    "    K = K + [metrics.cohen_kappa_score(y_test, y_pred)]\n",
    "    # confusion matrix\n",
    "    TP = TP + [np.where((y_test==1) & (y_pred==1))[0].size]\n",
    "    TN = TN + [np.where((y_test==0) & (y_pred==0))[0].size]\n",
    "    FP = FP + [np.where((y_test==0) & (y_pred==1))[0].size]\n",
    "    FN = FN + [np.where((y_test==1) & (y_pred==0))[0].size] \n",
    "\n",
    "    j+=1\n",
    "\n",
    "# Determine best classifier based on accuracy\n",
    "results = pd.DataFrame()\n",
    "results['Classifier'], results['Accuracy'], results['Kappa_score'] = names, accuracy, K\n",
    "results['TP'], results['TN'], results['FP'], results['FN'] = TP, TN, FP, FN\n",
    "clf_best_name = names[np.where(accuracy==np.max(accuracy))[0][0]]\n",
    "clf_best = classifiers[np.where(accuracy==np.max(accuracy))[0][0]]\n",
    "print(results)\n",
    "print('')\n",
    "print('Best accuracy classifier: ' + clf_best_name)\n",
    "\n",
    "# -----Save most accurate classifier\n",
    "if save_outputs==True:\n",
    "    clf_fn = out_path+'all_sites_classifier.sav'\n",
    "    pickle.dump(clf_best, open(clf_fn, 'wb'))\n",
    "    print('Most accurate classifier saved to file: ',clf_fn)\n",
    "    feature_cols_fn = out_path+'all_sites_classifier_feature_cols.pkl'\n",
    "    pickle.dump(feature_cols, open(feature_cols_fn, 'wb'))\n",
    "    print('Feature columns saved to file: ',feature_cols_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10187df6",
   "metadata": {},
   "source": [
    "### Test unsupervised classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e6c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----KMeans\n",
    "# image 1\n",
    "# I1_real = ~np.isnan(b1)\n",
    "# im1_x_mesh, im1_y_mesh = np.meshgrid(im1_x, im1_y)\n",
    "# im1_x_real = im1_x_mesh[I1_real]\n",
    "# im1_y_real = im1_y_mesh[I1_real]\n",
    "# b1_real = b1[I1_real].flatten()\n",
    "# g1_real = g1[I1_real].flatten()\n",
    "# r1_real = r1[I1_real].flatten()\n",
    "# nir1_real = nir1[I1_real].flatten()\n",
    "# ndsi1_real = ((r1_real - nir1_real)/(r1_real + nir1_real))\n",
    "# X1 = np.column_stack((b1_real, g1_real, r1_real, nir1_real, ndsi1_real))\n",
    "\n",
    "# # image 2\n",
    "# I2_real = ~np.isnan(b2)\n",
    "# im2_x_mesh, im2_y_mesh = np.meshgrid(im2_x, im2_y)\n",
    "# im2_x_real = im2_x_mesh[I2_real]\n",
    "# im2_y_real = im2_y_mesh[I2_real]\n",
    "# b2_real = b2[I2_real].flatten()\n",
    "# g2_real = g2[I2_real].flatten()\n",
    "# r2_real = r2[I2_real].flatten()\n",
    "# nir2_real = nir2[I2_real].flatten()\n",
    "# ndsi2_real = ((r2_real - nir2_real)/(r2_real + nir2_real))\n",
    "# X2 = np.column_stack((b2_real, g2_real, r2_real, nir2_real, ndsi2_real))\n",
    "\n",
    "# # generate classifier and classify images\n",
    "# n = 3 # number of clusters\n",
    "# Y1 = KMeans(n_clusters=n).fit(X1)\n",
    "# labels1 = Y1.labels_\n",
    "# Y2 = KMeans(n_clusters=n).fit(X2)\n",
    "# labels2 = Y2.labels_\n",
    "\n",
    "# # reshape from flat array to original shape\n",
    "# clusters1 = np.zeros((np.shape(b1)[0], np.shape(b1)[1]))\n",
    "# clusters1[:] = np.nan\n",
    "# clusters1[I1_real] = labels1\n",
    "# clusters2 = np.zeros((np.shape(b2)[0], np.shape(b2)[1]))\n",
    "# clusters2[:] = np.nan\n",
    "# clusters2[I2_real] = labels2\n",
    "\n",
    "# fig, ax = plt.subplots(2, 2, figsize=(12,12))\n",
    "# ax[0,0].imshow(np.dstack([r1, g1, b1]))\n",
    "# ax[0,1].imshow(clusters1)\n",
    "# ax[1,0].imshow(np.dstack([r2, g2, b2]))\n",
    "# ax[1,1].imshow(clusters2)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
