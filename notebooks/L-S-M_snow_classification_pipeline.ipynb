{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba08359f-4f07-4197-b84f-9d82a17e5246",
   "metadata": {},
   "source": [
    "# Classify snow-covered area (SCA) in Landsat, Sentinel-2, and MODIS surface reflectance imagery: full pipeline\n",
    "\n",
    "Rainey Aberle\n",
    "\n",
    "Department of Geosciences, Boise State University\n",
    "\n",
    "2022\n",
    "\n",
    "### Requirements:\n",
    "- Area of Interest (AOI) shapefile: where snow will be classified in all available images. \n",
    "- Google Earth Engine (GEE) account: used to pull DEM over the AOI. Sign up for a free account [here](https://earthengine.google.com/new_signup/). \n",
    "\n",
    "### Outline:\n",
    "__0. Setup__ paths in directory, AOI file location - _modify this section!_\n",
    "\n",
    "__1. Load images__ over the AOI since 2016. \n",
    "\n",
    "__2. Prepare image collections__ for classification: reprojection and image quality masking. \n",
    "\n",
    "__3. Classify SCA__ and use the snow elevations distribution to estimate the seasonal snowline\n",
    "\n",
    "-------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566a459b-0289-4488-953f-18f3c6462fff",
   "metadata": {},
   "source": [
    "### 0. Setup\n",
    "\n",
    "#### Define paths in directory and desired settings. \n",
    "Modify lines located within the following:\n",
    "\n",
    "`#### MODIFY HERE ####`  \n",
    "\n",
    "`#####################`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f331e9-ed10-4fa2-9b12-3ae384cbde17",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MODIFY HERE #####\n",
    "\n",
    "# -----Paths in directory\n",
    "site_name = 'Wolverine'\n",
    "# path to snow-cover-mapping/\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/'\n",
    "# path to AOI including the name of the shapefile\n",
    "AOI_fn = base_path + '../study-sites/' + site_name + '/glacier_outlines/' + site_name + '_USGS_*.shp'\n",
    "# path to DEM including the name of the tif file\n",
    "# Note: set DEM_fn=None if you want to use the ASTER GDEM on Google Earth Engine\n",
    "DEM_fn = base_path + '../study-sites/' + site_name + '/DEMs/' + site_name + '*_DEM_filled.tif'\n",
    "# path for output images\n",
    "out_path = base_path + '../study-sites/' + site_name + '/imagery/'\n",
    "# path for output figures\n",
    "figures_out_path = base_path + '../study-sites/' + site_name + '/figures/'\n",
    "\n",
    "# -----Determine settings\n",
    "plot_results = True # = True to plot figures of results for each image where applicable\n",
    "skip_clipped = False # = True to skip images where bands appear \"clipped\", i.e. max blue SR < 0.8\n",
    "crop_to_AOI = True # = True to crop images to AOI before calculating SCA\n",
    "save_outputs = True # = True to save SCA images to file\n",
    "save_figures = True # = True to save SCA output figures to file\n",
    "\n",
    "#######################\n",
    "\n",
    "# -----Import packages\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import wxee as wx\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from osgeo import gdal\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib import pyplot as plt, dates\n",
    "import rasterio as rio\n",
    "import rasterio.features\n",
    "from rasterio.mask import mask\n",
    "from rasterio.plot import show\n",
    "from shapely.geometry import Polygon, shape\n",
    "import shapely.geometry\n",
    "from scipy.interpolate import interp2d\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import geemap\n",
    "import math\n",
    "import sys\n",
    "import ee\n",
    "import fiona\n",
    "import pickle\n",
    "import wxee as wx\n",
    "import time\n",
    "\n",
    "# -----Add path to functions\n",
    "sys.path.insert(1, base_path+'functions/')\n",
    "import ps_pipeline_utils as f\n",
    "\n",
    "# -----Load dataset dictionary\n",
    "with open(base_path + 'inputs-outputs/datasets_characteristics.pkl', 'rb') as fn:\n",
    "    dataset_dict = pickle.load(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d19dfe9-0e44-44e7-b54e-3bd2391b7729",
   "metadata": {},
   "source": [
    "#### Authenticate and initialize Google Earth Engine (GEE). \n",
    "\n",
    "__Note:__ The first time you run the following cell, you will be asked to authenticate your GEE account for use in this notebook. This will send you to an external web page, where you will walk through the GEE authentication workflow and copy an authentication code back in this notebook when prompted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f87c0f-9ba4-4f34-8700-a23187894509",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ee.Initialize()\n",
    "except: \n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb2190f-f238-49a4-ad99-89576af423fb",
   "metadata": {},
   "source": [
    "#### Load AOI and DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38848d8c-2052-4ddc-89f5-6ada10db44f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Load AOI as gpd.GeoDataFrame\n",
    "AOI_fn = glob.glob(AOI_fn)[0]\n",
    "AOI = gpd.read_file(AOI_fn)\n",
    "# reproject the AOI to WGS to solve for the optimal UTM zone\n",
    "AOI_WGS = AOI.to_crs(4326)\n",
    "AOI_WGS_centroid = [AOI_WGS.geometry[0].centroid.xy[0][0],\n",
    "                    AOI_WGS.geometry[0].centroid.xy[1][0]]\n",
    "epsg_UTM = f.convert_wgs_to_utm(AOI_WGS_centroid[0], AOI_WGS_centroid[1])\n",
    "    \n",
    "# -----Load DEM as Xarray DataSet\n",
    "if DEM_fn==None:\n",
    "    \n",
    "    # query GEE for DEM\n",
    "    DEM, AOI_UTM = f.query_GEE_for_DEM(AOI)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # reproject AOI to UTM\n",
    "    AOI_UTM = AOI.to_crs(str(epsg_UTM))\n",
    "    # load DEM as xarray DataSet\n",
    "    DEM_fn = glob.glob(DEM_fn)[0]\n",
    "    DEM_rio = rio.open(DEM_fn) # open using rasterio to access the transform\n",
    "    DEM = xr.open_dataset(DEM_fn)\n",
    "    DEM = DEM.rename({'band_data': 'elevation'})\n",
    "    # reproject the DEM to the optimal UTM zone\n",
    "    DEM = DEM.rio.reproject(str('EPSG:'+epsg_UTM))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58403e13-eeb8-4413-b59a-11ef83557d99",
   "metadata": {},
   "source": [
    "### 1. Landsat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a82f35-5cfe-4d72-b8f0-20152e8cb99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Landsat'\n",
    "\n",
    "# -----Load images over the AOI\n",
    "# reformat AOI for clipping images\n",
    "AOI_WGS_bb_ee = ee.Geometry.Polygon(\n",
    "                        [[[AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.miny[0]],\n",
    "                          [AOI_WGS.geometry.bounds.maxx[0], AOI_WGS.geometry.bounds.miny[0]],\n",
    "                          [AOI_WGS.geometry.bounds.maxx[0], AOI_WGS.geometry.bounds.maxy[0]],\n",
    "                          [AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.maxy[0]],\n",
    "                          [AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.miny[0]]]\n",
    "                        ])\n",
    "\n",
    "# define search filters\n",
    "date_range_start = '2017-01-01'\n",
    "date_range_end = '2022-12-01'\n",
    "month_start = 5\n",
    "month_end = 10\n",
    "cloud_cover_max = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508420d8-5764-473a-b94e-38208eafda65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_image(im):\n",
    "    return im.clip(AOI_WGS_bb_ee.buffer(2000))\n",
    "\n",
    "print('Landsat image collection:')\n",
    "# Query GEE for imagery\n",
    "L = (ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\")\n",
    "         .filter(ee.Filter.lt(\"CLOUD_COVER\", cloud_cover_max))\n",
    "         .filterDate(ee.Date(date_range_start), ee.Date(date_range_end))\n",
    "         .filter(ee.Filter.calendarRange(month_start, month_end, 'month'))\n",
    "         .filterBounds(AOI_WGS_bb_ee))\n",
    "# define band names\n",
    "L_band_names = ['SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7', 'QA_PIXEL']\n",
    "#  clip images to AOI and select bands\n",
    "L_clip = L.map(clip_image).select(L_band_names)\n",
    "# convert image collection to xarray Dataset\n",
    "L_xr = L_clip.wx.to_xarray(scale=30, crs='EPSG:4326')\n",
    "# define RGB bands\n",
    "L_RGB_bands = ['SR_B4', 'SR_B3', 'SR_B2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d55b9c6-d96d-4c03-86e3-37c9516d2ca2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Prepare image collection for classification\n",
    "\n",
    "# Reproject image to UTM using rasterio.reproject\n",
    "L_xr_UTM = L_xr.rio.reproject(\"EPSG:\"+epsg_UTM)\n",
    "# replace no data values with NaN\n",
    "for band in L_band_names:\n",
    "    L_xr_UTM[band] = L_xr_UTM[band].where(L_xr_UTM[band] != L_xr_UTM[band]._FillValue)\n",
    "# account for image scalar\n",
    "for band in L_band_names[0:-1]:\n",
    "    L_xr_UTM[band] = L_xr_UTM[band] * dataset_dict[dataset]['SR_scalar']\n",
    "    \n",
    "# Mask cloud-covered pixels\n",
    "L_xr_UTM_mask = f.Landsat_mask_clouds(L_xr_UTM, L_band_names, plot_results=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d08f0-2a99-4b08-b2fe-e921a41ea9f0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Classify images\n",
    "import matplotlib\n",
    "# load image classifier and feature columns\n",
    "clf_fn = base_path+'inputs-outputs/L_classifier_all_sites.sav'\n",
    "clf = pickle.load(open(clf_fn, 'rb'))\n",
    "feature_cols_fn = base_path+'inputs-outputs/L_feature_cols.pkl'\n",
    "feature_cols = pickle.load(open(feature_cols_fn,'rb'))\n",
    "\n",
    "# calculate NDSI using red and NIR bands\n",
    "NDSI_bands = dataset_dict[dataset]['NDSI']\n",
    "L_xr_UTM_mask['NDSI'] = ((L_xr_UTM_mask[NDSI_bands[0]] - L_xr_UTM_mask[NDSI_bands[1]]) \n",
    "                         / (L_xr_UTM_mask[NDSI_bands[0]] + L_xr_UTM_mask[NDSI_bands[1]]))\n",
    "# loop through images\n",
    "for i, t in enumerate(L_xr_UTM_mask.time):\n",
    "    \n",
    "    # subset image collection to time\n",
    "    im = L_xr_UTM_mask.sel(time=t)\n",
    "    \n",
    "    # find indices of real numbers (no NaNs allowed in classification)\n",
    "    I_real = np.where((~np.isnan(im.to_array().data[0])) & (~np.isnan(im.to_array().data[1])) \n",
    "                      & (~np.isnan(im.to_array().data[2])) & (~np.isnan(im.to_array().data[3])))\n",
    "        \n",
    "    # create df of image band values\n",
    "    df = pd.DataFrame(columns=feature_cols)\n",
    "    for col in feature_cols:\n",
    "        df[col] = np.ravel(im[col].data[I_real])\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # -----Classify image\n",
    "    if len(df)>1:\n",
    "        array_classified = clf.predict(df[feature_cols])\n",
    "    else:\n",
    "        print(\"No real values found to classify, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # reshape from flat array to original shape\n",
    "    im_classified = np.zeros(np.shape(im.to_array().data[0]))\n",
    "    im_classified[:] = np.nan\n",
    "    im_classified[I_real] = array_classified\n",
    "    \n",
    "    # -----Determine snow-covered elevations\n",
    "    # mask the DEM using the AOI\n",
    "#     mask = rio.features.geometry_mask(AOI_UTM.geometry,\n",
    "#                                       out_shape=(len(DEM.y), len(DEM.x)),\n",
    "#                                       transform=DEM_rio.transform,\n",
    "#                                       invert=True)\n",
    "#     mask = xr.DataArray(mask , dims=(\"y\", \"x\"))\n",
    "#     # mask DEM values outside the AOI\n",
    "#     DEM_AOI = DEM.where(mask == True)\n",
    "#     # interpolate DEM to the image coordinates\n",
    "#     # im_classified = im_classified.squeeze(drop=True) # drop uneccesary dimensions\n",
    "#     x, y = im.indexes.values() # grab indices of image\n",
    "#     DEM_AOI_interp = DEM_AOI.interp(x=x, y=y, method=\"nearest\") # interpolate DEM to image coordinates\n",
    "#     # determine snow covered elevations\n",
    "#     DEM_AOI_interp_snow = DEM_AOI_interp.where(im_classified<=2) # mask pixels not classified as snow\n",
    "#     snow_est_elev = DEM_AOI_interp_snow.elevation.data.flatten() # create array of snow-covered pixel elevations\n",
    "#     snow_est_elev = snow_est_elev[~np.isnan(snow_est_elev)] # remove NaN values\n",
    "\n",
    "#     # -----Determine bins to use in histogram\n",
    "#     elev_min = np.fix(np.nanmin(DEM_AOI_interp.elevation.data.flatten())/10)*10\n",
    "#     elev_max = np.round(np.nanmax(DEM_AOI_interp.elevation.data.flatten())/10)*10\n",
    "#     bin_edges = np.linspace(elev_min, elev_max, num=int((elev_max-elev_min)/10 + 1))\n",
    "#     bin_centers = (bin_edges[1:] + bin_edges[0:-1]) / 2\n",
    "\n",
    "#     # -----Calculate elevation histograms\n",
    "#     H_DEM = np.histogram(DEM_AOI_interp.elevation.data.flatten(), bins=bin_edges)[0]\n",
    "#     H_snow_est_elev = np.histogram(snow_est_elev, bins=bin_edges)[0]\n",
    "#     H_snow_est_elev_norm = H_snow_est_elev / H_DEM\n",
    "\n",
    "    # -----Plot\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(16,8))#, gridspec_kw={'height_ratios': [3, 1]})\n",
    "    ax = ax.flatten()\n",
    "    # define x and y limits\n",
    "    xmin, xmax = np.min(im.x.data)/1e3, np.max(im.x.data)/1e3\n",
    "    ymin, ymax = np.min(im.y.data)/1e3, np.max(im.y.data)/1e3\n",
    "    # define colors for plotting\n",
    "    color_snow = '#4eb3d3'\n",
    "    color_ice = '#084081'\n",
    "    color_rock = '#fdbb84'\n",
    "    color_water = '#bdbdbd'\n",
    "    color_contour = '#f768a1'\n",
    "    # create colormap\n",
    "    colors = [color_snow, color_snow, color_ice, color_rock, color_water]\n",
    "    cmp = matplotlib.colors.ListedColormap(colors)\n",
    "    # RGB image\n",
    "    ax[0].imshow(np.dstack([im['SR_B4'].data, im['SR_B3'].data, im['SR_B2'].data]),\n",
    "               extent=(xmin, xmax, ymin, ymax))\n",
    "    ax[0].set_xlabel(\"Easting [km]\")\n",
    "    ax[0].set_ylabel(\"Northing [km]\")\n",
    "    ax[0].set_title('RGB image')\n",
    "    # classified image\n",
    "    ax[1].imshow(im_classified, cmap=cmp, vmin=1, vmax=5,\n",
    "                 extent=(xmin, xmax, ymin, ymax))\n",
    "    # plot dummy points for legend\n",
    "    ax[1].scatter(0, 0, color=color_snow, s=50, label='snow')\n",
    "    ax[1].scatter(0, 0, color=color_ice, s=50, label='ice')\n",
    "    ax[1].scatter(0, 0, color=color_rock, s=50, label='rock')\n",
    "    ax[1].scatter(0, 0, color=color_water, s=50, label='water')\n",
    "    ax[1].set_title('Classified image')\n",
    "    ax[1].set_xlabel('Easting [km]')\n",
    "    ax[1].legend(loc='best')\n",
    "    # NDSI threshold\n",
    "    im_ndsi_threshold = np.where(im['NDSI'].data >= 0.4, 1, 0)\n",
    "    cmp_snow = matplotlib.colors.ListedColormap(['white', color_snow])\n",
    "    im_ndsi = ax[2].imshow(im_ndsi_threshold, cmap=cmp_snow, clim=(0,1), extent=(xmin, xmax, ymin, ymax))\n",
    "    ax[2].set_xlabel(\"Easting [km]\")\n",
    "    ax[2].set_title('NDSI')\n",
    "    # fig.colorbar(im_ndsi, ax=ax[2], shrink=0.5)\n",
    "    # AOI\n",
    "    if AOI.geometry[0].geom_type=='MultiPolygon': # loop through geoms if AOI = MultiPolygon\n",
    "        for poly in AOI.geometry[0].geoms:\n",
    "            ax[0].plot([x/1e3 for x in poly.exterior.coords.xy[0]], [y/1e3 for y in poly.exterior.coords.xy[1]], '-k', linewidth=1, label='AOI')\n",
    "            ax[1].plot([x/1e3 for x in poly.exterior.coords.xy[0]], [y/1e3 for y in poly.exterior.coords.xy[1]], '-k', linewidth=1, label='_nolegend_')\n",
    "            ax[2].plot([x/1e3 for x in poly.exterior.coords.xy[0]], [y/1e3 for y in poly.exterior.coords.xy[1]], '-k', linewidth=1, label='_nolegend_')\n",
    "    else:\n",
    "        ax[0].plot([x/1e3 for x in AOI.geometry[0].exterior.coords.xy[0]], [y/1e3 for y in AOI.geometry[0].exterior.coords.xy[1]], '-k', linewidth=1, label='AOI')\n",
    "        ax[1].plot([x/1e3 for x in AOI.geometry[0].exterior.coords.xy[0]], [y/1e3 for y in AOI.geometry[0].exterior.coords.xy[1]], '-k', linewidth=1, label='_nolegend_')\n",
    "        ax[2].plot([x/1e3 for x in AOI.geometry[0].exterior.coords.xy[0]], [y/1e3 for y in AOI.geometry[0].exterior.coords.xy[1]], '-k', linewidth=1, label='_nolegend_')\n",
    "    # reset x and y limits\n",
    "    ax[0].set_xlim(xmin, xmax)\n",
    "    ax[0].set_ylim(ymin, ymax)\n",
    "    ax[1].set_xlim(xmin, xmax)\n",
    "    ax[1].set_ylim(ymin, ymax)\n",
    "    ax[2].set_xlim(xmin, xmax)\n",
    "    ax[2].set_ylim(ymin, ymax)\n",
    "    # image bands histogram\n",
    "    # h_b = ax[2].hist(im['SR_B2'].data.flatten(), color='blue', histtype='step', linewidth=2, bins=100, label=\"blue\")\n",
    "    # h_g = ax[2].hist(im['SR_B3'].data.flatten(), color='green', histtype='step', linewidth=2, bins=100, label=\"green\")\n",
    "    # h_r = ax[2].hist(im['SR_B4'].data.flatten(), color='red', histtype='step', linewidth=2, bins=100, label=\"red\")\n",
    "    # h_nir = ax[2].hist(im['SR_B5'].data.flatten(), color='brown', histtype='step', linewidth=2, bins=100, label=\"NIR\")\n",
    "    # ax[2].set_xlabel(\"Surface reflectance\")\n",
    "    # ax[2].set_ylabel(\"Pixel counts\")\n",
    "    # ax[2].legend(loc='best')\n",
    "    # ax[2].grid()\n",
    "    # # normalized snow elevations histogram\n",
    "    # ax[3].bar(bin_centers, H_snow_est_elev_norm, width=(bin_centers[1]-bin_centers[0]), color=color_snow, align='center')\n",
    "    # ax[3].set_xlabel(\"Elevation [m]\")\n",
    "    # ax[3].set_ylabel(\"% snow-covered\")\n",
    "    # ax[3].grid()\n",
    "    # ax[3].set_xlim(elev_min-10, elev_max+10)\n",
    "    # ax[3].set_ylim(0,1)\n",
    "    fig.suptitle(str(t.data)[0:10])\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # save figure\n",
    "    fig_fn = figures_out_path + 'L_'+str(t.data)[0:10]+'_SCA.png'\n",
    "    fig.savefig(fig_fn, dpi=300, facecolor='w')\n",
    "    print('figure saved to file: ' + fig_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65203f0f-467f-4132-b14c-7a691047ec9d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Test NDSI method\n",
    "for i, t in enumerate(L_xr_UTM_mask.time):\n",
    "    \n",
    "    # subset image collection to time\n",
    "    im = L_xr_UTM_mask.sel(time=t)\n",
    "    \n",
    "    # threshold NDSI and red bands\n",
    "    ndsi_thresh = np.where(im['NDSI'].data >=0.4, 1, np.nan)\n",
    "    red_thresh = np.where(im['SR_B4'].data >=0.9, 1, np.nan)\n",
    "    im_classified = np.where((ndsi_thresh==1) & (red_thresh==1), 1, np.nan)\n",
    "\n",
    "    # plot RGB, NDSI threshold, and red band threshold\n",
    "    fig, ax = plt.subplots(3, 2, figsize=(12,16))\n",
    "    ax = ax.flatten()\n",
    "    # fig.delaxes(ax[3])\n",
    "    # define x and y limits\n",
    "    xmin, xmax = np.min(im.x.data)/1e3, np.max(im.x.data)/1e3\n",
    "    ymin, ymax = np.min(im.y.data)/1e3, np.max(im.y.data)/1e3\n",
    "    # define colors for plotting\n",
    "    color_snow = '#4eb3d3'\n",
    "    color_ice = '#084081'\n",
    "    color_rock = '#fdbb84'\n",
    "    color_water = '#bdbdbd'\n",
    "    color_contour = '#f768a1'\n",
    "    # create colormap\n",
    "    cmp_snow = matplotlib.colors.ListedColormap([color_rock, color_snow])\n",
    "    cmp_red = matplotlib.colors.ListedColormap(['w', 'm'])\n",
    "    # RGB image\n",
    "    ax[0].imshow(np.dstack([im['SR_B4'].data, im['SR_B3'].data, im['SR_B2'].data]),\n",
    "               extent=(xmin, xmax, ymin, ymax))\n",
    "    ax[0].set_xlabel(\"Easting [km]\")\n",
    "    ax[0].set_ylabel(\"Northing [km]\")\n",
    "    ax[0].set_title('RGB image')\n",
    "    # classified image\n",
    "    ax[1].imshow(im_classified, cmap='Blues', clim=(0,1),\n",
    "               extent=(xmin, xmax, ymin, ymax))\n",
    "    ax[1].set_title('Classified image')\n",
    "    # NDSI\n",
    "    im_ndsi = ax[2].imshow(im['NDSI'].data, cmap='RdBu', clim=(-1,1),\n",
    "               extent=(xmin, xmax, ymin, ymax))\n",
    "    ax[2].set_title('NDSI')\n",
    "    fig.colorbar(im_ndsi, ax=ax[2], shrink=0.5)\n",
    "    # NDSI threshold\n",
    "    im_ndsi_thresh = ax[3].imshow(ndsi_thresh, cmap='Blues', clim=(0,1),\n",
    "               extent=(xmin, xmax, ymin, ymax))\n",
    "    ax[3].set_title('NDSI threshold')\n",
    "    fig.colorbar(im_ndsi_thresh, ax=ax[3], shrink=0.5)\n",
    "    # red band\n",
    "    im_red = ax[4].imshow(im['SR_B4'].data, cmap='Reds', clim=(0,1),\n",
    "               extent=(xmin, xmax, ymin, ymax))\n",
    "    ax[4].set_xlabel(\"Easting [km]\")\n",
    "    ax[4].set_ylabel(\"Northing [km]\")\n",
    "    ax[4].set_title('Red band')\n",
    "    fig.colorbar(im_red, ax=ax[4], shrink=0.5)\n",
    "    # red band threshold\n",
    "    im_ndsi_thresh = ax[5].imshow(red_thresh, cmap='Reds', clim=(0,1),\n",
    "               extent=(xmin, xmax, ymin, ymax))\n",
    "    ax[5].set_title('Red threshold')\n",
    "    fig.colorbar(im_ndsi_thresh, ax=ax[5], shrink=0.5)\n",
    "    \n",
    "    fig.suptitle(str(t.data)[0:10])\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76222c50-ebcd-4032-b8d8-775666e77ac5",
   "metadata": {},
   "source": [
    "## 2. Sentinel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399c6b58-d8c8-4aab-be5b-a4edcce155be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----Query GEE for Landsat, Sentinel, and MODIS imagery\n",
    "# S_col = (ee.ImageCollection(\"COPERNICUS/S2_SR\")\n",
    "#          .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', cloud_cover_max))\n",
    "#          .filterDate(ee.Date(date_range_start), ee.Date(date_range_end))\n",
    "#          .filter(ee.Filter.calendarRange(month_start, month_end, 'month'))\n",
    "#          .filterBounds(AOI_WGS_bb_ee))\n",
    "# M_col = (ee.ImageCollection(\"MODIS/061/MOD09GA\").merge(ee.ImageCollection(\"MODIS/061/MYD09GA\"))\n",
    "#          .filterDate(ee.Date(date_range_start), ee.Date(date_range_end))\n",
    "#          .filter(ee.Filter.calendarRange(month_start, month_end, 'month'))\n",
    "#          .filterBounds(AOI_WGS_bb_ee))\n",
    "\n",
    "# # -----Clip images to AOI and select bands\n",
    "# S_band_names = ['B2', 'B3', 'B4', 'B5', 'B6', 'B8', 'B11', 'B12', 'QA60']\n",
    "# M_band_names = ['sur_refl_'+x for x in ['b01', 'b04', 'b03', 'b05', 'b06', 'b07', 'state_1km']]\n",
    "\n",
    "# S_col_clip = S_col.map(clip_image).select(S_band_names)\n",
    "# M_col_clip = M_col.map(clip_image).select(M_band_names)\n",
    "\n",
    "# # -----Convert image collections to xarray Datasets\n",
    "# print('Sentinel-2 image collection:')\n",
    "# S_col_xr = S_col_clip.wx.to_xarray(scale=20, crs='EPSG:4326')\n",
    "# print('MODIS image collection:')\n",
    "# M_col_xr = M_col_clip.wx.to_xarray(scale=500, crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3bec14-1758-4232-8ab8-a70edf0c6441",
   "metadata": {},
   "source": [
    "### 2. Prepare image collections for classification: reproject to UTM and apply quality masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78af37bd-fba1-4009-a732-83e297297f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Classify images\n",
    "\n",
    "# -----Load image classifier and feature columns\n",
    "clf_fn = base_path+'inputs-outputs/L_classifier_all_sites.sav'\n",
    "clf = pickle.load(open(clf_fn, 'rb'))\n",
    "feature_cols_fn = base_path+'inputs-outputs/L_feature_cols.pkl'\n",
    "feature_cols = pickle.load(open(feature_cols_fn,'rb'))\n",
    "\n",
    "# calculate NDSI using red and NIR bands\n",
    "L_xr_UTM_mask['NDSI'] = (L_xr_UTM_mask['SR_B3'] - L_xr_UTM_mask['SR_B6']) / (L_xr_UTM_mask['SR_B3'] + L_xr_UTM_mask['SR_B6'])\n",
    "# loop through images\n",
    "for i, t in enumerate(L_xr_UTM_mask.time):\n",
    "    \n",
    "    # subset image collection to time\n",
    "    im = L_xr_UTM_mask.sel(time=t)\n",
    "    \n",
    "    # find indices of real numbers (no NaNs allowed in classification)\n",
    "    I_real = np.where(~np.isnan(im.to_array().data[0]))\n",
    "        \n",
    "    # create df of image band values\n",
    "    df = pd.DataFrame(columns=feature_cols)\n",
    "    for col in feature_cols:\n",
    "        df[col] = np.ravel(im[col].data[I_real])\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # -----Classify image\n",
    "    if len(df)>1:\n",
    "        array_classified = clf.predict(df[feature_cols])\n",
    "    else:\n",
    "        print(\"No real values found to classify, skipping...\")\n",
    "\n",
    "    # reshape from flat array to original shape\n",
    "    im_classified = np.zeros(np.shape(im.to_array().data[0]))\n",
    "    im_classified[:] = np.nan\n",
    "    im_classified[I_real] = array_classified\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    ax[0].imshow(im.to_array().data[0])\n",
    "    ax[1].imshow(im_classified)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7dd8f4-9e30-4a8e-baa5-324a857ca97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Reproject image collections to UTM\n",
    "# Reproject using rasterio.reproject\n",
    "# S_col_xr_UTM = S_col_xr.rio.reproject(\"EPSG:\"+epsg_code)\n",
    "# M_col_xr_UTM = M_col_xr.rio.reproject(\"EPSG:\"+epsg_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a979b36-2b7b-4957-bc1c-d2422fdd21b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Apply image quality masking to Landsat and Sentinel imagery\n",
    "# Landsat\n",
    "# Pixel quality band = \"QA_PIXEL\"\n",
    "# Bit 3 = cloud shadow, Bit 5 = cloud\n",
    "# def L8_QA_mask(im):\n",
    "#     cloudShadowBitMask = 1 << 3\n",
    "#     cloudsBitMask = 1 << 5;\n",
    "#     # Get the pixel QA band.\n",
    "#     qa = im.select('QA_PIXEL')\n",
    "#     # Both flags should be set to zero, indicating clear conditions.\n",
    "#     mask = (qa.bitwiseAnd(cloudShadowBitMask).eq(0) & (qa.bitwiseAnd(cloudsBitMask).eq(0)))\n",
    "#     # Return the masked image without the QA bands.\n",
    "#     return (\n",
    "#         image.updateMask(mask)\n",
    "#         .select(\"SR_B[0-9]*\")\n",
    "#         .copyProperties(image, [\"system:time_start\"])\n",
    "#     )\n",
    "\n",
    "# L_col_clip_mask = L_col_clip.map(L8_QA_mask)\n",
    "\n",
    "\n",
    "#   // Sentinel-2 image quality masking\n",
    "#   function S2QAMask(image){\n",
    "#     var QA60 = image.select(['QA60']);\n",
    "#     return image.updateMask(QA60.lt(1));\n",
    "#   }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74573ecd-cd7b-44f8-8083-bfda3ef184c9",
   "metadata": {},
   "source": [
    "### 3. Classify SCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42134ce3-8be3-4b1b-a988-42f3eef8ded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timer\n",
    "# t1 = time.time()\n",
    "\n",
    "# -----Load image classifiers and feature columns\n",
    "# Landsat\n",
    "L_clf_fn = base_path+'inputs-outputs/L_classifier_all_sites.sav'\n",
    "L_clf = pickle.load(open(L_clf_fn, 'rb'))\n",
    "L_feature_cols_fn = base_path+'inputs-outputs/L_feature_cols.pkl'\n",
    "L_feature_cols = pickle.load(open(L_feature_cols_fn,'rb'))\n",
    "# Sentinel-2\n",
    "S_clf_fn = base_path+'inputs-outputs/S2_classifier_all_sites.sav'\n",
    "S_clf = pickle.load(open(S_clf_fn, 'rb'))\n",
    "S_feature_cols_fn = base_path+'inputs-outputs/S2_feature_cols.pkl'\n",
    "S_feature_cols = pickle.load(open(S_feature_cols_fn,'rb'))\n",
    "# MODIS\n",
    "M_clf_fn = base_path+'inputs-outputs/M_classifier_all_sites.sav'\n",
    "M_clf = pickle.load(open(M_clf_fn, 'rb'))\n",
    "M_feature_cols_fn = base_path+'inputs-outputs/M_feature_cols.pkl'\n",
    "M_feature_cols = pickle.load(open(M_feature_cols_fn,'rb'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8c3ed2-5163-474e-b94e-6a2663e652a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Classify snow\n",
    "# Landsat\n",
    "for date in L_col_xr.time.data:\n",
    "    # create data frame to store pixel values\n",
    "    L_df = pd.DataFrame(columns=L_band_names+['NDSI'])\n",
    "    # extract pixel values\n",
    "    for band in L_band_names:\n",
    "        L_df[band] = L_col_xr.sel(time=date)[band].data.flatten()\n",
    "    # find indices of rows without NaN\n",
    "    I_real = np.where(pd.isnull(L_df).any(1)==False)[0]\n",
    "    # drop rows with NaN\n",
    "    L_df = L_df.dropna().reset_index(drop=True)\n",
    "    # calculate NDSI (G-SWIR)/(G+SWIR)\n",
    "    L_df['NDSI'] = (L_df['SR_B3'] -  L_df['SR_B6']) / (L_df['SR_B3'] +  L_df['SR_B6'])\n",
    "    \n",
    "    # classify SCA\n",
    "    array_classified = L_clf.predict(L_df[L_feature_cols])\n",
    "    \n",
    "    # reshape from flat array to original shape\n",
    "    im_classified = np.zeros((np.shape(L_col_xr['SR_B3'].sel(time=date).data)[0], np.shape(L_col_xr['SR_B3'].sel(time=date).data)[1]))\n",
    "    im_classified[:] = np.nan\n",
    "    im_classified[I_real] = array_classified\n",
    "    \n",
    "    # plot classified image\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n",
    "    plt.rcParams.update({'font.size': 14, 'font.sans-serif': 'Arial'})\n",
    "    # define x and y limits\n",
    "    xmin, xmax = np.min(L_col_xr.sel(time=date).x.data)/1000, np.max(L_col_xr.sel(time=date).x.data)/1000\n",
    "    ymin, ymax = np.min(L_col_xr.sel(time=date).y.data)/1000, np.max(L_col_xr.sel(time=date).y.data)/1000\n",
    "    # RGB image\n",
    "    ax1.imshow(np.dstack([L_col_xr['SR_B4'].sel(time=date), L_col_xr['SR_B3'].sel(time=date), L_col_xr['SR_B2'].sel(time=date)]), \n",
    "               extent=(xmin, xmax, ymin, ymax))\n",
    "    ax1.set_xlabel(\"Easting [km]\")\n",
    "    ax1.set_ylabel(\"Northing [km]\")\n",
    "    # define colors for plotting\n",
    "    color_snow = '#4eb3d3'\n",
    "    color_ice = '#084081'\n",
    "    color_rock = '#fdbb84'\n",
    "    color_water = '#bdbdbd'\n",
    "    # snow\n",
    "    if any(im_classified.flatten()==1):\n",
    "        ax2.imshow(np.where(im_classified == 1, 1, np.nan), cmap=matplotlib.colors.ListedColormap([color_snow, 'white']),\n",
    "                    extent=(xmin, xmax, ymin, ymax))\n",
    "        ax2.scatter(0, 0, color=color_snow, s=50, label='snow') # plot dummy point for legend\n",
    "    if any(im_classified.flatten()==2):\n",
    "        ax2.imshow(np.where(im_classified == 2, 4, np.nan), cmap=matplotlib.colors.ListedColormap([color_snow, 'white']),\n",
    "                    extent=(xmin, xmax, ymin, ymax))\n",
    "    # ice\n",
    "    if any(im_classified.flatten()==3):\n",
    "        ax2.imshow(np.where(im_classified == 3, 1, np.nan), cmap=matplotlib.colors.ListedColormap([color_ice, 'white']),\n",
    "                    extent=(xmin, xmax, ymin, ymax))\n",
    "        ax2.scatter(0, 0, color=color_ice, s=50, label='ice') # plot dummy point for legend\n",
    "    # rock/debris\n",
    "    if any(im_classified.flatten()==4):\n",
    "        ax2.imshow(np.where(im_classified == 4, 1, np.nan), cmap=matplotlib.colors.ListedColormap([color_rock, 'white']),\n",
    "                    extent=(xmin, xmax, ymin, ymax))\n",
    "        ax2.scatter(0, 0, color=color_rock, s=50, label='rock') # plot dummy point for legend\n",
    "    # water\n",
    "    if any(im_classified.flatten()==5):\n",
    "        ax2.imshow(np.where(im_classified == 5, 10, np.nan), cmap=matplotlib.colors.ListedColormap([color_water, 'white']),\n",
    "                    extent=(xmin, xmax, ymin, ymax))\n",
    "        ax2.scatter(0, 0, color=color_water, s=50, label='water') # plot\n",
    "    ax2.set_xlim(xmin, xmax)\n",
    "    ax2.set_ylim(ymin, ymax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27640d3a-b22a-48bf-8a66-d6985fd3dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data frame to store pixel values\n",
    "L_df = pd.DataFrame(columns=L_band_names+['NDSI'])\n",
    "# extract pixel values\n",
    "for band in L_band_names:\n",
    "    L_df[band] = L_col_xr.sel(time=date)[band].data.flatten()\n",
    "L_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaedf36-81a9-4329-8bdb-07d319932ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planet-snow",
   "language": "python",
   "name": "planet-snow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
