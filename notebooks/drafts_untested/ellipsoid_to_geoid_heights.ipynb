{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fdf19a3-16b9-4c04-878c-6181b4ff7009",
   "metadata": {},
   "source": [
    "# Transform elevations from the ellipsoid to the EGM96 geoid vertical reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8b84da-a2ed-40fd-9586-ce43202ee6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pyproj\n",
    "import os\n",
    "import glob\n",
    "import rioxarray as rxr\n",
    "import sys\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyproj.crs import CompoundCRS\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beae48a0-5310-4668-ad72-de73a3d56366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Define paths in directory\n",
    "# path to study-sites\n",
    "study_sites_path = '/Users/raineyaberle/Google Drive/My Drive/Research/CryoGARS-Glaciology/Advising/student-research/Alexandra-Friel/snow_cover_mapping_application/study-sites/'\n",
    "# path to snow-cover-mapping\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/'\n",
    "# add path to functions\n",
    "sys.path.insert(1, base_path+'functions/')\n",
    "import pipeline_utils as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a210ea6-52f5-4f67-a42f-b101b5af9991",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Grab list of site names in study_sites_path\n",
    "site_names = sorted(os.listdir(study_sites_path))\n",
    "site_names = [x for x in site_names if not x.startswith('.')]\n",
    "# only include sites with snowlines\n",
    "site_names = [x for x in site_names if len(glob.glob(study_sites_path + x + '/imagery/snowlines/*.csv')) > 0]\n",
    "# only include sites with ArcticDEM geoid files\n",
    "site_names = [x for x in site_names if os.path.exists(study_sites_path + x + '/DEMs/' + x + '_ArcticDEM_clip_geoid.tif')]\n",
    "print(str(len(site_names)) + ' study sites:')\n",
    "site_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4824d65-60c5-488e-beb9-a54030616c09",
   "metadata": {},
   "source": [
    "## Transform elevations for snowlines that used ArcticDEM Mosaic or USGS DEMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3560ca9-458f-4e39-a7a8-5de2a4bd76a0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Load EGM96 geoid heights\n",
    "egm96_fn = os.path.join(base_path, 'inputs-outputs', 'us_nga_egm96_15.tif')\n",
    "egm96 = xr.open_dataset(egm96_fn)\n",
    "egm96 = egm96.rename({'band_data': 'geoid_height'})\n",
    "\n",
    "# -----Iterate over sites\n",
    "bgotus_site_names = ['Wolverine', 'Gulkana', 'LemonCreek', 'SouthCascade', 'Sperry'] # BGOTUS\n",
    "    \n",
    "for site_name in tqdm(bgotus_site_names):\n",
    "\n",
    "    print(site_name)\n",
    "    \n",
    "    # load classified image and snowline file names\n",
    "    snowline_fns = sorted(glob.glob(study_sites_path + site_name + '/imagery/snowlines/*.csv'))\n",
    "    snowline_fns = [x for x in snowline_fns if '_adj' not in x]\n",
    "\n",
    "    # iterate over snowlines\n",
    "    for snowline_fn in tqdm(snowline_fns):\n",
    "        \n",
    "        # define adjusted file name and check if it exists\n",
    "        snowline_adj_fn = snowline_fn[0:-4] + '_adj.csv'\n",
    "        if os.path.exists(snowline_adj_fn):\n",
    "            continue\n",
    "\n",
    "        # load snowline\n",
    "        try:\n",
    "            snowline = pd.read_csv(snowline_fn)\n",
    "        except:\n",
    "            print('error opening ' + snowline_fn.split('/')[-1] + ', skipping...')\n",
    "            continue\n",
    "        \n",
    "        # convert to geopandas DataFrame\n",
    "        if snowline['geometry'][0] == '[]':\n",
    "            snowline_adj = snowline\n",
    "        else:\n",
    "            # adjust dataframe\n",
    "            snowline['geometry'] = snowline['geometry'].apply(wkt.loads)\n",
    "            snowline_gdf = gpd.GeoDataFrame(snowline, geometry=snowline['geometry'], crs=snowline['CRS'][0])\n",
    "            snowline_gdf[['snowlines_coords_X', 'snowlines_coords_Y']] = snowline_gdf[['snowlines_coords_X', \n",
    "                                                                                       'snowlines_coords_Y']].apply(pd.eval)\n",
    "            # some elevations are 'nan', so they must be handled differently\n",
    "            nan_dict = {'nan': np.nan}\n",
    "            snowline_gdf['snowline_elevs_m'] = snowline_gdf['snowline_elevs_m'].apply(lambda x: eval(x.replace('nan', 'nan_dict[\"nan\"]')))\n",
    "    \n",
    "            # reproject geometry to WGS84 horizontal coordinates\n",
    "            snowline_adj = snowline_gdf.to_crs('EPSG:4326') \n",
    "        \n",
    "            # interpolate geoid heights at snowline coordinates\n",
    "            geoid_heights = [egm96.sel(x=x, y=y, method='nearest').geoid_height.data[0] for x,y in \n",
    "                             list(zip(snowline_adj.geometry[0].coords.xy[0], snowline_adj.geometry[0].coords.xy[1]))]\n",
    "        \n",
    "            # subtract geoid heights from snowline elevations\n",
    "            snowline_adj['snowline_elevs_m'][0] = np.round(list(np.array(snowline_adj['snowline_elevs_m'][0]) - np.array(geoid_heights)),\n",
    "                                                           decimals=2)\n",
    "\n",
    "            # calculate median snowline elevation\n",
    "            snowline_adj['snowline_elevs_median_m'][0] = np.nanmedian(snowline_adj['snowline_elevs_m'][0])\n",
    "            \n",
    "            # adjust ELA from AAR\n",
    "            snowline_adj['ELA_from_AAR_m'] = np.round(snowline_adj['ELA_from_AAR_m'][0] - np.nanmean(geoid_heights),\n",
    "                                                      decimals=2)\n",
    "        \n",
    "        # rename and reorder columns\n",
    "        snowline_adj.rename(columns={'CRS': 'HorizontalCRS'}, inplace=True)\n",
    "        snowline_adj['VerticalCRS'] = 'EPSG:5773'\n",
    "        cols_adj = ['site_name', 'datetime', 'dataset', 'snowlines_coords_X', 'snowlines_coords_Y', \n",
    "                    'snowline_elevs_m', 'snowline_elevs_median_m', 'SCA_m2', 'AAR', 'ELA_from_AAR_m', \n",
    "                    'HorizontalCRS', 'VerticalCRS', 'geometry']\n",
    "        snowline_adj = snowline_adj[cols_adj]\n",
    "        \n",
    "        # save to file\n",
    "        snowline_adj.to_csv(snowline_adj_fn, index=False)\n",
    "        \n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5262a996-bf45-477e-9ccf-2d7225ae89d4",
   "metadata": {},
   "source": [
    "### Plot adjusted snowlines to check they make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147bebc5-f860-4377-9103-b4337441009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for site_name in bgotus_site_names:\n",
    "    \n",
    "    snowline_adj_fns = sorted(glob.glob(study_sites_path + site_name + '/imagery/snowlines/*_adj.csv'))\n",
    "    \n",
    "    snowlines = pd.DataFrame()\n",
    "    for fn in snowline_adj_fns:\n",
    "        \n",
    "        snowline = pd.read_csv(fn)\n",
    "        snowlines = pd.concat([snowlines, snowline])\n",
    "        \n",
    "    snowlines['datetime'] = pd.to_datetime(snowlines['datetime'], format='mixed')\n",
    "    snowlines.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(snowlines['datetime'], snowlines['snowline_elevs_median_m'], '.')\n",
    "    plt.grid()\n",
    "    plt.title(site_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4d65ee-ca3b-4dde-b865-405eae3d2268",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if each site has the same number of snowline files as adjusted snowline files\n",
    "\n",
    "for site_name in site_names:\n",
    "    \n",
    "    snowline_fns = sorted(glob.glob(study_sites_path + site_name + '/imagery/snowlines/*.csv'))\n",
    "    snowline_fns = [x for x in snowline_fns if '_adj' not in x]\n",
    "    snowline_adj_fns = sorted(glob.glob(study_sites_path + site_name + '/imagery/snowlines/*_adj.csv'))\n",
    "    \n",
    "    if len(snowline_fns)!=len(snowline_adj_fns):\n",
    "        print(site_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c689fac5-920e-4d25-a416-a6634308bbf6",
   "metadata": {},
   "source": [
    "### Delete old snowlines, rename adjusted files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd5431-d69c-42c5-a7af-e20c64d22622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for site_name in tqdm(bgotus_site_names):\n",
    "    \n",
    "#     snowline_fns = sorted(glob.glob(study_sites_path + site_name + '/imagery/snowlines/*.csv'))\n",
    "#     snowline_fns = [x for x in snowline_fns if '_adj' not in x]\n",
    "#     snowline_adj_fns = sorted(glob.glob(study_sites_path + site_name + '/imagery/snowlines/*_adj.csv'))\n",
    "        \n",
    "#     for snowline_fn in snowline_fns:\n",
    "#         os.remove(snowline_fn)\n",
    "        \n",
    "#     for snowline_adj_fn in snowline_adj_fns:\n",
    "#         snowline_adj_fn_new = snowline_adj_fn.replace('_adj', '')\n",
    "#         os.rename(snowline_adj_fn, snowline_adj_fn_new)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abe8568-7285-4444-b9b0-8a1a6a26237d",
   "metadata": {},
   "source": [
    "## Repeat for ELAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce53534b-0a56-43c7-bd70-b1b45f382109",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_names_elas = sorted(os.listdir(study_sites_path))\n",
    "site_names_elas = [x for x in site_names_elas if not x.startswith('.')]\n",
    "# only include sites with ELAs\n",
    "site_names_elas = [x for x in site_names_elas if len(glob.glob(study_sites_path + x + '/ELAs/*.csv')) > 0]\n",
    "# only include sites with ArcticDEM geoid files\n",
    "site_names_elas = [x for x in site_names_elas if os.path.exists(study_sites_path + x + '/DEMs/' + x + '_ArcticDEM_clip_geoid.tif')]\n",
    "print(str(len(site_names_elas)) + ' study sites:')\n",
    "site_names_elas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de57d239-1db1-48bc-9603-6beede1189f7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Load EGM96 geoid heights\n",
    "egm96_fn = os.path.join(base_path, 'inputs-outputs', 'us_nga_egm96_15.tif')\n",
    "egm96 = xr.open_dataset(egm96_fn)\n",
    "egm96 = egm96.rename({'band_data': 'geoid_height'})\n",
    "\n",
    "# -----Iterate over sites\n",
    "bgotus_site_names = ['Wolverine', 'Gulkana', 'LemonCreek', 'SouthCascade', 'Sperry'] # BGOTUS\n",
    "    \n",
    "for site_name in tqdm(site_names_elas[0:2]):\n",
    "\n",
    "    print(site_name)\n",
    "    \n",
    "    # load classified image and snowline file names\n",
    "    ela_fns = sorted(glob.glob(study_sites_path + site_name + '/ELAs/*.csv'))\n",
    "    ela_fns = [x for x in ela_fns if '_adj' not in x]\n",
    "\n",
    "    # iterate over snowlines\n",
    "    for ela_fn in tqdm(ela_fns):\n",
    "        \n",
    "        # define adjusted file name and check if it exists\n",
    "        ela_adj_fn = ela_fn[0:-4] + '_adj.csv'\n",
    "        if os.path.exists(ela_adj_fn):\n",
    "            continue\n",
    "\n",
    "        # load ELAs\n",
    "        try:\n",
    "            ela = pd.read_csv(ela_fn)\n",
    "        except:\n",
    "            print('error opening ' + ela_fn.split('/')[-1] + ', skipping...')\n",
    "            continue\n",
    "        \n",
    "        ela_gdf = gpd.GeoDataFrame(ela, geometry=ela['geometry'], crs=ela.loc[0, 'CRS'])\n",
    "        ela_gdf[['snowlines_coords_X', 'snowlines_coords_Y']] = ela_gdf[['snowlines_coords_X', \n",
    "                                                                         'snowlines_coords_Y']].apply(pd.eval)\n",
    "            \n",
    "        for i in range(0,len(ela)):\n",
    "            \n",
    "            if ela['geometry'][i] == '[]':\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # convert to GeoDataFrame\n",
    "                ela.loc[i, 'geometry'] = wkt.loads(ela.loc[i,'geometry'])\n",
    "                \n",
    "                # some elevations are 'nan', so they must be handled differently\n",
    "                nan_dict = {'nan': np.nan}\n",
    "                ela_gdf.loc[i,'snowline_elevs_m'] = pd.eval(ela_gdf.loc[i,'snowline_elevs_m'])#.replace('nan', 'nan_dict[\"nan\"]')\n",
    "\n",
    "                # reproject geometry to WGS84 horizontal coordinates\n",
    "                ela_adj = ela_gdf.to_crs('EPSG:4326') \n",
    "\n",
    "                # interpolate geoid heights at snowline coordinates\n",
    "                geoid_heights = [egm96.sel(x=x, y=y, method='nearest').geoid_height.data[0] for x,y in \n",
    "                                 list(zip(ela_adj.geometry[i].coords.xy[0], ela_adj.geometry[i].coords.xy[1]))]\n",
    "\n",
    "                # subtract geoid heights from snowline elevations\n",
    "                ela_adj.loc[i,'snowline_elevs_m'] = np.round(list(np.array(ela_adj.loc[i,'snowline_elevs_m']) - np.array(geoid_heights)),\n",
    "                                                             decimals=2)\n",
    "\n",
    "                # calculate median snowline elevation\n",
    "                ela_adj.loc[i,'snowline_elevs_median_m'] = np.nanmedian(ela_adj.loc[i,'snowline_elevs_m'])\n",
    "\n",
    "                # adjust ELA from AAR\n",
    "                ela_adj.loc[i,'ELA_from_AAR_m'] = np.round(ela_adj.loc[i,'ELA_from_AAR_m'] - np.nanmean(geoid_heights),\n",
    "                                                           decimals=2)\n",
    "        \n",
    "        # rename and reorder columns\n",
    "        ela_adj.rename(columns={'CRS': 'HorizontalCRS'}, inplace=True)\n",
    "        ela_adj['VerticalCRS'] = 'EPSG:5773'\n",
    "        cols_adj = ['site_name', 'datetime', 'dataset', 'snowlines_coords_X', 'snowlines_coords_Y', \n",
    "                    'snowline_elevs_m', 'snowline_elevs_median_m', 'SCA_m2', 'AAR', 'ELA_from_AAR_m', \n",
    "                    'HorizontalCRS', 'VerticalCRS', 'geometry']\n",
    "        ela_adj = ela_adj[cols_adj]\n",
    "        \n",
    "        # save to file\n",
    "        # ela_adj.to_csv(ela_adj_fn, index=False)\n",
    "        \n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8942a9-c5a0-43f2-b7c2-ea57a82f8a98",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ela_gdf.loc[i,'snowline_elevs_m']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7247b1d5-bbdf-43a8-9a6c-a6051165a286",
   "metadata": {},
   "source": [
    "## Reproject USGS DEMs to the geoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed404815-1377-4ed1-86ab-76f29adadb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Load EGM96 geoid heights\n",
    "egm96_fn = os.path.join(base_path, 'inputs-outputs', 'us_nga_egm96_15.tif')\n",
    "egm96 = xr.open_dataset(egm96_fn)\n",
    "egm96 = egm96.rename({'band_data': 'geoid_height'})\n",
    "\n",
    "# -----Iterate over sites\n",
    "bgotus_site_names = ['Wolverine', 'Gulkana', 'LemonCreek', 'SouthCascade', 'Sperry'] # BGOTUS\n",
    "\n",
    "for site_name in bgotus_site_names:\n",
    "    \n",
    "    print(site_name)\n",
    "    \n",
    "    DEM_fn = glob.glob(study_sites_path + site_name + '/DEMs/*USGS*.tif')[0]\n",
    "    DEM = xr.open_dataset(DEM_fn)\n",
    "    DEM = DEM.rio.reproject('EPSG:4326')\n",
    "    elevations = DEM.band_data.data[0]\n",
    "    DEM = DEM.drop_dims('band')\n",
    "    \n",
    "    # interpolate geoid heights at snowline coordinates\n",
    "    geoid_heights = egm96.sel(x=DEM.x.data, y=DEM.y.data, method='nearest').geoid_height.data[0]\n",
    "    \n",
    "    # subtract geoid heights from elevations\n",
    "    DEM['elevation'] = (('y', 'x'), elevations - geoid_heights)\n",
    "    \n",
    "    # plot \n",
    "    plt.imshow(DEM.elevation.data, extent=(np.min(DEM.x.data), np.max(DEM.x.data), \n",
    "                                           np.min(DEM.y.data), np.max(DEM.y.data)))\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    # save to file\n",
    "    DEM_geoid_fn = DEM_fn.replace('.tif', '_geoid.tif')\n",
    "    DEM.rio.to_raster(DEM_geoid_fn)\n",
    "    print('DEM referenced to geoid and saved to file: ' + DEM_geoid_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ece9562-e98f-4af1-93a8-b6b6f41c7e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "elevations_geoid = DEM.band_data.data - geoid_heights\n",
    "elevations_geoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd85350-5e48-414d-b750-c04c06833546",
   "metadata": {},
   "source": [
    "## For sites that use the NASADEM, adjust column names and resave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7728fecd-d30d-4d07-9213-9f99a408c26b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Grab list of site names in study_sites_path\n",
    "site_names = sorted(os.listdir(study_sites_path))\n",
    "site_names = [x for x in site_names if not x.startswith('.')]\n",
    "# only include sites with snowlines\n",
    "site_names = [x for x in site_names if len(glob.glob(study_sites_path + x + '/imagery/snowlines/*.csv')) > 0]\n",
    "# only include sites with ArcticDEM geoid files\n",
    "site_names = [x for x in site_names if os.path.exists(study_sites_path + x + '/DEMs/' + x + '_NASADEM_clip.tif')]\n",
    "print(str(len(site_names)) + ' study sites:')\n",
    "site_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2159acb0-54aa-419e-9003-44c4422aa8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define columns order\n",
    "cols = ['site_name', 'datetime', 'snowlines_coords_X', 'snowlines_coords_Y',\n",
    "        'HorizontalReference', 'VerticalReference', 'snowline_elevs_m',\n",
    "        'snowline_elevs_median_m', 'SCA_m2', 'AAR', 'ELA_from_AAR_m', 'dataset',\n",
    "        'geometry']\n",
    "\n",
    "# iterate over site names\n",
    "for site_name in site_names:\n",
    "\n",
    "    print(site_name)\n",
    "\n",
    "    # grab snowline filenames\n",
    "    snowlines_path = os.path.join(study_sites_path, site_name, 'imagery', 'snowlines')\n",
    "    snowline_fns = glob.glob(snowlines_path + '/*.csv')\n",
    "\n",
    "    # iterate over snowline file names\n",
    "    for snowline_fn in tqdm(snowline_fns):\n",
    "        snowline = pd.read_csv(snowline_fn)\n",
    "        if 'HorizontalReference' in snowline.keys():\n",
    "            continue\n",
    "\n",
    "        # reproject geometry to WGS84 lat lon\n",
    "        if snowline['geometry'][0]!='[]':\n",
    "            snowline['geometry'] = snowline['geometry'].apply(wkt.loads)\n",
    "            snowline_gdf = gpd.GeoDataFrame(snowline, crs=snowline['CRS'][0])\n",
    "            snowline_gdf = snowline_gdf.to_crs('EPSG:4326')\n",
    "        else:\n",
    "            snowline_gdf = snowline\n",
    "            \n",
    "        snowline_gdf['HorizontalReference'] = snowline_gdf['CRS']\n",
    "        snowline_gdf['VerticalReference'] = 'EGM96 geoid (EPSG:5773)'\n",
    "\n",
    "        snowline_gdf = snowline_gdf[cols]\n",
    "\n",
    "        # resave to file\n",
    "        snowline_gdf.to_csv(snowline_fn, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6373a118-4bc6-4ea5-93ad-2117e28d5f48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Grab list of site names in study_sites_path\n",
    "site_names = sorted(os.listdir(study_sites_path))\n",
    "site_names = [x for x in site_names if not x.startswith('.')]\n",
    "# only include sites with snowlines\n",
    "site_names = [x for x in site_names if len(glob.glob(study_sites_path + x + '/imagery/snowlines/*.csv')) > 0]\n",
    "print(str(len(site_names)) + ' study sites:')\n",
    "site_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06695e8-7107-4a8f-a337-ff632122b3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define columns order\n",
    "cols = ['site_name', 'datetime', 'snowlines_coords_X', 'snowlines_coords_Y',\n",
    "        'HorizontalCRS', 'VerticalCRS', 'snowline_elevs_m',\n",
    "        'snowline_elevs_median_m', 'SCA_m2', 'AAR', 'ELA_from_AAR_m', 'dataset',\n",
    "        'geometry']\n",
    "\n",
    "# iterate over site names\n",
    "for site_name in site_names:\n",
    "\n",
    "    print(site_name)\n",
    "\n",
    "    # grab snowline filenames\n",
    "    snowlines_path = os.path.join(study_sites_path, site_name, 'imagery', 'snowlines')\n",
    "    snowline_fns = glob.glob(snowlines_path + '/*.csv')\n",
    "\n",
    "    # iterate over snowline file names\n",
    "    for snowline_fn in tqdm(snowline_fns):\n",
    "        snowline = pd.read_csv(snowline_fn)\n",
    "        if 'HorizontalCRS' in snowline.keys():\n",
    "            continue\n",
    "            \n",
    "        snowline_gdf = snowline_gdf.rename({'HorizontalReference':'HorizontalCRS',\n",
    "                                            'VerticalReference':'VerticalCRS'})\n",
    "\n",
    "        # snowline_gdf = snowline_gdf[cols]\n",
    "\n",
    "        # resave to file\n",
    "        snowline_gdf.to_csv(snowline_fn, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2237ef9b-d4fe-4e77-97b8-109040474d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
