{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d60f0ee9-7e33-4cc4-aee9-9467869a69a6",
   "metadata": {},
   "source": [
    "# Testing new automated snowline filtering methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a08328",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MODIFY HERE #####\n",
    "# path to snow-cover-mapping\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/'\n",
    "# specify site name\n",
    "site_name = 'Emmons'\n",
    "# path where figures will be saved\n",
    "figures_out_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/snow_cover_mapping/study-sites/' + site_name + '/figures/' \n",
    "# path to snowline files\n",
    "sl_est_path = figures_out_path +'../imagery/snowlines/' \n",
    "# path where filtered snowlines will be saved\n",
    "out_path = sl_est_path \n",
    "# path to USGS mass balance data/ELA csvs \n",
    "# If no USGS files, set usgs_path=None\n",
    "usgs_path = None #'/Users/raineyaberle/Google Drive/My Drive/Research/PhD/GIS_data/USGS/benchmarkGlacier_massBalance/'\n",
    "#######################\n",
    "\n",
    "# import packages\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import iqr\n",
    "from time import mktime\n",
    "import seaborn as sns\n",
    "from scipy.stats import median_abs_deviation as mad\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3a7d4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Compile snowline files\n",
    "sl_est_fns = glob.glob(sl_est_path + '*snowline.csv')\n",
    "# compile all snowline files into one DataFrame\n",
    "sl_est_full = pd.DataFrame()\n",
    "for fn in sl_est_fns:\n",
    "    # read file\n",
    "    if 'csv' in fn:\n",
    "        sl_est = pd.read_csv(fn)\n",
    "    elif 'pkl' in fn:\n",
    "        sl_est = pickle.load(open(fn, 'rb'))\n",
    "    # concatenate to df\n",
    "    sl_est_full = pd.concat([sl_est_full, sl_est])\n",
    "sl_est_full = sl_est_full.reset_index(drop=True).sort_values(by=['datetime']) # renumber, sort by date\n",
    "\n",
    "# -----Reformat snowlines dataframes\n",
    "# unify datetime datatypes\n",
    "sl_est_full['datetime'] = sl_est_full['datetime'].astype('datetime64[ns]')\n",
    "# add month column\n",
    "sl_est_full['month'] = [x.month for x in sl_est_full['datetime']]\n",
    "# add year column\n",
    "sl_est_full['year'] = [x.year for x in sl_est_full['datetime']]\n",
    "\n",
    "# extract all unique years\n",
    "years = np.unique(sl_est_full['year'])\n",
    "# set datetime as index\n",
    "sl_est_full.index = sl_est_full['datetime']\n",
    "\n",
    "# plot all snowline elevations \n",
    "fig = plt.figure(figsize=(10,6))\n",
    "cmap = plt.cm.viridis\n",
    "for i, year in enumerate(years):\n",
    "    sl_est_year = sl_est_full.loc[sl_est_full['year']==year]\n",
    "    plt.plot(sl_est_year['datetime'], sl_est_year['snowlines_elevs_median_m'], '.', color=cmap(i/len(years)))\n",
    "plt.grid()\n",
    "plt.ylabel('Median snowline elevation [m]')\n",
    "plt.xlabel('Date')\n",
    "plt.title(site_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066a7a9b-3d3b-457d-bc6c-ea857ca07dc7",
   "metadata": {},
   "source": [
    "## Filter using gradient in median snowline elevations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2359182-878b-41c7-b1b4-0b518f8afb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Create array of days since first observation date\n",
    "min_date = np.nanmin(sl_est_full['datetime'].values.astype('datetime64[D]'))\n",
    "max_date = np.nanmax(sl_est_full['datetime'].values.astype('datetime64[D]'))\n",
    "sl_days = (sl_est_full['datetime'].values.astype('datetime64[D]') - min_date).astype(int)\n",
    "sl = sl_est_full['snowlines_elevs_median_m'].values\n",
    "\n",
    "# -----Filter snowline elevations using gradient in median snowline elevations\n",
    "# define threshold based on site elevation range\n",
    "min_elev, max_elev = np.nanmin(sl), np.nanmax(sl)\n",
    "threshold = (max_elev - min_elev)*0.3 # gradient threshold\n",
    "\n",
    "## First iteration\n",
    "Iremove = [] # indices of snowlines to remove\n",
    "gradient_recursive = np.zeros(len(sl))\n",
    "gradient_forward = np.zeros(len(sl))\n",
    "for i in np.arange(1,len(sl)-1):\n",
    "    \n",
    "    # -----Calculate both forward and recursive gradients (where possible)\n",
    "    # calculate recursive gradient\n",
    "    # if sl_days[i]!=sl_days[i-1]: # if previous point is on a different day\n",
    "    #     gradient_recursive[i] = np.abs((sl[i] - sl[i-1]) / (sl_days[i] - sl_days[i-1]))\n",
    "    # else: # if previous point is on the same day, don't count day change\n",
    "    #     gradient_recursive[i] = np.abs(sl[i] - sl[i-1])\n",
    "    # # calculate forward gradient\n",
    "    # if sl_days[i]!=sl_days[i+1]: # if previous point is on a different day\n",
    "    #     gradient_forward[i] = np.abs((sl[i+1] - sl[i]) / (sl_days[i+1] - sl_days[i]))\n",
    "    # else: # if previous point is on the same day, don't count day change\n",
    "    #     gradient_forward[i] = np.abs(sl[i+1] - sl[i])\n",
    "    # # if both gradients exceed the threshold, add i to Iremove to filter out\n",
    "    # if (gradient_recursive[i] > threshold) & (gradient_forward[i] > threshold):\n",
    "    #     Iremove += [i]\n",
    "    \n",
    "    # -----Calculate just the recursive gradient\n",
    "    # if previous point was removed, use second to last point\n",
    "    if i-1 in Iremove:\n",
    "        if sl_days[i]!=sl_days[i-2]: # if previous point is on a different day\n",
    "            gradient_recursive[i] = np.abs((sl[i] - sl[i-2]) / (sl_days[i] - sl_days[i-2]))\n",
    "        else: # if previous point is on the same day, don't count day change\n",
    "            gradient_recursive[i] = np.abs(sl[i] - sl[i-2])\n",
    "    # otherwise, use previous point\n",
    "    else:\n",
    "        if sl_days[i]!=sl_days[i-1]: # if previous point is on a different day\n",
    "            gradient_recursive[i] = np.abs((sl[i] - sl[i-1]) / (sl_days[i] - sl_days[i-1]))\n",
    "        else: # if previous point is on the same day, don't count day change\n",
    "            gradient_recursive[i] = np.abs(sl[i] - sl[i-1])\n",
    "    # if gradient exceed the threshold, add i to Iremove to filter out\n",
    "    if (gradient_recursive[i] > threshold):\n",
    "        Iremove += [i]     \n",
    "    \n",
    "# Apply filter\n",
    "sl_filt = np.delete(sl, Iremove)\n",
    "sl_days_filt = np.delete(sl_days, Iremove)\n",
    "sl_removed = sl[Iremove]\n",
    "sl_days_removed = sl_days[Iremove]\n",
    "\n",
    "## Second iteration - make sure points still fulfill the criteria\n",
    "# Iremove = [] # indices of snowlines to remove\n",
    "# gradient_recursive = np.zeros(len(sl_filt))\n",
    "# for i in np.arange(1,len(sl_filt)-1):\n",
    "    \n",
    "#     # -----Calculate just the recursive gradient\n",
    "#     # if previous point was removed, use second to last point\n",
    "#     if i-1 in Iremove:\n",
    "#         if sl_days_filt[i]!=sl_days_filt[i-2]: # if previous point is on a different day\n",
    "#             gradient_recursive[i] = np.abs((sl_filt[i] - sl_filt[i-2]) / (sl_days[i] - sl_days[i-2]))\n",
    "#         else: # if previous point is on the same day, don't count day change\n",
    "#             gradient_recursive[i] = np.abs(sl_filt[i] - sl_filt[i-2])\n",
    "#     # otherwise, use previous point\n",
    "#     else:\n",
    "#         if sl_days_filt[i]!=sl_days_filt[i-1]: # if previous point is on a different day\n",
    "#             gradient_recursive[i] = np.abs((sl_filt[i] - sl_filt[i-1]) / (sl_days_filt[i] - sl_days_filt[i-1]))\n",
    "#         else: # if previous point is on the same day, don't count day change\n",
    "#             gradient_recursive[i] = np.abs(sl_filt[i] - sl_filt[i-1])\n",
    "#     # if gradient exceed the threshold, add i to Iremove to filter out\n",
    "#     if (gradient_recursive[i] > threshold):\n",
    "#         Iremove += [i]      \n",
    "# # Apply filter\n",
    "# sl_filt_filt = np.delete(sl_filt, Iremove)\n",
    "# sl_days_filt_filt = np.delete(sl_days_filt, Iremove)\n",
    "# sl_removed_removed = sl_removed[Iremove]\n",
    "# sl_days_removed_removed = sl_days_removed[Iremove]\n",
    "    \n",
    "# -----Plot results\n",
    "fig, ax = plt.subplots(2,1, figsize=(10,8))\n",
    "ax[0].plot(sl_days_filt, sl_filt, '.k', label='Filtered time series')\n",
    "ax[0].plot(sl_days_removed, sl_removed, 'xr', label='Removed points')\n",
    "ax[0].set_ylabel('Median snowline elevation [m]')\n",
    "ax[0].grid()\n",
    "ax[0].legend(loc='best')\n",
    "ax[1].plot(sl_days, gradient_recursive, '.-k', markersize=10, linewidth=1, label='gradient$_{recursive}$')\n",
    "ax[1].plot(sl_days_removed, gradient_recursive[Iremove], 'xr')\n",
    "ax[1].set_ylabel('Gradient in median snowline elevations [m/day]')\n",
    "ax[1].set_xlabel('Days since first observation')\n",
    "ax[1].set_title('Threshold = '+str(np.round(threshold,2)))\n",
    "ax[1].legend(loc='best')\n",
    "ax[1].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0fe09e-00b4-4f18-a66b-d81edd2dace1",
   "metadata": {},
   "source": [
    "#### Apply Butterworth low-pass filter\n",
    "\n",
    "Adapted from [Neha Jirafe's Medium article](https://medium.com/analytics-vidhya/how-to-filter-noise-with-a-low-pass-filter-python-885223e5e9b7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a39fc-9b1d-4c93-bc8f-8f35923a9359",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.signal import butter,filtfilt\n",
    "\n",
    "# create array of all days in time series\n",
    "min_date = np.nanmin(sl_est_full['datetime'].values.astype('datetime64[D]'))\n",
    "max_date = np.nanmax(sl_est_full['datetime'].values.astype('datetime64[D]'))\n",
    "days = (np.arange(min_date, max_date) - min_date).astype(int)\n",
    "\n",
    "# interpolate snowline at each day\n",
    "sl_days = (sl_est_full['datetime'].values.astype('datetime64[D]') - min_date).astype(int)\n",
    "f = interp1d(sl_days, \n",
    "             sl_est_full['snowlines_elevs_median_m'].values)\n",
    "sl_interp = f(days)\n",
    "# remove NaNs\n",
    "for i in np.arange(0,len(sl_interp)):\n",
    "    if np.isnan(sl_interp[i]):\n",
    "        sl_interp[i] = sl_interp[i-1]\n",
    "\n",
    "# Filter requirements\n",
    "n = len(days)   # number of samples\n",
    "fs = 1          # sample rate [1/d]\n",
    "cutoff = 0.3    # desired cutoff frequency of the filter [1/d], must be betwen 0 and 1\n",
    "nyq = 0.5 * fs  # Nyquist Frequency\n",
    "order = 3       # sine wave can be approx represented as polynomial\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order):\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    # Get the filter coefficients \n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "# Filter the data\n",
    "y = butter_lowpass_filter(sl_interp, cutoff, fs, order)\n",
    "\n",
    "# plot raw, interpolated, and filtered data\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "# plt.plot(days, sl_interp, '.c')\n",
    "plt.plot(sl_days, sl_est_full['snowlines_elevs_median_m'].values, '.k', markersize=8)\n",
    "plt.plot(days, y, '.-m', markersize=3, linewidth=0.5)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0d5643",
   "metadata": {},
   "source": [
    "#### Align snowline elevation peaks and minimums for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0901f3a0-4cec-401b-b8f0-6d03a732c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate running median\n",
    "from collections import deque\n",
    "from bisect import insort, bisect_left\n",
    "from itertools import islice\n",
    "def running_median(x, N):\n",
    "    '''\n",
    "    Calculate running median. \n",
    "    Adapted from Peter Otten: https://stackoverflow.com/questions/37671432/how-to-calculate-running-median-efficiently\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: numpy.array\n",
    "        input array of values\n",
    "    N: int or float\n",
    "        window size over which to calculate running median\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    x_moving_median: np.array\n",
    "        moving median of x with a window of N\n",
    "    '''\n",
    "    seq = iter(x)\n",
    "    d = deque()\n",
    "    s = []\n",
    "    x_moving_median = []\n",
    "    for item in islice(x, N):\n",
    "        d.append(item)\n",
    "        insort(s, item)\n",
    "        x_moving_median.append(s[len(d)//2])\n",
    "    m = N // 2\n",
    "    for item in seq:\n",
    "        old = d.popleft()\n",
    "        d.append(item)\n",
    "        del s[bisect_left(s, old)]\n",
    "        insort(s, item)\n",
    "        x_moving_median.append(s[m])\n",
    "    # adjust values to original x where median is invalid\n",
    "    x_moving_median = np.concatenate((x[0:int((N-1)/2)], \n",
    "                                      x_moving_median[int(N*2)-1:], \n",
    "                                      x[-int((N-1)/2):]))\n",
    "    return x_moving_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb83127",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----Set up figure\n",
    "# fig = plt.figure(figsize=(20, 6))\n",
    "# plt.rcParams.update({'font.size':18, 'font.sans-serif':'Arial'})\n",
    "# spec = fig.add_gridspec(ncols=2, nrows=1, width_ratios=[1, 2.5])\n",
    "# ax1 = fig.add_subplot(spec[0, 0])\n",
    "# ax1.set_xlabel('Month')\n",
    "# ax1.set_ylabel('Median snowline elevations [m]')\n",
    "# ax1.grid()\n",
    "# ax2 = fig.add_subplot(spec[0, 1])\n",
    "# ax2.set_xlabel('Date')\n",
    "# ax2.grid()\n",
    "\n",
    "fig1, ax1 = plt.subplots(1, 1)\n",
    "plt.style.use('default')\n",
    "col = plt.cm.viridis\n",
    "\n",
    "fig2, ax2 = plt.subplots(1, 1)\n",
    "\n",
    "# -----Initialize variables\n",
    "df_full = pd.DataFrame()\n",
    "days_new_full = []\n",
    "snowline_elevs_full = []\n",
    "sl_min = np.nanmin(sl_est_year['snowlines_elevs_median_m'].values) # minimum snowline elevation for all dates\n",
    "\n",
    "# -----Loop through years\n",
    "for i, year in enumerate(years):\n",
    "    \n",
    "    # subset snowlines by year\n",
    "    sl_est_year = sl_est_full.loc[sl_est_full['year']==year]\n",
    "    # grab day of year (DOY) for each date\n",
    "    sl_est_year_DOY = (sl_est_year['datetime'].values.astype('datetime64[D]') \n",
    "                         - np.datetime64(str(year)+'-01-01')).astype(int)\n",
    "    sl_interp = sl_est_year['snowlines_elevs_median_m'].values\n",
    "    days = sl_est_year_DOY\n",
    "    # create array of days between minimum and maximum DOY\n",
    "    # days = np.arange(np.nanmin(sl_est_year_DOY), np.nanmax(sl_est_year_DOY)).astype(int)\n",
    "    # # interpolate snowline at each day\n",
    "    # f = interp1d(sl_est_year_DOY, sl_est_year['snowlines_elevs_median_m'])\n",
    "    # sl_interp = f(days)\n",
    "    \n",
    "    # calculate moving median \n",
    "    N = 3 # window size\n",
    "    sl_interp_med = running_median(sl_interp, N)\n",
    "    \n",
    "    # shift into \"relative space\" where min and max snowline elevations are ~aligned for all years\n",
    "    max_position = 100 # \"day\" where all maximum snowline elevations will be aligned\n",
    "    min_positions = [0, 200] # \"days\" where all minimum snowline elevations will be aligned\n",
    "    Imax = np.argwhere(sl_interp_med==np.nanmax(sl_interp_med))[-1][0] # index where maximum snowline elevation occurs\n",
    "    Imins = [0,len(sl_interp)] # index where minimum snowline elevation occurs - start with min and max indices\n",
    "    if any(sl_interp_med[0:Imax] == sl_min): # if any snowline elevs equal minimum BEFORE reaching maximum\n",
    "        Imins[0] = np.argwhere(sl_interp_med[0:Imax]==sl_min)[-1][0] # minimum before max snowline elev\n",
    "    if any(sl_interp_med[Imax:] == sl_min): # if any snowline elevs equal minimum AFTER reaching maximum\n",
    "        Imins[1] = np.argwhere(sl_interp_med[Imax:]==sl_min)[0][0] + Imax # minimum after max snowline elev\n",
    "    print(Imax, Imins)\n",
    "    # create new days array\n",
    "    days_new = np.zeros(len(days))\n",
    "    # align first minimum and maximum snowline elevations\n",
    "    days_new[Imins[0]:Imax] = [i*(max_position - min_positions[0]) for i in np.linspace(0, 1, num=Imax-Imins[0])]\n",
    "    # align maximum and second minimum snowlines\n",
    "    days_new[Imax:Imins[1]] = [i*(min_positions[1]-max_position)+max_position for i in np.linspace(0, 1, num=Imins[1]-Imax)]\n",
    "    # align points outside the minimum snowline elevations\n",
    "    days_new[0:Imins[0]] = [i-Imins[0] for i in np.arange(0,Imins[0])]\n",
    "    days_new[Imins[1]:] = [Imins[1]+i for i in np.arange(0,len(days)-Imins[1])]\n",
    "    # calculate magnitude shift from original days\n",
    "    days_shift = days_new - days\n",
    "    \n",
    "    # plot\n",
    "    ax1.plot(days_new, sl_interp, '.', color=col(i/len(years)), label=year)\n",
    "    ax2.plot(days_new, sl_interp_med, '.', color=col(i/len(years)), label=year)\n",
    "\n",
    "    # add to full data variables\n",
    "    days_new_full = np.concatenate((days_new_full, days_new))\n",
    "    snowline_elevs_full = np.concatenate((snowline_elevs_full, sl_interp_med))\n",
    "    df = pd.DataFrame({'year': [year],\n",
    "                       'days': [days],\n",
    "                       'days_new': [days_new],\n",
    "                       'sl_est': [sl_interp],\n",
    "                      })\n",
    "    df_full = pd.concat([df_full, df])\n",
    "    \n",
    "# adjust full dataframe\n",
    "df_full = df_full.reset_index(drop=True)\n",
    "\n",
    "# sort days and snowlines by date\n",
    "Isort = np.argsort(days_new_full)\n",
    "days_new_full_sort = days_new_full[Isort]\n",
    "snowline_elevs_full_sort = snowline_elevs_full[Isort]\n",
    "\n",
    "# characterize acceptable range\n",
    "unique_days = np.unique(np.round(days_new_full_sort))\n",
    "# weeks = np.arange(np.fix(unique_days[0]/7), np.round(unique_days[-1]/7))\n",
    "med = np.zeros(len(unique_days))\n",
    "IQR_min, IQR_max = np.zeros(len(unique_days)), np.zeros(len(unique_days))\n",
    "for i, day in enumerate(unique_days):\n",
    "    # Iweek = np.argwhere((days_new_full_sort/7 > weeks[i]) & (days_new_full_sort/7 < weeks[i+1]))\n",
    "    Iday = np.argwhere(days_new_full_sort==day)\n",
    "    med[i] = np.nanmedian(snowline_elevs_full_sort[Iday])\n",
    "    IQR_min[i] = iqr(snowline_elevs_full_sort[Iday], rng=(10,50), nan_policy='omit')\n",
    "    IQR_max[i] = iqr(snowline_elevs_full_sort[Iday], rng=(50,90), nan_policy='omit')\n",
    "    \n",
    "# ax1.fill_between(unique_days, med-IQR_min, med+IQR_max, color='#4eb3d3', label='Acceptable range')\n",
    "# ax1.plot(unique_days, med, '-k')\n",
    "ax1.grid()\n",
    "ax1.legend(loc='center right', bbox_to_anchor=[1.2, 0.4, 0.2, 0.2])\n",
    "ax2.set_title('Median trend')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2c1fac-0f04-402d-a6ea-e1be57ecff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(sl_interp_med[0:Imax]==sl_min)[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1bd1c8-80e2-43d3-8ac2-efac5603b6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
